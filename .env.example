# ============================================
# Configuration RAGFab
# ============================================

# -------------------------------------------
# PostgreSQL Database Configuration
# -------------------------------------------
POSTGRES_USER=raguser
POSTGRES_PASSWORD=changeme_secure_password
POSTGRES_DB=ragdb
POSTGRES_PORT=5432

# Database URL pour l'application
# Format: postgresql://user:password@host:port/database
# Local: postgresql://raguser:changeme_secure_password@localhost:5432/ragdb
# Docker: postgresql://raguser:changeme_secure_password@postgres:5432/ragdb
# Coolify: postgresql://raguser:changeme_secure_password@postgres.internal:5432/ragdb
DATABASE_URL=postgresql://raguser:changeme_secure_password@postgres:5432/ragdb

# -------------------------------------------
# Serveur d'Embeddings Configuration
# -------------------------------------------
# URL du serveur d'embeddings
# Local: http://localhost:8001
# Docker: http://embeddings:8001
# Coolify: http://embeddings.internal:8001 ou https://embeddings.votredomaine.fr
EMBEDDINGS_API_URL=http://embeddings:8001

# Port d'exposition du serveur d'embeddings
EMBEDDINGS_PORT=8001

# Modèle d'embeddings à utiliser
EMBEDDING_MODEL=intfloat/multilingual-e5-large

# Dimension des vecteurs d'embeddings (doit correspondre au modèle)
# multilingual-e5-large = 1024
# multilingual-e5-base = 768
EMBEDDING_DIMENSION=1024

# -------------------------------------------
# Provider RAG Configuration
# -------------------------------------------
# Provider à utiliser: "chocolatine" (manuel) ou "mistral" (avec tools)
RAG_PROVIDER=chocolatine

# -------------------------------------------
# API LLM Chocolatine Configuration (provider manuel)
# -------------------------------------------
# URL de votre API LLM Chocolatine-2-14B
CHOCOLATINE_API_URL=https://apigpt.mynumih.fr

# Clé API Chocolatine (si nécessaire)
CHOCOLATINE_API_KEY=

# Nom du modèle (le nom complet tel qu'il apparaît dans l'API)
CHOCOLATINE_MODEL_NAME=jpacifico/Chocolatine-2-14B-Instruct-v2.0.3

# -------------------------------------------
# API Mistral Configuration (provider avec tools)
# -------------------------------------------
# Clé API Mistral (obtenir sur https://console.mistral.ai/api-keys)
MISTRAL_API_KEY=your_mistral_api_key_here

# URL de l'API Mistral
MISTRAL_API_URL=https://api.mistral.ai

# Nom du modèle Mistral à utiliser
# Options: open-mistral-7b, mistral-small-latest, mistral-medium-latest, mistral-large-latest
MISTRAL_MODEL_NAME=open-mistral-7b

# Timeout pour les requêtes Mistral (en secondes)
MISTRAL_TIMEOUT=120.0

# -------------------------------------------
# Ingestion Configuration
# -------------------------------------------
# Taille des chunks (en caractères)
CHUNK_SIZE=1000

# Chevauchement entre les chunks
CHUNK_OVERLAP=200

# Taille maximale d'un chunk
MAX_CHUNK_SIZE=2000

# Activer le chunking sémantique (true/false)
USE_SEMANTIC_CHUNKING=false

# -------------------------------------------
# Application Configuration
# -------------------------------------------
# Niveau de log (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Mode debug (true/false)
DEBUG_MODE=false

# -------------------------------------------
# Docker / Coolify Configuration
# -------------------------------------------
# Ces variables sont utilisées uniquement pour le déploiement

# Limites de ressources pour embeddings (CPU cores)
EMBEDDINGS_CPU_LIMIT=4
EMBEDDINGS_CPU_RESERVATION=2

# Limites de mémoire pour embeddings (en GB)
EMBEDDINGS_MEMORY_LIMIT=8
EMBEDDINGS_MEMORY_RESERVATION=4
