# ============================================
# Configuration RAGFab
# ============================================

# -------------------------------------------
# PostgreSQL Database Configuration
# -------------------------------------------
POSTGRES_USER=raguser
POSTGRES_PASSWORD=changeme_secure_password
POSTGRES_DB=ragdb
POSTGRES_PORT=5432

# Database URL pour l'application
# Format: postgresql://user:password@host:port/database
# Local: postgresql://raguser:changeme_secure_password@localhost:5432/ragdb
# Docker: postgresql://raguser:changeme_secure_password@postgres:5432/ragdb
# Coolify: postgresql://raguser:changeme_secure_password@postgres.internal:5432/ragdb
DATABASE_URL=postgresql://raguser:changeme_secure_password@postgres:5432/ragdb

# -------------------------------------------
# Serveur d'Embeddings Configuration
# -------------------------------------------
# URL du serveur d'embeddings
# Local: http://localhost:8001
# Docker: http://embeddings:8001
# Coolify: http://embeddings.internal:8001 ou https://embeddings.votredomaine.fr
EMBEDDINGS_API_URL=http://embeddings:8001

# Port d'exposition du serveur d'embeddings
EMBEDDINGS_PORT=8001

# Modèle d'embeddings à utiliser
EMBEDDING_MODEL=intfloat/multilingual-e5-large

# Dimension des vecteurs d'embeddings (doit correspondre au modèle)
# multilingual-e5-large = 1024
# multilingual-e5-base = 768
EMBEDDING_DIMENSION=1024

# -------------------------------------------
# Serveur de Reranking Configuration
# -------------------------------------------
# Activer/désactiver le reranking (true/false)
# false = recherche vectorielle directe (RAPIDE - DÉFAUT)
# true = vector search puis reranking pour affiner les résultats (PRÉCIS - activation manuelle via interface)
# Le reranking améliore +20-30% précision, latence additionnelle: +200-500ms
# RECOMMANDATION : Laisser false par défaut, activer manuellement via toggle "Recherche approfondie" pour questions complexes
RERANKER_ENABLED=false

# URL du serveur de reranking
# Local: http://localhost:8002
# Docker: http://reranker:8002
# Coolify: http://reranker.internal:8002
RERANKER_API_URL=http://reranker:8002

# Port d'exposition du serveur de reranking
RERANKER_PORT=8002

# Modèle de reranking à utiliser
# BAAI/bge-reranker-v2-m3 = multilingue, excellent pour le français
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# Nombre de chunks à récupérer AVANT reranking (si RERANKER_ENABLED=true)
# Plus élevé = plus de candidats pour le reranking, mais plus lent
# Recommandé: 20 (équilibre performance/qualité)
RERANKER_TOP_K=20

# Nombre de chunks à retourner APRÈS reranking (si RERANKER_ENABLED=true)
# C'est le nombre final de chunks envoyés au LLM
# Recommandé: 5 (suffisant pour la plupart des cas)
RERANKER_RETURN_K=5

# -------------------------------------------
# Contextual Retrieval Configuration
# -------------------------------------------
# Activer la récupération des chunks adjacents (prev/next) pour contexte enrichi
# true = Récupère les chunks précédents et suivants pour chaque résultat de recherche
# false = Retourne uniquement les chunks trouvés par la recherche vectorielle
# NOUVEAUTÉ (2025-01-24): Améliore la cohérence des réponses en incluant le contexte séquentiel
# Recommandé: true (latence additionnelle négligeable, pertinence +15-25%)
USE_ADJACENT_CHUNKS=true

# Activer le chunking hiérarchique parent-enfant
# true = Crée des chunks parents (large contexte 2000t) + enfants (précision 600t)
# false = Utilise le chunking standard (DoclingHybridChunker)
# NOUVEAUTÉ (2025-01-24): Architecture avancée pour meilleur équilibre précision/contexte
# Search dans enfants (précis) → Retourne parents (contexte riche)
# Recommandé: false par défaut (expérimental, nécessite migration 06)
USE_PARENT_CHILD_CHUNKS=false

# -------------------------------------------
# Provider RAG Configuration
# -------------------------------------------
# Provider à utiliser: "chocolatine" (manuel) ou "mistral" (avec tools)
# NOTE: Cette variable est deprecated, utilisez LLM_USE_TOOLS à la place
RAG_PROVIDER=chocolatine

# -------------------------------------------
# Generic LLM Configuration (RECOMMENDED)
# -------------------------------------------
# URL de l'API LLM (compatible OpenAI format)
# Exemples:
#   - Mistral: https://api.mistral.ai
#   - Chocolatine: https://apigpt.mynumih.fr
#   - Ollama: http://localhost:11434
#   - LiteLLM: http://localhost:4000
LLM_API_URL=https://api.mistral.ai

# Clé API LLM (laissez vide si pas nécessaire)
LLM_API_KEY=your_api_key_here

# Nom du modèle LLM
# Exemples:
#   - Mistral: mistral-small-latest, mistral-large-latest
#   - Chocolatine: jpacifico/Chocolatine-2-14B-Instruct-v2.0.3
#   - Ollama: llama3, mistral, codellama
LLM_MODEL_NAME=mistral-small-latest

# Activer function calling (true/false)
# true = Le LLM utilise les tools (search_knowledge_base_tool) automatiquement
# false = Injection manuelle du contexte dans le prompt
LLM_USE_TOOLS=true

# Timeout pour les requêtes LLM (en secondes)
LLM_TIMEOUT=120.0

# -------------------------------------------
# API LLM Chocolatine Configuration (LEGACY)
# -------------------------------------------
# NOTE: Ces variables sont deprecated, utilisez LLM_* à la place
# Conservées pour rétrocompatibilité

# URL de votre API LLM Chocolatine-2-14B
CHOCOLATINE_API_URL=https://apigpt.mynumih.fr

# Clé API Chocolatine (si nécessaire)
CHOCOLATINE_API_KEY=

# Nom du modèle (le nom complet tel qu'il apparaît dans l'API)
CHOCOLATINE_MODEL_NAME=jpacifico/Chocolatine-2-14B-Instruct-v2.0.3

# -------------------------------------------
# API Mistral Configuration (LEGACY)
# -------------------------------------------
# NOTE: Ces variables sont deprecated, utilisez LLM_* à la place
# Conservées pour rétrocompatibilité

# Clé API Mistral (obtenir sur https://console.mistral.ai/api-keys)
MISTRAL_API_KEY=your_mistral_api_key_here

# URL de l'API Mistral
MISTRAL_API_URL=https://api.mistral.ai

# Nom du modèle Mistral à utiliser
# Options: open-mistral-7b, mistral-small-latest, mistral-medium-latest, mistral-large-latest
MISTRAL_MODEL_NAME=mistral-small-latest

# Timeout pour les requêtes Mistral (en secondes)
MISTRAL_TIMEOUT=120.0

# -------------------------------------------
# Ingestion Configuration
# -------------------------------------------
# Taille des chunks (en caractères)
CHUNK_SIZE=1500

# Chevauchement entre les chunks (en caractères)
# Augmenté à 400 pour préserver la continuité sémantique entre chunks
# Particulièrement important pour petits documents (<3 pages)
# Évite la perte d'information aux frontières de chunks
CHUNK_OVERLAP=400

# Taille maximale d'un chunk
MAX_CHUNK_SIZE=2000

# Activer le chunking sémantique (true/false)
USE_SEMANTIC_CHUNKING=true

# -------------------------------------------
# Ingestion Worker Configuration
# -------------------------------------------
# Intervalle de polling du worker (en secondes)
WORKER_POLL_INTERVAL=3

# Timeout pour considérer un job comme bloqué (en minutes)
WORKER_TIMEOUT_MINUTES=30

# Répertoire de stockage des fichiers uploadés
UPLOAD_DIR=/app/uploads

# Taille maximale des fichiers uploadés (en MB)
MAX_UPLOAD_SIZE=100

# -------------------------------------------
# Web API & Frontend Configuration
# -------------------------------------------
# Port de l'API Backend
API_PORT=8000

# Port du Frontend
FRONTEND_PORT=3000

# URL de l'API Backend (utilisée par le frontend)
# Dev: http://localhost:8000
# Prod: https://api-rag.votredomaine.fr
VITE_API_URL=http://localhost:8000

# CORS: Origines autorisées (séparées par virgule)
# Exemple: https://rag.votredomaine.fr,https://api-rag.votredomaine.fr
# En dev, localhost est autorisé par défaut
CORS_ORIGINS=

# JWT Secret (à changer en production)
JWT_SECRET=your-secret-key-change-in-production

# Admin credentials
ADMIN_USERNAME=admin
ADMIN_PASSWORD=admin

# -------------------------------------------
# OCR Engine Configuration (Docling)
# -------------------------------------------
# Docling OCR Engine: Moteur OCR par défaut pour extraction de texte des PDFs
# Options:
#   - rapidocr: Basé sur PaddlePaddle, ~2x plus rapide qu'EasyOCR, multilingue (RECOMMANDÉ)
#   - easyocr: Défaut original de Docling, multilingue, plus lent mais robuste
#   - tesseract: Open-source référence, excellent pour scans haute qualité
# Note: Peut être surchargé par utilisateur à l'upload via interface admin (dropdown "Moteur OCR")
DOCLING_OCR_ENGINE=rapidocr

# -------------------------------------------
# VLM (Vision Language Model) Configuration
# -------------------------------------------
# Activer/désactiver l'extraction d'images avec VLM (true/false)
# true = Analyse images dans PDFs avec VLM (PaddleOCR-VL ou InternVL)
# false = Pas d'extraction d'images
VLM_ENABLED=false

# Moteur VLM par défaut pour extraction et analyse d'images
# Options:
#   - paddleocr-vl: Local, rapide (~1-3s/image), excellent OCR (RECOMMANDÉ pour documents techniques)
#   - internvl: API distant (~10-15s/image), descriptions riches (meilleur pour diagrammes complexes)
#   - none: Pas d'extraction d'images
# Note: Peut être surchargé par utilisateur à l'upload via interface admin (dropdown "Moteur VLM")
IMAGE_PROCESSOR_ENGINE=paddleocr-vl

# -------- PaddleOCR-VL Configuration (local) --------
# Activer accélération GPU pour PaddleOCR (true/false)
# true = Utilise GPU (nécessite paddlepaddle-gpu)
# false = Utilise CPU uniquement (plus lent mais pas de dépendances GPU)
PADDLEOCR_USE_GPU=false

# Langues OCR supportées (séparées par virgule)
# Exemples: fr (français), en (anglais), ch (chinois), de (allemand), es (espagnol)
# PaddleOCR supporte 109 langues au total
PADDLEOCR_LANG=fr,en

# Afficher les logs PaddleOCR (true/false)
# Recommandé: false (réduit le bruit dans les logs)
PADDLEOCR_SHOW_LOG=false

# -------- InternVL Configuration (API distant) --------
# URL de l'API VLM distante (FastAPI format)
# Exemple: https://apivlm.mynumih.fr (API Vision Générique avec InternVL3_5-8B)
# Format: Pas de suffix /v1, endpoints directs (/extract-and-describe, /describe-image, /extract-text)
VLM_API_URL=https://apivlm.mynumih.fr

# Clé API pour le service VLM (optionnelle, laisser vide si pas nécessaire)
VLM_API_KEY=

# Modèle VLM utilisé (informatif uniquement, configuré côté serveur)
# Modèle actuel: OpenGVLab/InternVL3_5-8B (optimisé pour documents)
VLM_MODEL_NAME=OpenGVLab/InternVL3_5-8B

# Timeout pour les requêtes VLM (en secondes)
# InternVL3_5-8B: ~10-15s par image
# PaddleOCR-VL: ~1-3s par image (local)
VLM_TIMEOUT=60.0

# Prompt pour l'analyse des images (utilisé par InternVL uniquement)
VLM_PROMPT=Décris cette image en détail en français. Extrais tout le texte visible.

# -------------------------------------------
# Image Processing Configuration
# -------------------------------------------
# Répertoire de stockage des images extraites
# /app/uploads/images dans Docker
IMAGE_STORAGE_PATH=/app/uploads/images

# Taille maximale des images à extraire (en MB)
IMAGE_MAX_SIZE_MB=10

# Qualité de compression JPEG pour les images (1-100)
IMAGE_QUALITY=85

# Format de sortie des images (png, jpeg)
IMAGE_OUTPUT_FORMAT=png

# Filtrage des petites images (icônes, logos, décorations)
# Largeur minimale pour conserver une image (en pixels)
IMAGE_MIN_WIDTH=200

# Hauteur minimale pour conserver une image (en pixels)
IMAGE_MIN_HEIGHT=200

# Surface minimale pour conserver une image (en pixels²)
# 40000 = équivalent à 200x200px
# Augmenter pour documents avec beaucoup d'icônes (ex: 60000 pour 250x250px)
IMAGE_MIN_AREA=40000

# Ratio d'aspect maximum (largeur/hauteur ou hauteur/largeur)
# Évite les bannières horizontales étroites et bordures verticales
# 10.0 = accepte jusqu'à 1000x100px ou 100x1000px
IMAGE_ASPECT_RATIO_MAX=10.0

# -------------------------------------------
# Application Configuration
# -------------------------------------------
# Niveau de log (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Mode debug (true/false)
DEBUG_MODE=false

# -------------------------------------------
# Database Migrations Configuration
# -------------------------------------------
# Activer l'application automatique des migrations SQL au démarrage
# true = Les migrations dans /database/migrations/ sont appliquées automatiquement
# false = Les migrations doivent être appliquées manuellement
# RECOMMANDÉ: true (simplifie le déploiement et les mises à jour)
AUTO_APPLY_MIGRATIONS=true

# -------------------------------------------
# Docker / Coolify Configuration
# -------------------------------------------
# Ces variables sont utilisées uniquement pour le déploiement

# Limites de ressources pour embeddings (CPU cores)
EMBEDDINGS_CPU_LIMIT=4
EMBEDDINGS_CPU_RESERVATION=2

# Limites de mémoire pour embeddings (en GB)
EMBEDDINGS_MEMORY_LIMIT=8
EMBEDDINGS_MEMORY_RESERVATION=4
