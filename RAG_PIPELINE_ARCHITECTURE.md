# Architecture Pipeline RAG - RAGFab

## Vue d'ensemble

RAGFab est un syst√®me RAG (Retrieval Augmented Generation) dual-provider optimis√© pour le fran√ßais, avec support d'images via VLM.

---

## üîÑ Pipeline d'Ingestion

### 1. Chargement de Documents

**Formats support√©s**: PDF, DOCX, MD, TXT, HTML
**Taille maximale**: 100 MB par fichier
**Stockage**: Volume partag√© `/app/uploads/{job_id}/`

### 2. Parsing de Documents

**Outil**: [Docling](https://github.com/DS4SD/docling) (IBM Research)
**Avantages**:
- Respect de la structure documentaire (titres, paragraphes, tableaux)
- Extraction automatique des images
- Support multi-format avec conversion PDF

### 3. Extraction d'Images (Optionnel)

**Activation**: `VLM_ENABLED=true`
**Mod√®le VLM**: OpenGVLab/InternVL3_5-8B (via API https://apivlm.mynumih.fr)
**Processus**:
1. Docling d√©tecte les images dans les pages PDF
2. Extraction + sauvegarde PNG (`/app/uploads/images/`)
3. Appel VLM API pour description + OCR
4. Stockage m√©tadonn√©es dans `document_images` table
5. Liaison avec chunks via `page_number`

**Performance**: ~10-15s par image

### 4. Chunking

**Strat√©gie principale**: Docling HybridChunker
**Param√®tres**:
- `CHUNK_SIZE=1500` tokens
- `CHUNK_OVERLAP=200` tokens

**Caract√©ristiques**:
- Respect des fronti√®res naturelles du document
- Pr√©servation du contexte s√©mantique
- Fallback automatique sur SimpleChunker en cas d'erreur

**Fallback**: SimpleChunker (split sur `\n\n`)

### 5. G√©n√©ration d'Embeddings

**Mod√®le**: `intfloat/multilingual-e5-large`
**Dimension**: 1024
**Service**: FastAPI d√©di√© (port 8001)
**Batch processing**: 20 chunks par batch
**Timeout**: 90 secondes

**Avantages du mod√®le**:
- Optimis√© multilingue (excellent fran√ßais)
- Dense retrieval performant
- Taille mod√©r√©e (1.1B param√®tres)

### 6. Stockage Vectoriel

**Base de donn√©es**: PostgreSQL + pgvector
**Index**: HNSW (Hierarchical Navigable Small World)
**Distance**: Cosine similarity

**Tables principales**:
- `documents`: M√©tadonn√©es des documents
- `chunks`: Texte + embeddings (vector(1024))
- `document_images`: Images + descriptions VLM

---

## üîç Pipeline d'Interrogation

### 1. Reformulation de Question (Chocolatine avec tools uniquement)

**D√©tection de r√©f√©rences contextuelles**:
- **R√©f√©rences fortes**: celle, celui, celles, ceux ‚Üí reformulation syst√©matique
- **R√©f√©rences moyennes**: √ßa, cela, ce, cette ‚Üí si question <8 mots
- **Pronoms en d√©but**: il, elle, ils, elles, y, en ‚Üí si premier mot
- **Patterns**: "et celle", "et celui", "et √ßa"

**Processus**:
1. D√©tection r√©f√©rence ‚Üí Appel Chocolatine API
2. Reformulation autonome avec contexte conversationnel
3. Question reformul√©e envoy√©e au RAG agent

### 2. Recherche Vectorielle

**Sans reranking** (`RERANKER_ENABLED=false`):
1. Question ‚Üí Embedding (E5-Large)
2. Recherche similarit√© cosinus
3. Top-5 chunks directs ‚Üí Contexte LLM

**Avec reranking** (`RERANKER_ENABLED=true`):
1. Question ‚Üí Embedding (E5-Large)
2. Recherche vectorielle ‚Üí Top-20 candidats
3. Reranking CrossEncoder ‚Üí Score pr√©cis
4. Top-5 apr√®s reranking ‚Üí Contexte LLM

### 3. Reranking (Optionnel)

**Mod√®le**: BAAI/bge-reranker-v2-m3
**Service**: FastAPI d√©di√© (port 8002)
**Configuration**:
- `RERANKER_TOP_K=20` (candidats avant reranking)
- `RERANKER_RETURN_K=5` (r√©sultats finaux)

**Cas d'usage recommand√©s**:
- Documentation technique/m√©dicale/juridique
- Terminologie similaire avec nuances s√©mantiques
- Base documentaire >1000 documents
- Besoin de pr√©cision maximale

**Performance**:
- Latence additionnelle: +100-300ms
- Ressources: ~4GB RAM
- Am√©lioration pertinence: +20-30%

**Fallback gracieux**: Si √©chec reranker ‚Üí Top-5 du vector search

### 4. G√©n√©ration de R√©ponse

#### Mode 1: LLM avec Function Calling (`LLM_USE_TOOLS=true`)

**LLM utilis√©**: Chocolatine-2-14B-Instruct-v2.0.3
**API**: Compatible OpenAI (vLLM proxy)
**Processus**:
1. Agent cr√©√© avec `tools=[search_knowledge_base_tool]`
2. **Pas d'historique pass√©** ‚Üí Force l'appel d'outil
3. LLM appelle automatiquement `search_knowledge_base_tool()`
4. Recherche vectorielle ex√©cut√©e
5. Sources stock√©es dans variable globale `_current_request_sources`
6. LLM g√©n√®re r√©ponse avec contexte
7. Sources affich√©es dans frontend

**Optimisation syst√®me prompt**:
- D√©finitions JSON explicites des outils dans le prompt
- Exemples d'utilisation correcte
- R√®gles absolues pour forcer l'appel d'outil
- Approche double: `tool_choice="required"` + JSON prompt

#### Mode 2: LLM sans tools (`LLM_USE_TOOLS=false`)

**Processus**:
1. Recherche vectorielle ex√©cut√©e **avant** cr√©ation agent
2. Contexte inject√© manuellement dans system prompt
3. Historique conversationnel peut √™tre inclus
4. LLM g√©n√®re r√©ponse sans appel d'outil

**Avantage**: Plus simple, compatible tout LLM
**Inconv√©nient**: Pas de recherche adaptative

---

## üõ†Ô∏è Configuration des Mod√®les

### Embeddings

```bash
EMBEDDINGS_API_URL=http://embeddings:8001
EMBEDDING_DIMENSION=1024
# Mod√®le: intfloat/multilingual-e5-large (1.1B param√®tres)
```

### Reranker (Optionnel)

```bash
RERANKER_ENABLED=false  # true pour activer
RERANKER_API_URL=http://reranker:8002
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_TOP_K=20
RERANKER_RETURN_K=5
```

### LLM Principal

```bash
LLM_API_URL=https://apigpt.mynumih.fr
LLM_API_KEY=your_api_key
LLM_MODEL_NAME=Chocolatine-2-14B-Instruct-v2.0.3  # vLLM proxy
LLM_USE_TOOLS=true  # Function calling activ√©
LLM_TIMEOUT=120.0
```

**Caract√©ristiques Chocolatine**:
- Mod√®le: 14B param√®tres, optimis√© pour le fran√ßais
- Architecture: Compatible OpenAI API via vLLM
- Function calling: Support natif des outils
- Context window: 8K tokens

### VLM (Vision Language Model)

```bash
VLM_ENABLED=false  # true pour extraction d'images
VLM_API_URL=https://apivlm.mynumih.fr
VLM_MODEL_NAME=OpenGVLab/InternVL3_5-8B
VLM_TIMEOUT=60.0
```

---

## üìä Workflow Complet

### Ingestion (Asynchrone via Worker)

```
Upload PDF ‚Üí Validation (type, taille)
  ‚Üì
Job cr√©√© (status='pending')
  ‚Üì
Worker claim job (polling 3s)
  ‚Üì
Docling parsing ‚Üí Extraction structure + images
  ‚Üì
HybridChunker (1500 tokens, overlap 200)
  ‚Üì
Batch embeddings (20 chunks, E5-Large 1024d)
  ‚Üì
Stockage PostgreSQL (chunks + embeddings)
  ‚Üì
Si VLM activ√©: Images ‚Üí API VLM ‚Üí Description + OCR ‚Üí DB
  ‚Üì
Job completed (status='completed')
```

### Interrogation (Temps r√©el)

#### Avec Function Calling (Chocolatine)

```
Question utilisateur
  ‚Üì
D√©tection r√©f√©rence contextuelle? ‚Üí OUI ‚Üí Reformulation Chocolatine
  ‚Üì
Agent cr√©√© (tools, NO history)
  ‚Üì
LLM appelle search_knowledge_base_tool()
  ‚Üì
Embedding question (E5-Large)
  ‚Üì
Recherche vectorielle (cosine similarity)
  ‚Üì
Si reranker: Top-20 ‚Üí CrossEncoder ‚Üí Top-5
Sinon: Top-5 direct
  ‚Üì
Sources ‚Üí Variable globale _current_request_sources
  ‚Üì
LLM g√©n√®re r√©ponse finale avec contexte
  ‚Üì
R√©ponse + Sources + Images ‚Üí Frontend
```

#### Sans Function Calling

```
Question utilisateur
  ‚Üì
Recherche vectorielle imm√©diate
  ‚Üì
Contexte inject√© dans system prompt
  ‚Üì
Agent cr√©√© (avec historique possible)
  ‚Üì
LLM g√©n√®re r√©ponse
  ‚Üì
R√©ponse + Sources ‚Üí Frontend
```

---

## üéØ Points Techniques Critiques

### PydanticAI & Function Calling

- **Ne jamais passer d'historique** quand on veut forcer tool calling
- Utiliser `run()` pas `run_stream()` pour execution automatique des tools
- Variable globale `_current_request_sources` n√©cessaire (ContextVar perdu en async)

### Chunking

- HybridChunker respecte structure s√©mantique (pas de coupure arbitraire)
- Overlap de 200 tokens pour pr√©server contexte entre chunks
- Fallback SimpleChunker sur `\n\n` (pas sur caract√®res)

### UTF-8 Handling

- PDFs contiennent souvent caract√®res invalides
- Nettoyage syst√©matique: `.encode('utf-8', errors='replace').decode('utf-8')`

### Base de donn√©es

- Index HNSW pour recherche vectorielle rapide
- Dimension fixe: 1024 (changement = recr√©ation table)
- Images li√©es aux chunks via `page_number`

---

## üìà Performance

| Composant | Latence | Ressources |
|-----------|---------|------------|
| Embedding (1 chunk) | ~50ms | 4-8GB RAM |
| Vector search | ~10-50ms | Index HNSW |
| Reranker (20‚Üí5) | +100-300ms | ~4GB RAM |
| VLM (1 image) | ~10-15s | API externe |
| LLM g√©n√©ration | 2-10s | API externe |

**Total requ√™te RAG**:
- Sans reranking: ~2-10s
- Avec reranking: ~2.5-11s
- Avec images: Pr√©-calcul√© (pas d'impact runtime)

---

## üîê S√©curit√© & Multi-utilisateur

- JWT authentication sur toutes routes frontend
- Conversations isol√©es par `user_id`
- Admin panel (RBAC) pour gestion utilisateurs
- Mot de passe obligatoire au premier login
- Validation force mot de passe (8 chars, uppercase, lowercase, digit)

---

## üöÄ D√©ploiement

**Architecture**: Docker Compose + Coolify
**Services**:
- PostgreSQL + pgvector
- Embeddings API (E5-Large)
- Reranker API (BGE-M3) - optionnel
- Web API (FastAPI)
- Frontend (React + Vite)
- Ingestion Worker (background processing)

**R√©seau**: Traefik reverse proxy avec labels pour routing automatique



Matraquer qu'il faut des moyens, pour industrialiser et r√©pondre √† la conformit√© et la qualit√©
Comment tu arrives √† ca , dans d'autres usages


curl -X POST https://backend-nava.lab-numihfrance.fr/api/reports/test-email \
    -H "Content-Type: application/json" \
    -d '{"to":"famatulli@gmail.com"}'
