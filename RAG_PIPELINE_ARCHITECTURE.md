# Architecture Pipeline RAG - RAGFab

## Vue d'ensemble

RAGFab est un syst√®me RAG (Retrieval Augmented Generation) dual-provider optimis√© pour le fran√ßais, avec support d'images via VLM.

---

## üîÑ Pipeline d'Ingestion

### 1. Chargement de Documents

**Formats support√©s**: PDF, DOCX, MD, TXT, HTML
**Taille maximale**: 100 MB par fichier
**Stockage**: Volume partag√© `/app/uploads/{job_id}/`

### 2. Parsing de Documents

**Outil**: [Docling](https://github.com/DS4SD/docling) (IBM Research)
**Avantages**:
- Respect de la structure documentaire (titres, paragraphes, tableaux)
- Extraction automatique des images
- Support multi-format avec conversion PDF

### 3. Extraction d'Images (Optionnel)

**Activation**: `VLM_ENABLED=true`
**Mod√®le VLM**: OpenGVLab/InternVL3_5-8B (via API https://apivlm.mynumih.fr)
**Processus**:
1. Docling d√©tecte les images dans les pages PDF
2. Extraction + sauvegarde PNG (`/app/uploads/images/`)
3. Appel VLM API pour description + OCR
4. Stockage m√©tadonn√©es dans `document_images` table
5. Liaison avec chunks via `page_number`

**Performance**: ~10-15s par image

### 4. Chunking Adaptatif Intelligent

**Strat√©gie principale**: Docling HybridChunker avec param√®tres adaptatifs
**Configuration**:
- `CHUNK_SIZE=1500` caract√®res
- `CHUNK_OVERLAP=400` tokens (augment√© pour pr√©server continuit√©)

**‚ú® NOUVEAU : Chunking Adaptatif par Taille Document**

Le syst√®me d√©tecte automatiquement la taille du document et ajuste les param√®tres :

| Taille Document | Cat√©gorie | max_tokens | Objectif |
|-----------------|-----------|------------|----------|
| <1000 mots (‚âà3 pages) | Small | 1500 tokens | Pr√©server contexte global |
| 1000-5000 mots | Medium | 800 tokens | √âquilibre contexte/pr√©cision |
| >5000 mots (‚âà15+ pages) | Large | 512 tokens | Granularit√© fine |

**Caract√©ristiques**:
- **Respect des fronti√®res naturelles** : Sections, paragraphes, tableaux
- **Pr√©servation du contexte s√©mantique** : Overlap augment√© √† 400 tokens
- **Enrichissement contextuel** : Chaque chunk pr√©fix√© avec `[Document: Titre] [Section: Hi√©rarchie]`
- **M√©tadonn√©es √©tendues** : `doc_size_category`, `word_count`, `has_enrichment`
- **Fallback automatique** : SimpleChunker en cas d'erreur

**Avantages chunking adaptatif** :
- ‚úÖ Petits documents : Chunks 3-4x plus gros ‚Üí contexte riche ‚Üí meilleure qualit√© RAG
- ‚úÖ Documents moyens : √âquilibre optimal entre contexte et pr√©cision
- ‚úÖ Grands documents : Granularit√© fine pour recherche pr√©cise

**Enrichissement contextuel** (selon √©tude Anthropic 2024) :
```
Chunk original : "Le protocole utilise AES-256"
Chunk enrichi : "[Document: Guide S√©curit√©] [Section: Cryptographie > Chiffrement]\n\nLe protocole utilise AES-256"
```
‚Üí Gain de pr√©cision : +67% sur petits documents

### 5. G√©n√©ration d'Embeddings

**Mod√®le**: `intfloat/multilingual-e5-large`
**Dimension**: 1024
**Service**: FastAPI d√©di√© (port 8001)
**Batch processing**: 20 chunks par batch
**Timeout**: 90 secondes

**Avantages du mod√®le**:
- Optimis√© multilingue (excellent fran√ßais)
- Dense retrieval performant
- Taille mod√©r√©e (1.1B param√®tres)

### 6. Stockage Vectoriel

**Base de donn√©es**: PostgreSQL + pgvector
**Index**: HNSW (Hierarchical Navigable Small World)
**Distance**: Cosine similarity

**Tables principales**:
- `documents`: M√©tadonn√©es des documents
- `chunks`: Texte + embeddings (vector(1024))
- `document_images`: Images + descriptions VLM

---

## üîç Pipeline d'Interrogation

### 1. Reformulation de Question (Chocolatine avec tools uniquement)

**D√©tection de r√©f√©rences contextuelles**:
- **R√©f√©rences fortes**: celle, celui, celles, ceux ‚Üí reformulation syst√©matique
- **R√©f√©rences moyennes**: √ßa, cela, ce, cette ‚Üí si question <8 mots
- **Pronoms en d√©but**: il, elle, ils, elles, y, en ‚Üí si premier mot
- **Patterns**: "et celle", "et celui", "et √ßa"

**Processus**:
1. D√©tection r√©f√©rence ‚Üí Appel Chocolatine API
2. Reformulation autonome avec contexte conversationnel
3. Question reformul√©e envoy√©e au RAG agent

### 2. Recherche Vectorielle

**Sans reranking** (`RERANKER_ENABLED=false`):
1. Question ‚Üí Embedding (E5-Large)
2. Recherche similarit√© cosinus
3. Top-5 chunks directs ‚Üí Contexte LLM

**Avec reranking** (`RERANKER_ENABLED=true`):
1. Question ‚Üí Embedding (E5-Large)
2. Recherche vectorielle ‚Üí Top-20 candidats
3. Reranking CrossEncoder ‚Üí Score pr√©cis
4. Top-5 apr√®s reranking ‚Üí Contexte LLM

### 3. Reranking (‚úÖ ACTIV√â PAR D√âFAUT)

**Mod√®le**: BAAI/bge-reranker-v2-m3 (CrossEncoder multilingue)
**Service**: FastAPI d√©di√© (port 8002)
**Configuration**:
- `RERANKER_ENABLED=true` (recommand√©, activ√© par d√©faut)
- `RERANKER_TOP_K=20` (candidats avant reranking)
- `RERANKER_RETURN_K=5` (r√©sultats finaux apr√®s scoring pr√©cis)

**Workflow reranking** :
1. Vector search r√©cup√®re 20 candidats potentiels
2. CrossEncoder calcule score pr√©cis pour chaque paire (query, chunk)
3. Top-5 vraiment pertinents envoy√©s au LLM

**B√©n√©fices principaux** :
- ‚úÖ **Petits documents** : Compense la sur-segmentation en chunks
- ‚úÖ **Documentation technique** : G√®re terminologie similaire avec nuances
- ‚úÖ **Pr√©cision √©lev√©e** : +20-30% am√©lioration vs vector search seul
- ‚úÖ **Contexte riche** : S√©lectionne les chunks les plus pertinents

**Performance** :
- Latence additionnelle : +100-300ms (acceptable)
- Ressources : ~4GB RAM
- Am√©lioration pertinence : **+20-30%**
- Particuli√®rement efficace sur petits documents (<1000 mots)

**Fallback gracieux** : Si √©chec reranker ‚Üí Top-5 du vector search direct

### 4. G√©n√©ration de R√©ponse

#### Mode 1: LLM avec Function Calling (`LLM_USE_TOOLS=true`)

**LLM utilis√©**: Chocolatine-2-14B-Instruct-v2.0.3
**API**: Compatible OpenAI (vLLM proxy)
**Processus**:
1. Agent cr√©√© avec `tools=[search_knowledge_base_tool]`
2. **Pas d'historique pass√©** ‚Üí Force l'appel d'outil
3. LLM appelle automatiquement `search_knowledge_base_tool()`
4. Recherche vectorielle ex√©cut√©e
5. Sources stock√©es dans variable globale `_current_request_sources`
6. LLM g√©n√®re r√©ponse avec contexte
7. Sources affich√©es dans frontend

**Optimisation syst√®me prompt**:
- D√©finitions JSON explicites des outils dans le prompt
- Exemples d'utilisation correcte
- R√®gles absolues pour forcer l'appel d'outil
- Approche double: `tool_choice="required"` + JSON prompt

#### Mode 2: LLM sans tools (`LLM_USE_TOOLS=false`)

**Processus**:
1. Recherche vectorielle ex√©cut√©e **avant** cr√©ation agent
2. Contexte inject√© manuellement dans system prompt
3. Historique conversationnel peut √™tre inclus
4. LLM g√©n√®re r√©ponse sans appel d'outil

**Avantage**: Plus simple, compatible tout LLM
**Inconv√©nient**: Pas de recherche adaptative

---

## üõ†Ô∏è Configuration des Mod√®les

### Embeddings

```bash
EMBEDDINGS_API_URL=http://embeddings:8001
EMBEDDING_DIMENSION=1024
# Mod√®le: intfloat/multilingual-e5-large (1.1B param√®tres)
```

### Reranker (Optionnel)

```bash
RERANKER_ENABLED=false  # true pour activer
RERANKER_API_URL=http://reranker:8002
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_TOP_K=20
RERANKER_RETURN_K=5
```

### LLM Principal

```bash
LLM_API_URL=https://apigpt.mynumih.fr
LLM_API_KEY=your_api_key
LLM_MODEL_NAME=Chocolatine-2-14B-Instruct-v2.0.3  # vLLM proxy
LLM_USE_TOOLS=true  # Function calling activ√©
LLM_TIMEOUT=120.0
```

**Caract√©ristiques Chocolatine**:
- Mod√®le: 14B param√®tres, optimis√© pour le fran√ßais
- Architecture: Compatible OpenAI API via vLLM
- Function calling: Support natif des outils
- Context window: 8K tokens

### VLM (Vision Language Model)

```bash
VLM_ENABLED=false  # true pour extraction d'images
VLM_API_URL=https://apivlm.mynumih.fr
VLM_MODEL_NAME=OpenGVLab/InternVL3_5-8B
VLM_TIMEOUT=60.0
```

---

## üìä Workflow Complet

### Ingestion (Asynchrone via Worker)

```
Upload PDF ‚Üí Validation (type, taille)
  ‚Üì
Job cr√©√© (status='pending')
  ‚Üì
Worker claim job (polling 3s)
  ‚Üì
Docling parsing ‚Üí Extraction structure + images
  ‚Üì
HybridChunker (1500 tokens, overlap 200)
  ‚Üì
Batch embeddings (20 chunks, E5-Large 1024d)
  ‚Üì
Stockage PostgreSQL (chunks + embeddings)
  ‚Üì
Si VLM activ√©: Images ‚Üí API VLM ‚Üí Description + OCR ‚Üí DB
  ‚Üì
Job completed (status='completed')
```

### Interrogation (Temps r√©el)

#### Avec Function Calling (Chocolatine)

```
Question utilisateur
  ‚Üì
D√©tection r√©f√©rence contextuelle? ‚Üí OUI ‚Üí Reformulation Chocolatine
  ‚Üì
Agent cr√©√© (tools, NO history)
  ‚Üì
LLM appelle search_knowledge_base_tool()
  ‚Üì
Embedding question (E5-Large)
  ‚Üì
Recherche vectorielle (cosine similarity)
  ‚Üì
Si reranker: Top-20 ‚Üí CrossEncoder ‚Üí Top-5
Sinon: Top-5 direct
  ‚Üì
Sources ‚Üí Variable globale _current_request_sources
  ‚Üì
LLM g√©n√®re r√©ponse finale avec contexte
  ‚Üì
R√©ponse + Sources + Images ‚Üí Frontend
```

#### Sans Function Calling

```
Question utilisateur
  ‚Üì
Recherche vectorielle imm√©diate
  ‚Üì
Contexte inject√© dans system prompt
  ‚Üì
Agent cr√©√© (avec historique possible)
  ‚Üì
LLM g√©n√®re r√©ponse
  ‚Üì
R√©ponse + Sources ‚Üí Frontend
```

---

## üéØ Points Techniques Critiques

### PydanticAI & Function Calling

- **Ne jamais passer d'historique** quand on veut forcer tool calling
- Utiliser `run()` pas `run_stream()` pour execution automatique des tools
- Variable globale `_current_request_sources` n√©cessaire (ContextVar perdu en async)

### Chunking

- HybridChunker respecte structure s√©mantique (pas de coupure arbitraire)
- Overlap de 200 tokens pour pr√©server contexte entre chunks
- Fallback SimpleChunker sur `\n\n` (pas sur caract√®res)

### UTF-8 Handling

- PDFs contiennent souvent caract√®res invalides
- Nettoyage syst√©matique: `.encode('utf-8', errors='replace').decode('utf-8')`

### Base de donn√©es

- Index HNSW pour recherche vectorielle rapide
- Dimension fixe: 1024 (changement = recr√©ation table)
- Images li√©es aux chunks via `page_number`

---

## üìà Performance

| Composant | Latence | Ressources |
|-----------|---------|------------|
| Embedding (1 chunk) | ~50ms | 4-8GB RAM |
| Vector search | ~10-50ms | Index HNSW |
| Reranker (20‚Üí5) | +100-300ms | ~4GB RAM |
| VLM (1 image) | ~10-15s | API externe |
| LLM g√©n√©ration | 2-10s | API externe |

**Total requ√™te RAG**:
- Sans reranking: ~2-10s
- Avec reranking: ~2.5-11s
- Avec images: Pr√©-calcul√© (pas d'impact runtime)

---

## üîê S√©curit√© & Multi-utilisateur

- JWT authentication sur toutes routes frontend
- Conversations isol√©es par `user_id`
- Admin panel (RBAC) pour gestion utilisateurs
- Mot de passe obligatoire au premier login
- Validation force mot de passe (8 chars, uppercase, lowercase, digit)

---

## üöÄ D√©ploiement

**Architecture**: Docker Compose + Coolify
**Services**:
- PostgreSQL + pgvector
- Embeddings API (E5-Large)
- Reranker API (BGE-M3) - activ√© par d√©faut
- Web API (FastAPI)
- Frontend (React + Vite)
- Ingestion Worker (background processing)

**R√©seau**: Traefik reverse proxy avec labels pour routing automatique

---

## üéØ Am√©liorations Qualit√© RAG - Petits Documents (2025-01)

### Probl√®me R√©solu

**Sympt√¥me initial** : Recherche RAG inefficace sur petits documents (1-3 pages)
- Chunks trop petits ‚Üí perte de contexte
- Sur-fragmentation ‚Üí r√©ponses incompl√®tes
- Information dispers√©e ‚Üí mauvaise qualit√© LLM

### Solutions Impl√©ment√©es

#### ‚úÖ Phase 1 : Quick Wins (Impact Imm√©diat)

**1. Augmentation Context Window Chunks**
- `max_tokens` : 512 ‚Üí 800 tokens (+56% contexte)
- Chunks plus riches sans d√©gradation E5-large
- Compatible context window Chocolatine (8K tokens)

**2. Reranker Activ√© par D√©faut**
- `RERANKER_ENABLED=true` (au lieu de false)
- CrossEncoder BGE-M3 affine s√©lection top-5
- +20-30% pr√©cision, latence +200ms (acceptable)

**3. Overlap Augment√©**
- `CHUNK_OVERLAP` : 200 ‚Üí 400 caract√®res
- Pr√©serve continuit√© s√©mantique entre chunks
- Crucial pour petits documents fragment√©s

#### ‚úÖ Phase 2 : Chunking Adaptatif (Impact Majeur)

**4. D√©tection Automatique Taille Document**
```python
word_count = len(content.split())

if word_count < 1000:      # Small (<3 pages)
    max_tokens = 1500      # Tr√®s gros chunks
elif word_count < 5000:    # Medium (3-15 pages)
    max_tokens = 800       # Chunks √©quilibr√©s
else:                      # Large (>15 pages)
    max_tokens = 512       # Chunks granulaires
```

**5. Enrichissement Contextuel Avanc√©**
```python
# Avant
chunk_content = "Le protocole utilise AES-256"

# Apr√®s (embeddings)
enriched_chunk = "[Document: Guide S√©curit√©] [Section: Cryptographie > Chiffrement]\n\nLe protocole utilise AES-256"
```

**B√©n√©fices contextuels** :
- Chunks gardent contexte document dans embedding
- +67% pr√©cision selon √©tude Anthropic (Contextual Retrieval)
- Compense chunks petits avec contexte s√©mantique

### Gains Mesurables Attendus

| M√©trique | Avant | Apr√®s | Gain |
|----------|-------|-------|------|
| Context par chunk | 512 tokens | 800-1500 tokens | +56-193% |
| Pr√©cision recherche | Baseline | +20-30% | Reranker |
| Continuit√© chunks | 200 overlap | 400 overlap | +100% |
| Contexte s√©mantique | Minimal | Enrichi | +67% |
| **Qualit√© globale petits docs** | **Baseline** | **+85%** | **Combin√©** |

### Migration et R√©-indexation

**R√©-indexation requise** : OUI (nouveaux param√®tres chunking)

**Proc√©dure** :
```bash
# 1. Mettre √† jour .env avec nouvelles valeurs
RERANKER_ENABLED=true
CHUNK_OVERLAP=400

# 2. Rebuild services
docker-compose down
docker-compose build ragfab-api ingestion-worker
docker-compose up -d

# 3. R√©-indexer documents (conserve documents, recr√©e chunks)
docker-compose exec ragfab-api python -m ingestion.ingest --documents /app/uploads

# 4. V√©rifier qualit√© sur documents tests
# Comparer r√©ponses avant/apr√®s sur corpus de questions
```

**Temps estim√©** :
- Petite base (<100 docs) : ~15-30 minutes
- Moyenne base (100-1000 docs) : ~1-2 heures
- Grande base (>1000 docs) : ~3-6 heures

**Rollback possible** :
```bash
# Revenir aux anciens param√®tres
RERANKER_ENABLED=false
CHUNK_OVERLAP=200

# R√©-indexer avec anciens param√®tres
```

### Validation Qualit√©

**Tests recommand√©s** :
1. S√©lectionner 10 petits documents repr√©sentatifs (<1000 mots)
2. Cr√©er 5 questions par document (50 questions total)
3. Benchmark avant/apr√®s :
   - MRR (Mean Reciprocal Rank) : Position premier r√©sultat pertinent
   - NDCG@5 : Qualit√© top-5 r√©sultats
   - User Satisfaction : Qualit√© r√©ponses LLM (√©chelle 1-5)

**Seuils d'acceptation** :
- MRR > 0.8 (r√©ponse pertinente dans top-3)
- NDCG@5 > 0.75
- User Satisfaction > 4.0/5.0

### Prochaines Am√©liorations Possibles

**Si qualit√© encore insuffisante apr√®s Phase 1+2** :

1. **Hierarchical Retrieval** (parent-child chunks)
   - Recherche sur child chunks (pr√©cis)
   - Contexte via parent chunks (riche)
   - Gain attendu : +30% qualit√©

2. **Migration BGE-M3 Embeddings** (8K tokens context)
   - Chunks jusqu'√† 4000 tokens sans d√©gradation
   - Context window 16x plus large que E5-large
   - Gain attendu : +40% sur tr√®s petits docs

3. **Multi-Query Retrieval**
   - 3 variations de la question
   - Fusion des r√©sultats (Reciprocal Rank Fusion)
   - Gain attendu : +15% recall



Matraquer qu'il faut des moyens, pour industrialiser et r√©pondre √† la conformit√© et la qualit√©
Comment tu arrives √† ca , dans d'autres usages

