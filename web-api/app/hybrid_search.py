"""
Module pour Hybrid Search (BM25 + Vector) avec support fran√ßais
Combine recherche s√©mantique (embeddings) et recherche par mots-cl√©s (full-text)
"""
import re
import logging
from typing import List, Dict, Optional
from uuid import UUID

from . import database

logger = logging.getLogger(__name__)

# Stopwords fran√ßais pour preprocessing des requ√™tes
FRENCH_STOPWORDS = {
    "le", "la", "les", "l", "un", "une", "des",
    "de", "du", "d", "des",
    "√†", "au", "aux",
    "et", "ou", "mais", "donc", "or", "ni", "car",
    "ce", "cet", "cette", "ces",
    "mon", "ton", "son", "ma", "ta", "sa", "mes", "tes", "ses",
    "notre", "votre", "leur", "nos", "vos", "leurs",
    "je", "tu", "il", "elle", "on", "nous", "vous", "ils", "elles",
    "me", "te", "se", "lui",
    "qui", "que", "quoi", "dont", "o√π",
    "dans", "par", "pour", "sur", "avec", "sans", "sous",
    "√™tre", "avoir", "faire", "dire", "aller", "voir", "pouvoir", "vouloir"
}


def preprocess_query_for_tsquery(query: str) -> str:
    """
    Convertit une requ√™te utilisateur en format PostgreSQL tsquery pour recherche full-text.

    Transformations appliqu√©es:
    - Suppression des stopwords fran√ßais (le, la, de, etc.)
    - Nettoyage caract√®res sp√©ciaux (pr√©serve - et ')
    - Jointure avec '&' (AND operator)
    - Gestion des acronymes (pr√©servation)

    Exemples:
        "Quelle est la politique de t√©l√©travail ?"
        ‚Üí "politique & t√©l√©travail"

        "proc√©dure RTT cong√©s pay√©s"
        ‚Üí "proc√©dure & RTT & cong√©s & pay√©s"

        "l'entreprise et les employ√©s"
        ‚Üí "entreprise & employ√©s"

    Args:
        query: Question utilisateur en fran√ßais

    Returns:
        Requ√™te format√©e pour to_tsquery('french', ...)
    """
    # Convertir en minuscules
    query_lower = query.lower()

    # Remplacer apostrophes typographiques par apostrophe standard
    query_lower = query_lower.replace("'", "'").replace("'", "'")

    # Nettoyer caract√®res sp√©ciaux (sauf tirets et apostrophes)
    query_lower = re.sub(r"[^\w\s'-]", " ", query_lower)

    # Tokeniser
    tokens = query_lower.split()

    # Filtrer stopwords ET tokens vides
    filtered = []
    for token in tokens:
        # Nettoyer le token
        token = token.strip("'-")

        # Garder si:
        # 1. Pas un stopword
        # 2. OU est un acronyme (2+ lettres majuscules cons√©cutives dans original)
        # 3. ET token non vide
        is_acronym = bool(re.search(r'\b[A-Z]{2,}\b', token.upper()))

        if token and (token not in FRENCH_STOPWORDS or is_acronym):
            filtered.append(token)

    # Si tous les tokens sont filtr√©s, retourner au moins 1 token
    if not filtered and tokens:
        # Prendre le token le plus long (probablement le plus significatif)
        filtered = [max(tokens, key=len).strip("'-")]

    # Joindre avec '&' (AND operator PostgreSQL)
    result = " & ".join(filtered) if filtered else ""

    # Log pour debugging
    if result != query_lower:
        logger.debug(f"Query preprocessing: '{query}' ‚Üí '{result}'")

    return result


def adaptive_alpha(query: str) -> float:
    """
    Ajuste dynamiquement le poids alpha selon le type de question.

    Alpha = 0.0 : 100% keyword search (BM25)
    Alpha = 0.5 : √âquilibr√© (50% vector, 50% keyword)
    Alpha = 1.0 : 100% vector search (semantic)

    Strat√©gies:
    - Acronymes (RTT, CDI) ‚Üí alpha=0.3 (privil√©gie keyword)
    - Noms propres (PeopleDoc) ‚Üí alpha=0.3 (privil√©gie keyword)
    - Questions conceptuelles (pourquoi, comment) ‚Üí alpha=0.7 (privil√©gie s√©mantique)
    - Par d√©faut ‚Üí alpha=0.5 (√©quilibr√©)

    Args:
        query: Question utilisateur

    Returns:
        Alpha optimal (0.0 √† 1.0)
    """
    query_lower = query.lower()

    # D√©tecter acronymes (2+ lettres majuscules cons√©cutives)
    if re.search(r'\b[A-Z]{2,}\b', query):
        logger.debug(f"Acronyme d√©tect√©, alpha=0.3 (keyword bias)")
        return 0.3

    # D√©tecter noms propres (mots avec majuscule apr√®s le premier mot)
    words = query.split()
    if len(words) > 1:
        proper_nouns = [w for w in words[1:] if w and w[0].isupper()]
        if proper_nouns:
            logger.debug(f"Nom propre d√©tect√© ({proper_nouns}), alpha=0.3 (keyword bias)")
            return 0.3

    # Questions conceptuelles
    conceptual_keywords = [
        "pourquoi", "comment", "expliquer", "signifie", "d√©finition",
        "diff√©rence", "comparer", "avantage", "inconv√©nient", "principe"
    ]
    if any(keyword in query_lower for keyword in conceptual_keywords):
        logger.debug(f"Question conceptuelle d√©tect√©e, alpha=0.7 (semantic bias)")
        return 0.7

    # Questions tr√®s courtes (<5 mots) souvent recherchent terme exact
    if len(words) <= 4:
        logger.debug(f"Question courte ({len(words)} mots), alpha=0.4 (l√©ger keyword bias)")
        return 0.4

    # Par d√©faut: √©quilibr√©
    logger.debug(f"Alpha par d√©faut=0.5 (√©quilibr√©)")
    return 0.5


async def hybrid_search(
    query: str,
    query_embedding: List[float],
    k: int = 5,
    alpha: Optional[float] = None,
    use_adaptive_alpha: bool = True,
    use_hierarchical: bool = False
) -> List[Dict]:
    """
    Recherche hybride combinant similarit√© vectorielle (E5-Large) et mots-cl√©s (BM25).

    Utilise la fonction PostgreSQL match_chunks_hybrid() qui impl√©mente:
    - Vector search via pgvector (cosine similarity)
    - Keyword search via tsvector + GIN index (BM25-like)
    - RRF (Reciprocal Rank Fusion) pour combiner les r√©sultats

    Args:
        query: Question utilisateur en fran√ßais
        query_embedding: Embedding de la question (vecteur 1024 dimensions)
        k: Nombre de r√©sultats √† retourner (d√©faut: 5)
        alpha: Poids entre vector (1.0) et keyword (0.0). Si None, utilise adaptive
        use_adaptive_alpha: Si True et alpha=None, calcule alpha optimal automatiquement
        use_hierarchical: Si True, cherche dans chunks enfants et retourne parents

    Returns:
        Liste de chunks avec scores (similarity, bm25_score, combined_score) et m√©tadonn√©es compl√®tes

    Raises:
        Exception: Si erreur PostgreSQL ou embedding invalide
    """
    # D√©terminer alpha optimal
    if alpha is None:
        if use_adaptive_alpha:
            alpha = adaptive_alpha(query)
        else:
            alpha = 0.5  # D√©faut √©quilibr√©

    # Valider alpha range
    alpha = max(0.0, min(1.0, alpha))

    # Pr√©processer query pour full-text search
    processed_query = preprocess_query_for_tsquery(query)

    # Si query vide apr√®s preprocessing, fallback sur query originale
    if not processed_query:
        processed_query = query.lower()
        logger.warning(f"‚ö†Ô∏è Query vide apr√®s preprocessing, fallback sur query originale")

    logger.info(f"üîÄ Hybrid search: query='{query}' ‚Üí tsquery='{processed_query}', alpha={alpha:.2f}, k={k}")

    # Convertir embedding liste Python en cha√Æne PostgreSQL vector
    embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"

    try:
        async with database.db_pool.acquire() as conn:
            # Appeler fonction PostgreSQL match_chunks_hybrid
            results = await conn.fetch("""
                SELECT
                    id,
                    content,
                    similarity,
                    bm25_score,
                    combined_score,
                    metadata,
                    document_id,
                    chunk_index,
                    prev_chunk_id,
                    next_chunk_id,
                    section_hierarchy,
                    heading_context,
                    document_position,
                    chunk_level,
                    parent_chunk_id
                FROM match_chunks_hybrid($1::vector, $2, $3, $4, $5)
            """,
            embedding_str,     # $1: query_embedding vector(1024) as string
            processed_query,   # $2: query_text preprocessed
            k,                 # $3: match_count
            alpha,             # $4: alpha weight
            use_hierarchical   # $5: use_hierarchical boolean
            )

        # Formater r√©sultats
        formatted_results = []
        for row in results:
            formatted_results.append({
                "chunk_id": str(row["id"]),
                "content": row["content"],
                "scores": {
                    "vector_similarity": float(row["similarity"]),
                    "bm25_score": float(row["bm25_score"]),
                    "combined_score": float(row["combined_score"]),
                    "alpha_used": alpha
                },
                "metadata": row["metadata"] or {},
                "document_id": str(row["document_id"]),
                "chunk_index": row["chunk_index"],
                "adjacent": {
                    "prev_chunk_id": str(row["prev_chunk_id"]) if row["prev_chunk_id"] else None,
                    "next_chunk_id": str(row["next_chunk_id"]) if row["next_chunk_id"] else None,
                },
                "structure": {
                    "section_hierarchy": row["section_hierarchy"] or [],
                    "heading_context": row["heading_context"],
                    "document_position": float(row["document_position"]) if row["document_position"] else None,
                },
                "parent_child": {
                    "chunk_level": row["chunk_level"],
                    "parent_chunk_id": str(row["parent_chunk_id"]) if row["parent_chunk_id"] else None,
                }
            })

        # Log statistiques
        if formatted_results:
            avg_similarity = sum(r["scores"]["vector_similarity"] for r in formatted_results) / len(formatted_results)
            avg_bm25 = sum(r["scores"]["bm25_score"] for r in formatted_results) / len(formatted_results)
            avg_combined = sum(r["scores"]["combined_score"] for r in formatted_results) / len(formatted_results)

            logger.info(
                f"‚úÖ Hybrid search: {len(formatted_results)} r√©sultats | "
                f"Scores moyens - Vector: {avg_similarity:.3f}, BM25: {avg_bm25:.3f}, Combined: {avg_combined:.4f}"
            )
        else:
            logger.warning(f"‚ö†Ô∏è Hybrid search: 0 r√©sultats pour query '{query}'")

        return formatted_results

    except Exception as e:
        logger.error(f"‚ùå Erreur hybrid search: {e}", exc_info=True)
        raise


async def smart_hybrid_search(
    query: str,
    query_embedding: List[float],
    k: int = 5,
    alpha: Optional[float] = None
) -> List[Dict]:
    """
    Recherche hybride intelligente qui g√®re automatiquement parent-child chunks.

    Utilise match_chunks_smart_hybrid() qui:
    - D√©tecte automatiquement si parent-child chunks existent
    - Si oui: cherche dans children, retourne parents
    - Si non: recherche hybride classique

    Args:
        query: Question utilisateur
        query_embedding: Embedding de la question
        k: Nombre de r√©sultats
        alpha: Poids vector/keyword (None = adaptive)

    Returns:
        R√©sultats hybrid search avec meilleur contexte (parents si disponibles)
    """
    # D√©terminer alpha
    if alpha is None:
        alpha = adaptive_alpha(query)

    alpha = max(0.0, min(1.0, alpha))
    processed_query = preprocess_query_for_tsquery(query)

    if not processed_query:
        processed_query = query.lower()

    logger.info(f"üß† Smart hybrid search: query='{query}', alpha={alpha:.2f}")

    # Convertir embedding liste Python en cha√Æne PostgreSQL vector
    embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"

    try:
        async with database.db_pool.acquire() as conn:
            # Appeler directement la fonction sans SELECT (√©vite r√©ordonnancement)
            results = await conn.fetch("""
                SELECT * FROM match_chunks_smart_hybrid($1::vector, $2, $3, $4)
            """,
            embedding_str,
            processed_query,
            k,
            alpha
            )

        # Formater identique √† hybrid_search()
        formatted_results = []
        for row in results:
            formatted_results.append({
                "chunk_id": str(row["id"]),
                "content": row["content"],
                "scores": {
                    "vector_similarity": float(row["similarity"]),
                    "bm25_score": float(row["bm25_score"]),
                    "combined_score": float(row["combined_score"]),
                    "alpha_used": alpha
                },
                "metadata": row["metadata"] or {},
                "document_id": str(row["document_id"]),
                "chunk_index": row["chunk_index"],
                "adjacent": {
                    "prev_chunk_id": str(row["prev_chunk_id"]) if row["prev_chunk_id"] else None,
                    "next_chunk_id": str(row["next_chunk_id"]) if row["next_chunk_id"] else None,
                },
                "structure": {
                    "section_hierarchy": row["section_hierarchy"] or [],
                    "heading_context": row["heading_context"],
                    "document_position": float(row["document_position"]) if row["document_position"] else None,
                },
                "parent_child": {
                    "chunk_level": row["chunk_level"],
                    "parent_chunk_id": str(row["parent_chunk_id"]) if row["parent_chunk_id"] else None,
                }
            })

        logger.info(f"‚úÖ Smart hybrid: {len(formatted_results)} r√©sultats")
        return formatted_results

    except Exception as e:
        logger.error(f"‚ùå Erreur smart hybrid search: {e}", exc_info=True)
        raise
