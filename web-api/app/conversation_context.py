"""
Module de gestion du contexte conversationnel intelligent pour RAGFab.

Ce module fournit des fonctions pour:
- Construire un contexte conversationnel structur√©
- Enrichir les queries avec le contexte
- D√©tecter les changements de sujet
- Cr√©er des system prompts contextuels

Author: RAGFab Optimization Team
Date: 2025-01-24
"""

import logging
import os
from typing import List, Dict, Optional
from uuid import UUID
import httpx
from datetime import datetime

logger = logging.getLogger(__name__)


async def extract_main_topic(messages: List[dict], db_pool) -> str:
    """
    Extrait le sujet principal d'une conversation √† partir des messages r√©cents.

    Args:
        messages: Liste des messages (user + assistant) tri√©s du plus r√©cent au plus ancien
        db_pool: Pool de connexions PostgreSQL

    Returns:
        Sujet principal de la conversation (string concis)
    """
    if not messages or len(messages) < 2:
        return "nouvelle conversation"

    # Prendre les 3 premiers √©changes (6 messages max)
    recent_msgs = messages[:min(6, len(messages))]

    # Construire un r√©sum√© des questions utilisateur
    user_questions = []
    for msg in recent_msgs:
        if msg["role"] == "user":
            user_questions.append(msg["content"][:100])

    if not user_questions:
        return "nouvelle conversation"

    # Utiliser LLM rapide pour extraire topic (appel l√©ger, 50 tokens max)
    try:
        from app.utils.generic_llm_provider import get_generic_llm_model

        model = get_generic_llm_model()
        api_url = model.api_url.rstrip('/')

        topic_prompt = f"""Extrait le sujet principal de cette conversation en 3-5 mots maximum.

Questions pos√©es:
{chr(10).join(f"- {q}" for q in user_questions)}

Sujet (3-5 mots):"""

        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                f"{api_url}/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {model.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model.model_name,
                    "messages": [{"role": "user", "content": topic_prompt}],
                    "temperature": 0.1,
                    "max_tokens": 50
                }
            )
            response.raise_for_status()
            result = response.json()

            topic = result["choices"][0]["message"]["content"].strip()
            logger.info(f"üìå Sujet extrait: '{topic}'")
            return topic

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur extraction topic: {e}")
        # Fallback: utiliser premi√®re question comme topic
        return user_questions[0][:50] if user_questions else "conversation g√©n√©rale"


async def build_conversation_context(
    conversation_id: UUID,
    db_pool,
    limit: int = 5
) -> Optional[Dict]:
    """
    Construit un contexte conversationnel structur√© intelligent.

    Ce contexte enrichit le system prompt sans passer l'historique complet au LLM,
    permettant de maintenir la coh√©rence conversationnelle tout en for√ßant le function calling.

    Args:
        conversation_id: ID de la conversation
        db_pool: Pool de connexions PostgreSQL
        limit: Nombre maximum d'√©changes √† consid√©rer (d√©faut: 5)

    Returns:
        Dictionnaire avec:
        - current_topic: Sujet principal de la conversation
        - conversation_flow: Liste des √©changes r√©cents (question + r√©sum√© r√©ponse)
        - all_sources_consulted: Liste de toutes les sources consult√©es
        - last_exchange: Dernier √©change (pour r√©f√©rence rapide)
    """
    try:
        # R√©cup√©rer les derniers messages
        async with db_pool.acquire() as conn:
            messages = await conn.fetch(
                """
                SELECT role, content, sources, created_at
                FROM messages
                WHERE conversation_id = $1
                ORDER BY created_at DESC
                LIMIT $2
                """,
                conversation_id,
                limit * 2  # *2 car user+assistant par √©change
            )

        if not messages or len(messages) < 2:
            logger.info(f"üì≠ Pas assez de messages pour construire contexte (conv_id={conversation_id})")
            return None

        # Identifier le sujet principal
        topic = await extract_main_topic(messages, db_pool)

        # Construire le contexte structur√©
        context = {
            "current_topic": topic,
            "conversation_flow": [],
            "all_sources_consulted": [],
            "last_exchange": None
        }

        # Parcourir √©changes du plus ancien au plus r√©cent
        for i in range(len(messages) - 1, -1, -2):
            if i == 0:
                break  # Dernier message sans paire

            assistant_msg = messages[i]
            user_msg = messages[i - 1] if i > 0 else None

            if not user_msg:
                continue

            # R√©sum√© de la r√©ponse (300 chars max)
            assistant_summary = assistant_msg["content"][:300]
            if len(assistant_msg["content"]) > 300:
                assistant_summary += "..."

            # Sources utilis√©es dans cette r√©ponse
            sources_used = []
            if assistant_msg["sources"]:
                sources_used = [
                    s.get("document_title", "Document sans titre")
                    for s in assistant_msg["sources"]
                ][:3]  # Max 3 sources par √©change

            exchange = {
                "user_asked": user_msg["content"],
                "assistant_answered": assistant_summary,
                "sources_used": sources_used,
                "timestamp": user_msg["created_at"].isoformat()
            }

            context["conversation_flow"].insert(0, exchange)  # Insert au d√©but (ordre chronologique)

            # Agr√©ger toutes les sources consult√©es
            if assistant_msg["sources"]:
                for source in assistant_msg["sources"]:
                    if source not in context["all_sources_consulted"]:
                        context["all_sources_consulted"].append(source)

            # Sauvegarder le dernier √©change
            if context["last_exchange"] is None:
                context["last_exchange"] = exchange

        logger.info(
            f"‚úÖ Contexte construit: topic='{topic}', "
            f"{len(context['conversation_flow'])} √©changes, "
            f"{len(context['all_sources_consulted'])} sources"
        )

        return context

    except Exception as e:
        logger.error(f"‚ùå Erreur construction contexte: {e}", exc_info=True)
        return None


async def create_contextual_system_prompt(context: Optional[Dict], base_prompt: str) -> str:
    """
    Cr√©e un system prompt enrichi avec le contexte conversationnel.

    Args:
        context: Contexte conversationnel (ou None si premi√®re question)
        base_prompt: System prompt de base (avec d√©finition des tools)

    Returns:
        System prompt enrichi avec contexte structur√©
    """
    if not context:
        # Pas de contexte ‚Üí retourner prompt de base
        return base_prompt

    # Formater le flux conversationnel
    flow_lines = []
    for i, exchange in enumerate(context["conversation_flow"][-3:], 1):  # 3 derniers √©changes
        flow_lines.append(
            f"{i}. **Question**: {exchange['user_asked']}\n"
            f"   **R√©ponse**: {exchange['assistant_answered']}"
        )
        if exchange["sources_used"]:
            flow_lines.append(f"   **Sources**: {', '.join(exchange['sources_used'])}")

    flow_summary = "\n\n".join(flow_lines)

    # Formater les sources consult√©es (titres uniques)
    sources_titles = list(set([
        s.get("document_title", "Document")
        for s in context["all_sources_consulted"]
    ]))[:5]  # Max 5 titres

    sources_str = ", ".join(sources_titles) if sources_titles else "Aucune source consult√©e"

    # Injecter contexte dans le prompt
    contextual_section = f"""
üìö **CONTEXTE DE LA CONVERSATION EN COURS**

**Sujet principal**: {context['current_topic']}

**√âchanges pr√©c√©dents** (r√©sum√© des 3 derniers):
{flow_summary}

**Documents d√©j√† consult√©s**: {sources_str}

---

**üéØ NOUVELLE QUESTION DE L'UTILISATEUR** (ci-dessous)

**INSTRUCTIONS CRITIQUES POUR UTILISER LE CONTEXTE**:

1. **COMPRENDRE LA CONTINUIT√â**:
   - Cette question fait probablement suite aux √©changes pr√©c√©dents
   - L'utilisateur explore le sujet: "{context['current_topic']}"
   - Il peut faire r√©f√©rence implicite aux r√©ponses pr√©c√©dentes

2. **ENRICHIR LA RECHERCHE**:
   - Si la question est courte/vague (ex: "Comment faire ?", "Et pour √ßa ?"):
     ‚Üí Enrichir avec le contexte du sujet principal
     ‚Üí Exemple: "Comment faire ?" devient "Comment {context['current_topic']}"

   - Si la question fait r√©f√©rence √† une r√©ponse pr√©c√©dente:
     ‚Üí Int√©grer les √©l√©ments cl√©s de l'√©change pr√©c√©dent
     ‚Üí Exemple: "Et les exceptions ?" devient "Quelles sont les exceptions √† {context['current_topic']}"

3. **MAINTENIR LA COH√âRENCE**:
   - R√©f√©rencer explicitement si c'est une suite de proc√©dure
   - Exemple: "Suite √† l'√©tape pr√©c√©dente (activation Bluetooth), maintenant..."
   - √âviter de r√©p√©ter des informations d√©j√† donn√©es sauf si n√©cessaire

4. **APPEL DU TOOL OBLIGATOIRE**:
   - Tu DOIS TOUJOURS appeler `search_knowledge_base_tool` avant de r√©pondre
   - La query doit √™tre enrichie du contexte si n√©cessaire
   - NE PAS juste chercher "√ßa" ou "celle-ci" ‚Üí reformuler avec le sujet

---

"""

    # Ins√©rer contexte AVANT les r√®gles absolues du prompt de base
    enriched_prompt = contextual_section + base_prompt

    logger.info(f"üìã System prompt enrichi cr√©√© ({len(enriched_prompt)} chars, +{len(contextual_section)} contexte)")

    return enriched_prompt


async def enrich_query_with_context(
    user_message: str,
    context: Optional[Dict]
) -> str:
    """
    Enrichit une query utilisateur avec le contexte conversationnel si n√©cessaire.

    D√©tecte les questions courtes ou avec r√©f√©rences implicites et les enrichit
    avec le contexte du sujet principal.

    Args:
        user_message: Message utilisateur original
        context: Contexte conversationnel (ou None)

    Returns:
        Query enrichie ou originale si pas besoin d'enrichissement
    """
    if not context:
        return user_message

    # Compter les mots
    word_count = len(user_message.split())

    # Questions tr√®s courtes (‚â§ 5 mots) ‚Üí probablement des follow-ups
    if word_count <= 5:
        enriched = f"{user_message} (contexte: {context['current_topic']})"
        logger.info(f"üîß Query enrichie (courte): '{user_message}' ‚Üí '{enriched}'")
        return enriched

    # D√©tecter r√©f√©rences implicites
    implicit_refs = [
        "comment", "pourquoi", "et si", "et apr√®s", "ensuite",
        "√ßa", "celle", "celui", "le", "la", "cette", "ce",
        "pr√©cise", "d√©veloppe", "d√©taille", "explique"
    ]

    first_words = user_message.lower().split()[:2]  # 2 premiers mots
    has_implicit_ref = any(word in implicit_refs for word in first_words)

    if has_implicit_ref and context.get("last_exchange"):
        # Ajouter contexte du dernier √©change
        last_q = context["last_exchange"]["user_asked"][:80]
        enriched = f"{user_message} (suite de: {last_q})"
        logger.info(f"üîß Query enrichie (r√©f√©rence): '{user_message}' ‚Üí '{enriched}'")
        return enriched

    # Question autonome ‚Üí pas d'enrichissement
    logger.info(f"‚úÖ Query autonome, pas d'enrichissement: '{user_message}'")
    return user_message


async def detect_topic_shift(
    new_message: str,
    context: Optional[Dict],
    db_pool
) -> bool:
    """
    D√©tecte si l'utilisateur change de sujet dans sa nouvelle question.

    Utilise un appel LLM rapide pour classifier si la question continue
    le m√™me sujet ou introduit un nouveau sujet.

    Args:
        new_message: Nouvelle question de l'utilisateur
        context: Contexte conversationnel actuel
        db_pool: Pool de connexions DB

    Returns:
        True si changement de sujet d√©tect√©, False sinon
    """
    if not context:
        # Premi√®re question ‚Üí pas de topic shift
        return False

    current_topic = context["current_topic"]

    try:
        from app.utils.generic_llm_provider import get_generic_llm_model

        model = get_generic_llm_model()
        api_url = model.api_url.rstrip('/')

        classification_prompt = f"""Sujet actuel de la conversation: {current_topic}

Nouvelle question: {new_message}

L'utilisateur change-t-il de sujet ou continue-t-il le m√™me sujet ?
R√©ponds uniquement: MEME_SUJET ou NOUVEAU_SUJET"""

        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                f"{api_url}/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {model.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model.model_name,
                    "messages": [{"role": "user", "content": classification_prompt}],
                    "temperature": 0.1,
                    "max_tokens": 10
                }
            )
            response.raise_for_status()
            result = response.json()

            classification = result["choices"][0]["message"]["content"].strip().upper()
            is_new_topic = "NOUVEAU" in classification

            if is_new_topic:
                logger.info(f"üîÄ Topic shift d√©tect√©: '{current_topic}' ‚Üí nouveau sujet")
            else:
                logger.info(f"‚úÖ M√™me sujet: '{current_topic}'")

            return is_new_topic

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur d√©tection topic shift: {e}")
        # En cas d'erreur, assumer m√™me sujet (plus safe)
        return False
