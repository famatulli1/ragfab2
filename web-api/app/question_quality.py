"""
Module d'analyse de qualit√© des questions pour RAGFab.

Ce module fournit:
- Heuristiques rapides de d√©tection de questions vagues (<5ms)
- Analyse LLM approfondie si n√©cessaire (~500ms)
- G√©n√©ration de suggestions de reformulation
- D√©tection de vocabulaire m√©tier manquant

Architecture dual-path:
- Fast path (score >= 0.7): Direct vers recherche RAG
- Slow path (score < 0.7): Analyse LLM + suggestions

Author: RAGFab Team
Date: 2025-01-25
"""

import logging
import os
import re
import json
import hashlib
from enum import Enum
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional, Tuple
from functools import lru_cache
from uuid import UUID
import httpx

logger = logging.getLogger(__name__)

# ============================================================================
# Configuration
# ============================================================================

QUESTION_QUALITY_ENABLED = os.getenv("QUESTION_QUALITY_ENABLED", "true").lower() == "true"
QUESTION_QUALITY_PHASE = os.getenv("QUESTION_QUALITY_PHASE", "shadow")  # shadow | soft | interactive
HEURISTIC_THRESHOLD = float(os.getenv("QUESTION_QUALITY_HEURISTIC_THRESHOLD", "0.7"))
LLM_CONFIDENCE_THRESHOLD = float(os.getenv("QUESTION_QUALITY_LLM_CONFIDENCE_THRESHOLD", "0.75"))
LLM_TIMEOUT = float(os.getenv("QUESTION_QUALITY_LLM_TIMEOUT", "5"))


# ============================================================================
# Enums & Dataclasses
# ============================================================================

class QuestionClassification(str, Enum):
    """Classification du probl√®me d√©tect√© dans la question."""
    CLEAR = "clear"                         # Question bien formul√©e
    TOO_VAGUE = "too_vague"                 # Trop g√©n√©rale
    WRONG_VOCABULARY = "wrong_vocabulary"   # Termes incorrects/non-m√©tier
    MISSING_CONTEXT = "missing_context"     # R√©f√©rences floues (√ßa, celui-l√†)
    OUT_OF_SCOPE = "out_of_scope"           # Hors p√©rim√®tre documentaire


@dataclass
class QuestionSuggestion:
    """Une suggestion de reformulation."""
    text: str
    type: str  # 'reformulation' | 'clarification' | 'domain_term'
    reason: Optional[str] = None


@dataclass
class QualityAnalysisResult:
    """R√©sultat de l'analyse de qualit√© d'une question."""
    classification: QuestionClassification
    confidence: float  # 0.0 - 1.0
    heuristic_score: float  # Score des heuristiques (0.0 - 1.0)
    suggestions: List[QuestionSuggestion] = field(default_factory=list)
    detected_terms: List[str] = field(default_factory=list)  # Termes domaine d√©tect√©s
    suggested_terms: List[str] = field(default_factory=list)  # Termes sugg√©r√©s
    reasoning: Optional[str] = None  # Explication courte
    analyzed_by: str = "heuristics"  # 'heuristics' | 'llm'

    def to_dict(self) -> Dict:
        """Convertit en dictionnaire pour la r√©ponse API."""
        return {
            "classification": self.classification.value,
            "confidence": self.confidence,
            "heuristic_score": self.heuristic_score,
            "suggestions": [
                {"text": s.text, "type": s.type, "reason": s.reason}
                for s in self.suggestions
            ],
            "detected_terms": self.detected_terms,
            "suggested_terms": self.suggested_terms,
            "reasoning": self.reasoning,
            "analyzed_by": self.analyzed_by
        }


# ============================================================================
# Patterns de D√©tection (Fran√ßais)
# ============================================================================

# Red flags: indicateurs de question probl√©matique
RED_FLAG_PATTERNS = [
    # Questions mono-mot ou tr√®s courtes
    (r"^(comment|pourquoi|quoi|ou|quand)\s*\?*$", "question_monoword", 0.5),
    # "c'est quoi X" sans contexte
    (r"^c['']?est\s+quoi\s+\w+\s*\?*$", "cest_quoi_vague", 0.4),
    # Trop vague
    (r"^(ca|√ßa)\s+(marche|fonctionne|se passe)\s*(comment)?\s*\?*$", "ca_vague", 0.5),
    # D√©but par conjonction (suite implicite)
    (r"^(et|ou|mais|donc)\s+", "starts_conjunction", 0.3),
    # Pronoms seuls
    (r"^(celui|celle|ceux|celles)[-\s]?(ci|la|l√†)?\s*\?*$", "pronouns_only", 0.5),
    # Multiples espaces (potentiel copier-coller mal format√©)
    (r"\s{3,}", "multiple_spaces", 0.1),
    # Questions ultra-courtes
    (r"^.{1,10}\?*$", "ultra_short", 0.3),
]

# Green flags: indicateurs de bonne question
GREEN_FLAG_PATTERNS = [
    # Termes m√©tier Sillage
    (r"\b(sillage|sipsdm|bis_lme)\b", "sillage_term", 0.2),
    # Termes techniques DB
    (r"\b(bdd|base\s+de\s+donn[√©e]es?|table|schema)\b", "db_term", 0.15),
    # Termes m√©dicaux/hospitaliers
    (r"\b(patient|dossier|maternit[√©e]|obst[√©e]trique)\b", "medical_term", 0.15),
    # Lien m√®re-enfant sp√©cifique
    (r"\b(lien\s+m[e√®]re|m[e√®]re[-\s]enfant|ipp|iep)\b", "lien_mere_enfant", 0.25),
    # Proc√©dures
    (r"\b(proc[√©e]dure|protocole|[√©e]tape|processus)\b", "procedure_term", 0.1),
    # Questions structur√©es
    (r"^(comment|quelle?\s+est|o[u√π]\s+(est|se\s+trouve|trouver))\s+.{15,}", "structured_question", 0.15),
    # R√©f√©rences num√©riques (IDs, num√©ros)
    (r"\b(n[¬∞o]?\s*\d+|ref\.?\s*\d+|id\s*[:=]?\s*\d+)\b", "has_reference", 0.1),
]

# Vocabulaire utilisateur ‚Üí vocabulaire m√©tier
VOCABULARY_CORRECTIONS = {
    # Expressions courantes ‚Üí termes techniques
    "rattacher la maman": "cr√©er le lien m√®re-enfant",
    "rattacher maman": "cr√©er lien m√®re-enfant",
    "rattacher le b√©b√©": "cr√©er le lien m√®re-enfant",
    "lier maman b√©b√©": "cr√©er lien m√®re-enfant",
    "lier la m√®re": "cr√©er le lien m√®re-enfant",
    "maman et b√©b√©": "lien m√®re-enfant",
    # Termes g√©n√©riques ‚Üí sp√©cifiques
    "base de donn√©es": "BDD Sillage",
    "le logiciel": "Sillage",
    "l'application": "Sillage",
    "le syst√®me": "Sillage",
}

# Stopwords fran√ßais pour le scoring
FRENCH_STOPWORDS = {
    "le", "la", "les", "un", "une", "des", "du", "de", "d",
    "et", "ou", "mais", "donc", "car", "ni", "que", "qui",
    "ce", "cette", "ces", "mon", "ma", "mes", "ton", "ta", "tes",
    "son", "sa", "ses", "notre", "nos", "votre", "vos", "leur", "leurs",
    "je", "tu", "il", "elle", "nous", "vous", "ils", "elles", "on",
    "me", "te", "se", "lui", "y", "en",
    "√†", "au", "aux", "avec", "pour", "par", "sur", "sous", "dans",
    "est", "sont", "a", "ont", "fait", "faire", "√™tre", "avoir",
    "comment", "pourquoi", "quand", "o√π", "quoi", "quel", "quelle",
}


# ============================================================================
# Fonctions Heuristiques (Fast Path)
# ============================================================================

def normalize_question(question: str) -> str:
    """Normalise une question pour comparaison/cache."""
    normalized = question.lower().strip()
    normalized = re.sub(r'\s+', ' ', normalized)
    normalized = re.sub(r'[^\w\s]', '', normalized)
    return normalized


def compute_length_score(question: str) -> float:
    """
    Score bas√© sur la longueur de la question.
    Optimal: 5-30 mots
    """
    words = question.split()
    word_count = len(words)

    if word_count < 3:
        return 0.2
    elif word_count < 5:
        return 0.5
    elif word_count <= 30:
        return 1.0
    elif word_count <= 50:
        return 0.7
    else:
        return 0.4


def compute_structure_score(question: str) -> float:
    """
    Score bas√© sur la structure grammaticale.
    V√©rifie pr√©sence sujet + verbe + contexte.
    """
    words = question.lower().split()

    # Trop court pour avoir une structure
    if len(words) < 3:
        return 0.3

    # Pr√©sence d'un verbe interrogatif/d'action
    question_verbs = {"comment", "pourquoi", "quand", "o√π", "quel", "quelle", "quels", "quelles"}
    action_verbs = {"faire", "cr√©er", "modifier", "supprimer", "ajouter", "configurer", "activer", "d√©sactiver"}

    has_question_word = any(w in question_verbs for w in words[:3])
    has_action_verb = any(v in question for v in action_verbs)

    score = 0.5

    if has_question_word:
        score += 0.25

    if has_action_verb:
        score += 0.25

    # Bonus si question se termine par "?"
    if question.strip().endswith("?"):
        score += 0.1

    return min(1.0, score)


def compute_vocabulary_score(question: str) -> Tuple[float, List[str], List[str]]:
    """
    Score bas√© sur le vocabulaire m√©tier d√©tect√©.
    Retourne (score, termes_detectes, termes_suggeres)
    """
    question_lower = question.lower()
    detected_terms = []
    suggested_terms = []

    # V√©rifier green flags (vocabulaire m√©tier)
    domain_score = 0.0
    for pattern, term_type, bonus in GREEN_FLAG_PATTERNS:
        if re.search(pattern, question_lower, re.IGNORECASE):
            match = re.search(pattern, question_lower, re.IGNORECASE)
            if match:
                detected_terms.append(match.group())
            domain_score += bonus

    # V√©rifier si vocabulaire utilisateur peut √™tre am√©lior√©
    for user_term, domain_term in VOCABULARY_CORRECTIONS.items():
        if user_term in question_lower:
            suggested_terms.append(domain_term)

    # Score de base si pas de termes m√©tier
    if not detected_terms and not suggested_terms:
        # Question g√©n√©rique
        base_score = 0.4
    elif suggested_terms and not detected_terms:
        # Utilise vocabulaire utilisateur, pas m√©tier
        base_score = 0.5
    else:
        # Utilise vocabulaire m√©tier
        base_score = 0.7 + min(0.3, domain_score)

    return (min(1.0, base_score), detected_terms, suggested_terms)


def compute_specificity_score(question: str) -> float:
    """
    Score bas√© sur la sp√©cificit√© de la question.
    V√©rifie pr√©sence d'entit√©s nomm√©es, IDs, dates, etc.
    """
    score = 0.5

    # Pr√©sence de nombres/IDs
    if re.search(r'\d{3,}', question):
        score += 0.2  # Num√©ros significatifs

    # Pr√©sence de noms propres (majuscules)
    proper_nouns = re.findall(r'\b[A-Z][a-z]+\b', question)
    if proper_nouns:
        score += min(0.2, len(proper_nouns) * 0.1)

    # Pr√©sence de termes techniques entre guillemets
    if re.search(r'["¬´¬ª\'].*?["¬´¬ª\']', question):
        score += 0.1

    # P√©nalit√© pour pronoms vagues
    vague_pronouns = ["√ßa", "ca", "celui", "celle", "ceux", "celles", "ceci", "cela"]
    if any(p in question.lower().split() for p in vague_pronouns):
        score -= 0.2

    return max(0.0, min(1.0, score))


def apply_pattern_modifiers(question: str, base_score: float) -> Tuple[float, List[str]]:
    """
    Applique les modificateurs red/green flags au score.
    Retourne (score_modifie, raisons)
    """
    question_lower = question.lower()
    score = base_score
    reasons = []

    # Red flags (p√©nalit√©s)
    for pattern, flag_type, penalty in RED_FLAG_PATTERNS:
        if re.search(pattern, question_lower, re.IGNORECASE):
            score *= (1 - penalty)
            reasons.append(f"red_flag:{flag_type}")

    # Green flags sont d√©j√† compt√©s dans vocabulary_score
    # mais on peut ajouter des bonus suppl√©mentaires

    return (max(0.0, min(1.0, score)), reasons)


def quick_quality_check(question: str) -> Tuple[float, Dict]:
    """
    V√©rification rapide de qualit√© via heuristiques (<5ms).

    Args:
        question: Question utilisateur

    Returns:
        (score, debug_info) o√π score est entre 0.0 et 1.0
    """
    # Scores individuels
    length_score = compute_length_score(question)
    structure_score = compute_structure_score(question)
    vocab_score, detected_terms, suggested_terms = compute_vocabulary_score(question)
    specificity_score = compute_specificity_score(question)

    # Pond√©ration
    weights = {
        "length": 0.20,
        "structure": 0.25,
        "vocabulary": 0.35,
        "specificity": 0.20
    }

    weighted_score = (
        length_score * weights["length"] +
        structure_score * weights["structure"] +
        vocab_score * weights["vocabulary"] +
        specificity_score * weights["specificity"]
    )

    # Appliquer modificateurs
    final_score, flags = apply_pattern_modifiers(question, weighted_score)

    debug_info = {
        "scores": {
            "length": round(length_score, 3),
            "structure": round(structure_score, 3),
            "vocabulary": round(vocab_score, 3),
            "specificity": round(specificity_score, 3),
            "weighted": round(weighted_score, 3),
            "final": round(final_score, 3)
        },
        "detected_terms": detected_terms,
        "suggested_terms": suggested_terms,
        "flags": flags,
        "word_count": len(question.split())
    }

    return (final_score, debug_info)


# ============================================================================
# Analyse LLM (Slow Path)
# ============================================================================

LLM_ANALYSIS_PROMPT = """Tu es un assistant de contr√¥le qualit√© pour un syst√®me RAG m√©dical/hospitalier.

QUESTION UTILISATEUR: "{question}"

{context_section}

DOMAINE DE LA BASE DOCUMENTAIRE:
- Documentation technique Sillage (logiciel hospitalier)
- Proc√©dures m√©dicales et hospitali√®res
- Guides utilisateur et fiches solutions

VOCABULAIRE M√âTIER IMPORTANT:
- "lien m√®re-enfant" (pas "rattacher maman/b√©b√©")
- "BDD Sillage" / "table BIS_LME" / "sch√©ma SIPSDM"
- "IPP" (Identifiant Patient Permanent)
- "IEP" (Identifiant √âpisode Patient)
- "dossier patient" (pas "fiche patient")

CLASSIFIE LA QUESTION parmi:
- clear: Question bien formul√©e, vocabulaire appropri√©, pr√™te pour la recherche
- too_vague: Question trop g√©n√©rale, manque de pr√©cision (ex: "comment faire ?")
- wrong_vocabulary: Termes incorrects/familiers au lieu du vocabulaire m√©tier (ex: "rattacher maman" ‚Üí "cr√©er lien m√®re-enfant")
- missing_context: Utilise des r√©f√©rences floues sans contexte ("celui-l√†", "√ßa", "cette chose")
- out_of_scope: Question clairement hors p√©rim√®tre documentaire (m√©t√©o, recettes, etc.)

R√âPONDS UNIQUEMENT EN JSON (pas de texte avant/apr√®s):
{{
  "classification": "clear|too_vague|wrong_vocabulary|missing_context|out_of_scope",
  "confidence": 0.0-1.0,
  "reasoning": "Explication en 1 phrase",
  "suggestions": ["Reformulation 1", "Reformulation 2"],
  "domain_terms_suggested": ["terme_correct1", "terme_correct2"]
}}"""


async def analyze_with_llm(
    question: str,
    context: Optional[Dict] = None,
    heuristic_info: Optional[Dict] = None
) -> QualityAnalysisResult:
    """
    Analyse approfondie via LLM pour les questions ambigu√´s.

    Args:
        question: Question utilisateur
        context: Contexte conversationnel (optionnel)
        heuristic_info: Info des heuristiques (pour suggestions)

    Returns:
        QualityAnalysisResult avec classification et suggestions
    """
    try:
        from app.utils.generic_llm_provider import get_generic_llm_model

        model = get_generic_llm_model()
        api_url = model.api_url.rstrip('/')

        # Construire section contexte
        context_section = ""
        if context and context.get("current_topic"):
            context_section = f"""CONTEXTE CONVERSATIONNEL:
- Sujet actuel: {context['current_topic']}
- Dernier √©change: {context.get('last_exchange', {}).get('user_asked', 'N/A')[:100]}"""

        prompt = LLM_ANALYSIS_PROMPT.format(
            question=question,
            context_section=context_section
        )

        async with httpx.AsyncClient(timeout=LLM_TIMEOUT) as client:
            response = await client.post(
                f"{api_url}/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {model.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1,
                    "max_tokens": 300
                }
            )
            response.raise_for_status()
            result = response.json()

            content = result["choices"][0]["message"]["content"].strip()

            # Parser le JSON
            # Nettoyer le contenu (enlever markdown si pr√©sent)
            if content.startswith("```"):
                content = re.sub(r'^```json?\s*', '', content)
                content = re.sub(r'\s*```$', '', content)

            analysis = json.loads(content)

            classification = QuestionClassification(analysis.get("classification", "too_vague"))
            confidence = float(analysis.get("confidence", 0.5))
            reasoning = analysis.get("reasoning", "")
            suggestions_raw = analysis.get("suggestions", [])
            suggested_terms = analysis.get("domain_terms_suggested", [])

            # Convertir suggestions en objets
            suggestions = []
            for i, sugg_text in enumerate(suggestions_raw[:3]):  # Max 3 suggestions
                suggestions.append(QuestionSuggestion(
                    text=sugg_text,
                    type="reformulation",
                    reason=reasoning if i == 0 else None
                ))

            # Utiliser les termes sugg√©r√©s par heuristiques si LLM n'en fournit pas
            if not suggested_terms and heuristic_info:
                suggested_terms = heuristic_info.get("suggested_terms", [])

            logger.info(
                f"ü§ñ Analyse LLM: classification={classification.value}, "
                f"confidence={confidence:.2f}, suggestions={len(suggestions)}"
            )

            return QualityAnalysisResult(
                classification=classification,
                confidence=confidence,
                heuristic_score=heuristic_info.get("scores", {}).get("final", 0.5) if heuristic_info else 0.5,
                suggestions=suggestions,
                detected_terms=heuristic_info.get("detected_terms", []) if heuristic_info else [],
                suggested_terms=suggested_terms,
                reasoning=reasoning,
                analyzed_by="llm"
            )

    except json.JSONDecodeError as e:
        logger.warning(f"‚ö†Ô∏è Erreur parsing JSON LLM: {e}")
        return _fallback_result(question, heuristic_info, "json_parse_error")
    except httpx.TimeoutException:
        logger.warning(f"‚ö†Ô∏è Timeout analyse LLM ({LLM_TIMEOUT}s)")
        return _fallback_result(question, heuristic_info, "timeout")
    except Exception as e:
        logger.error(f"‚ùå Erreur analyse LLM: {e}", exc_info=True)
        return _fallback_result(question, heuristic_info, str(e))


def _fallback_result(
    question: str,
    heuristic_info: Optional[Dict],
    error_reason: str
) -> QualityAnalysisResult:
    """G√©n√®re un r√©sultat de fallback bas√© sur les heuristiques."""
    heuristic_score = heuristic_info.get("scores", {}).get("final", 0.5) if heuristic_info else 0.5
    suggested_terms = heuristic_info.get("suggested_terms", []) if heuristic_info else []

    # G√©n√©rer une suggestion basique si vocabulaire incorrect
    suggestions = []
    if suggested_terms:
        # Remplacer dans la question
        improved_question = question
        for user_term, domain_term in VOCABULARY_CORRECTIONS.items():
            if user_term in question.lower():
                improved_question = re.sub(
                    re.escape(user_term),
                    domain_term,
                    improved_question,
                    flags=re.IGNORECASE
                )
        if improved_question != question:
            suggestions.append(QuestionSuggestion(
                text=improved_question,
                type="domain_term",
                reason="Vocabulaire m√©tier sugg√©r√©"
            ))

    classification = QuestionClassification.CLEAR if heuristic_score >= HEURISTIC_THRESHOLD else QuestionClassification.TOO_VAGUE

    return QualityAnalysisResult(
        classification=classification,
        confidence=heuristic_score,
        heuristic_score=heuristic_score,
        suggestions=suggestions,
        detected_terms=heuristic_info.get("detected_terms", []) if heuristic_info else [],
        suggested_terms=suggested_terms,
        reasoning=f"Fallback heuristique ({error_reason})",
        analyzed_by="heuristics_fallback"
    )


# ============================================================================
# Point d'Entr√©e Principal
# ============================================================================

async def analyze_question_quality(
    question: str,
    conversation_context: Optional[Dict] = None,
    threshold: float = None
) -> QualityAnalysisResult:
    """
    Point d'entr√©e principal pour l'analyse de qualit√©.

    Architecture dual-path:
    1. Fast path: Si heuristiques >= threshold ‚Üí question OK
    2. Slow path: Si heuristiques < threshold ‚Üí analyse LLM

    Args:
        question: Question utilisateur
        conversation_context: Contexte conversationnel (optionnel)
        threshold: Seuil pour d√©clencher LLM (d√©faut: HEURISTIC_THRESHOLD)

    Returns:
        QualityAnalysisResult avec classification et suggestions
    """
    if not QUESTION_QUALITY_ENABLED:
        # Module d√©sactiv√© ‚Üí toujours OK
        return QualityAnalysisResult(
            classification=QuestionClassification.CLEAR,
            confidence=1.0,
            heuristic_score=1.0,
            reasoning="Quality check disabled",
            analyzed_by="disabled"
        )

    if threshold is None:
        threshold = HEURISTIC_THRESHOLD

    # Phase 1: Heuristiques rapides
    heuristic_score, heuristic_info = quick_quality_check(question)

    logger.info(
        f"üìä Heuristiques: score={heuristic_score:.3f}, "
        f"threshold={threshold}, phase={QUESTION_QUALITY_PHASE}"
    )

    # Fast path: question claire
    if heuristic_score >= threshold:
        logger.info(f"‚úÖ Fast path: question OK (score={heuristic_score:.3f})")

        return QualityAnalysisResult(
            classification=QuestionClassification.CLEAR,
            confidence=heuristic_score,
            heuristic_score=heuristic_score,
            detected_terms=heuristic_info.get("detected_terms", []),
            suggested_terms=heuristic_info.get("suggested_terms", []),
            reasoning="Question claire (heuristiques)",
            analyzed_by="heuristics"
        )

    # Slow path: analyse LLM n√©cessaire
    logger.info(f"üîç Slow path: analyse LLM (score={heuristic_score:.3f} < {threshold})")

    result = await analyze_with_llm(
        question=question,
        context=conversation_context,
        heuristic_info=heuristic_info
    )

    return result


# ============================================================================
# Utilitaires pour Feedback/Learning
# ============================================================================

def get_cache_key(question: str) -> str:
    """G√©n√®re une cl√© de cache pour une question."""
    normalized = normalize_question(question)
    return hashlib.md5(normalized.encode()).hexdigest()[:16]


async def store_quality_feedback(
    question: str,
    analysis_result: QualityAnalysisResult,
    search_results_count: int,
    max_similarity: float,
    message_id: Optional[str] = None,  # Accepte str ou UUID
    user_rating: Optional[int] = None,
    db_pool = None
) -> None:
    """
    Stocke le feedback de qualit√© pour apprentissage.

    Args:
        question: Question analys√©e
        analysis_result: R√©sultat de l'analyse
        search_results_count: Nombre de r√©sultats de recherche
        max_similarity: Score de similarit√© max
        message_id: ID du message (optionnel)
        user_rating: Rating utilisateur -1/1 (optionnel)
        db_pool: Pool de connexions DB
    """
    if not db_pool:
        logger.debug("Pas de DB pool pour stocker feedback qualit√©")
        return

    try:
        # Convertir message_id en UUID si c'est une string
        message_uuid = None
        if message_id:
            message_uuid = UUID(message_id) if isinstance(message_id, str) else message_id

        async with db_pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO question_quality_feedback (
                    original_question,
                    normalized_question,
                    heuristic_score,
                    llm_classification,
                    results_count,
                    max_similarity,
                    message_id,
                    user_rating
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                ON CONFLICT DO NOTHING
                """,
                question,
                normalize_question(question),
                analysis_result.heuristic_score,
                analysis_result.classification.value,
                search_results_count,
                max_similarity,
                message_uuid,  # UUID converti
                user_rating
            )
            logger.debug(f"üìù Feedback qualit√© stock√© pour question")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur stockage feedback qualit√©: {e}")


# ============================================================================
# Export pour tests
# ============================================================================

__all__ = [
    "QuestionClassification",
    "QuestionSuggestion",
    "QualityAnalysisResult",
    "analyze_question_quality",
    "quick_quality_check",
    "analyze_with_llm",
    "store_quality_feedback",
    "QUESTION_QUALITY_ENABLED",
    "QUESTION_QUALITY_PHASE",
    "HEURISTIC_THRESHOLD",
]
