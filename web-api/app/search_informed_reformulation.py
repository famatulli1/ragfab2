"""
Module de reformulation de questions bas√© sur les r√©sultats de recherche.

Ce module fournit une approche G√âN√âRIQUE pour sugg√©rer des reformulations
en extrayant le vocabulaire dynamiquement depuis les documents trouv√©s,
sans patterns hardcod√©s.

Architecture:
1. Probe Search: Recherche rapide (k=3) pour obtenir du contexte
2. Extraction: Vocabulaire dynamique depuis les documents
3. LLM Suggestions: G√©n√©ration avec contexte documentaire
4. Fallback: Suggestions bas√©es sur termes extraits si timeout

Author: RAGFab Team
Date: 2025-01-25
"""

import logging
import re
import json
import asyncio
from collections import Counter
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from uuid import UUID
import httpx

logger = logging.getLogger(__name__)

# ============================================================================
# Configuration
# ============================================================================

import os

REFORMULATION_ENABLED = os.getenv("REFORMULATION_ENABLED", "true").lower() == "true"
REFORMULATION_PROBE_K = int(os.getenv("REFORMULATION_PROBE_K", "3"))
REFORMULATION_LLM_TIMEOUT = float(os.getenv("REFORMULATION_LLM_TIMEOUT", "8"))  # R√©duit de 25s
REFORMULATION_HEURISTIC_THRESHOLD = float(os.getenv("REFORMULATION_HEURISTIC_THRESHOLD", "0.65"))  # Align√© avec question_quality

# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class ExtractedVocabulary:
    """Vocabulaire extrait dynamiquement des r√©sultats de recherche."""
    terms: List[str] = field(default_factory=list)
    context_snippets: List[str] = field(default_factory=list)
    term_sources: Dict[str, str] = field(default_factory=dict)  # term -> document_title


@dataclass
class ReformulationSuggestion:
    """Une suggestion de reformulation."""
    text: str
    type: str  # 'vocabulary' | 'clarification' | 'expansion'
    reason: str
    source_document: Optional[str] = None


@dataclass
class ReformulationResult:
    """R√©sultat de l'analyse de reformulation."""
    needs_reformulation: bool
    suggestions: List[ReformulationSuggestion] = field(default_factory=list)
    extracted_terms: List[str] = field(default_factory=list)
    reasoning: Optional[str] = None
    analyzed_by: str = "probe_search"  # 'probe_search' | 'llm' | 'fallback' | 'disabled'

    def to_dict(self) -> Dict:
        """Convertit en dictionnaire pour la r√©ponse API."""
        return {
            "needs_reformulation": self.needs_reformulation,
            "suggestions": [
                {
                    "text": s.text,
                    "type": s.type,
                    "reason": s.reason,
                    "source_document": s.source_document
                }
                for s in self.suggestions
            ],
            "extracted_terms": self.extracted_terms,
            "reasoning": self.reasoning,
            "analyzed_by": self.analyzed_by
        }


# ============================================================================
# Heuristiques Structurelles (G√©n√©rique, sans vocabulaire domaine)
# ============================================================================

# Patterns structurels seulement - PAS de termes de domaine
STRUCTURAL_RED_FLAGS = [
    # Questions mono-mot
    (r"^(comment|pourquoi|quoi|ou|quand)\s*\?*$", "question_monoword", 0.5),
    # D√©but par conjonction (suite implicite)
    (r"^(et|ou|mais|donc)\s+", "starts_conjunction", 0.3),
    # Pronoms seuls
    (r"^(celui|celle|ceux|celles)[-\s]?(ci|la|l√†)?\s*\?*$", "pronouns_only", 0.5),
    # Questions ultra-courtes
    (r"^.{1,10}\?*$", "ultra_short", 0.3),
    # Pronoms flous sans contexte
    (r"^(√ßa|ca|cela)\s+", "vague_pronoun_start", 0.4),
    # "c'est quoi" tr√®s court
    (r"^c['']?est\s+quoi\s+\w{1,10}\s*\?*$", "cest_quoi_short", 0.3),
]

# Stopwords fran√ßais
FRENCH_STOPWORDS = {
    "le", "la", "les", "un", "une", "des", "du", "de", "d",
    "et", "ou", "mais", "donc", "car", "ni", "que", "qui",
    "ce", "cette", "ces", "mon", "ma", "mes", "ton", "ta", "tes",
    "son", "sa", "ses", "notre", "nos", "votre", "vos", "leur", "leurs",
    "je", "tu", "il", "elle", "nous", "vous", "ils", "elles", "on",
    "me", "te", "se", "lui", "y", "en",
    "√†", "au", "aux", "avec", "pour", "par", "sur", "sous", "dans",
    "est", "sont", "a", "ont", "fait", "faire", "√™tre", "avoir",
    "comment", "pourquoi", "quand", "o√π", "quoi", "quel", "quelle",
}

# Patterns √† exclure des titres de documents (noms de fichiers, versions, mots g√©n√©riques)
TITLE_EXCLUSION_PATTERNS = [
    r'.*\.(pdf|docx?|xlsx?|pptx?|txt|md|html?)$',  # Extensions de fichiers
    r'.*_V\d+.*',  # Patterns de version (_V25, _V1.2)
    r'.*\.\w+_V\d+.*',  # Patterns comme "Webmail.Administration_V25"
    r'^v?\d+(\.\d+)*$',  # Num√©ros de version seuls
    r'.*V\d+\.?\d*$',  # Se termine par version (inistration_V25, Admin_V2.1)
    r'^[a-z]*tion_V\d+',  # Mots cass√©s finissant par "tion_V25" (inistration_V25)
    r'^\d+$',  # Nombres seuls
]

# Mots g√©n√©riques √† exclure (m√©tadonn√©es et termes non pertinents pour suggestions)
GENERIC_TITLE_WORDS = {
    # M√©tadonn√©es de documents
    'document', 'documents', 'fichier', 'fichiers', 'file', 'files',
    'guide', 'manuel', 'manual', 'procedure', 'proc√©dure', 'process',
    'version', 'administration', 'configuration', 'installation',
    # Noms de produits/services g√©n√©riques
    'webmail', 'outlook', 'office', 'microsoft', 'google', 'gmail',
    # Mots trop g√©n√©riques pour √™tre utiles
    'cas', 'type', 'mode', 'liste', 'page', 'menu', 'option', 'onglet',
    'bouton', 'clic', 'cliquez', 's√©lectionner', 'choisir',
    'suivant', 'pr√©c√©dent', 'exemple', 'note', 'remarque', 'attention',
    'voir', 'figure', 'image', 'tableau', 'section', 'partie', 'chapitre',
}


def is_valid_vocabulary_term(term: str) -> bool:
    """
    V√©rifie si un terme extrait est pertinent pour les suggestions.

    Exclut:
    - Mots g√©n√©riques (Document, Guide, Manuel, Cas, etc.)
    - Noms de fichiers avec extensions
    - Patterns de version (_V25, V1.2, inistration_V25, etc.)
    - Termes contenant des points (sauf acronymes)
    - Mots trop g√©n√©riques pour √™tre utiles dans une suggestion

    Args:
        term: Le terme √† valider

    Returns:
        True si le terme est pertinent pour les suggestions
    """
    term_lower = term.lower()

    # Exclure mots g√©n√©riques
    if term_lower in GENERIC_TITLE_WORDS:
        return False

    # Exclure patterns de fichiers/versions
    for pattern in TITLE_EXCLUSION_PATTERNS:
        if re.match(pattern, term, re.IGNORECASE):
            return False

    # Exclure si contient un point (probablement nom de fichier)
    # Exception: garder les acronymes comme "B.A.L."
    if '.' in term and not term.isupper():
        return False

    return True


def compute_structural_score(question: str) -> float:
    """
    Calcule un score bas√© uniquement sur la structure de la question.
    PAS de vocabulaire de domaine - purement structurel.

    Returns:
        Score entre 0.0 et 1.0 (plus haut = meilleure structure)
    """
    question_lower = question.lower().strip()
    words = question_lower.split()
    word_count = len(words)

    # Score de longueur
    if word_count < 3:
        length_score = 0.3
    elif word_count < 5:
        length_score = 0.6
    elif word_count <= 30:
        length_score = 1.0
    elif word_count <= 50:
        length_score = 0.8
    else:
        length_score = 0.5

    # Score de structure grammaticale
    structure_score = 0.5
    question_words = {"comment", "pourquoi", "quand", "o√π", "quel", "quelle", "quels", "quelles"}
    action_verbs = {"faire", "cr√©er", "modifier", "supprimer", "ajouter", "configurer", "trouver", "chercher"}

    if any(w in question_words for w in words[:3]):
        structure_score += 0.25
    if any(v in question_lower for v in action_verbs):
        structure_score += 0.25
    if question.strip().endswith("?"):
        structure_score += 0.1

    structure_score = min(1.0, structure_score)

    # Score de sp√©cificit√© (sans vocabulaire domaine)
    specificity_score = 0.5
    # Pr√©sence de nombres significatifs
    if re.search(r'\d{3,}', question):
        specificity_score += 0.2
    # Pr√©sence de noms propres
    proper_nouns = re.findall(r'\b[A-Z][a-z]+\b', question)
    if proper_nouns:
        specificity_score += min(0.2, len(proper_nouns) * 0.1)
    # Termes entre guillemets
    if re.search(r'["¬´¬ª\'].*?["¬´¬ª\']', question):
        specificity_score += 0.1
    # P√©nalit√© pour pronoms vagues
    vague_pronouns = ["√ßa", "ca", "celui", "celle", "ceux", "celles", "ceci", "cela"]
    if any(p in words for p in vague_pronouns):
        specificity_score -= 0.2

    specificity_score = max(0.0, min(1.0, specificity_score))

    # Score combin√©
    base_score = (
        length_score * 0.30 +
        structure_score * 0.35 +
        specificity_score * 0.35
    )

    # Appliquer red flags structurels
    for pattern, flag_type, penalty in STRUCTURAL_RED_FLAGS:
        if re.search(pattern, question_lower, re.IGNORECASE):
            base_score *= (1 - penalty)
            logger.debug(f"Red flag '{flag_type}' d√©tect√©, p√©nalit√© {penalty}")

    return max(0.0, min(1.0, base_score))


# ============================================================================
# Probe Search (Recherche Rapide)
# ============================================================================

async def probe_search(
    question: str,
    db_pool,
    universe_ids: Optional[List[UUID]] = None,
    k: int = None
) -> List[dict]:
    """
    Recherche rapide pour extraire du contexte documentaire.

    - k r√©sultats seulement (d√©faut: REFORMULATION_PROBE_K)
    - Pas de reranking
    - Pas de chunks adjacents
    - Target: <100ms

    Args:
        question: Question utilisateur
        db_pool: Pool de connexions DB
        universe_ids: Filtrage par univers
        k: Nombre de r√©sultats (d√©faut: config)

    Returns:
        Liste de r√©sultats avec content, document_title, similarity
    """
    if k is None:
        k = REFORMULATION_PROBE_K

    try:
        # G√©n√©rer l'embedding via le service HTTP (m√™me m√©thode que main.py)
        embeddings_url = os.getenv("EMBEDDINGS_API_URL", "http://ragfab-embeddings:8001")
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{embeddings_url}/embed",
                json={"text": question},
                timeout=10.0
            )
            response.raise_for_status()
            embedding = response.json()["embedding"]

        # Convertir en string pour PostgreSQL
        embedding_str = "[" + ",".join(map(str, embedding)) + "]"

        logger.info(f"üîç Probe search: embedding g√©n√©r√© ({len(embedding)} dims), universe_ids={universe_ids}")

        async with db_pool.acquire() as conn:
            if universe_ids:
                # Convertir les UUIDs en liste pour PostgreSQL
                universe_list = [str(uid) for uid in universe_ids]
                logger.info(f"üîç Probe search: filtrage sur {len(universe_list)} univers: {universe_list}")

                results = await conn.fetch("""
                    SELECT c.content, c.metadata, d.title as document_title,
                           1 - (c.embedding <=> $1::vector) as similarity
                    FROM chunks c
                    JOIN documents d ON c.document_id = d.id
                    WHERE d.universe_id = ANY($2::uuid[])
                    ORDER BY c.embedding <=> $1::vector
                    LIMIT $3
                """, embedding_str, universe_list, k)
            else:
                logger.info(f"üîç Probe search: pas de filtrage univers")
                results = await conn.fetch("""
                    SELECT c.content, c.metadata, d.title as document_title,
                           1 - (c.embedding <=> $1::vector) as similarity
                    FROM chunks c
                    JOIN documents d ON c.document_id = d.id
                    ORDER BY c.embedding <=> $1::vector
                    LIMIT $2
                """, embedding_str, k)

        logger.info(f"‚úÖ Probe search: {len(results)} r√©sultats trouv√©s")
        return [dict(r) for r in results]

    except Exception as e:
        logger.error(f"‚ùå Erreur probe search: {e}", exc_info=True)
        return []


# ============================================================================
# Extraction Dynamique de Vocabulaire
# ============================================================================

def extract_vocabulary_from_search_results(
    search_results: List[dict],
    user_question: str
) -> ExtractedVocabulary:
    """
    Extrait le vocabulaire pertinent des documents trouv√©s.

    M√©thodes d'extraction:
    1. Termes capitalis√©s (noms propres, syst√®mes, acronymes)
    2. Termes r√©p√©t√©s dans plusieurs r√©sultats (haute fr√©quence)
    3. Termes des titres de documents (souvent cl√©s)
    4. Termes proches des mots de la question (contexte s√©mantique)

    Args:
        search_results: R√©sultats de probe_search
        user_question: Question originale

    Returns:
        ExtractedVocabulary avec termes class√©s par pertinence
    """
    if not search_results:
        return ExtractedVocabulary()

    extracted_terms = []
    term_sources = {}
    question_words = set(user_question.lower().split()) - FRENCH_STOPWORDS

    for result in search_results:
        content = result.get("content", "")
        title = result.get("document_title", "")

        # 1. Termes capitalis√©s (syst√®mes, noms propres, acronymes)
        capitalized = re.findall(r'\b[A-Z][a-zA-Z]{2,}(?:\s+[A-Z][a-zA-Z]+)*\b', content)
        for term in capitalized:
            if term.lower() not in question_words and len(term) > 2:
                extracted_terms.append(term)
                if term not in term_sources:
                    term_sources[term] = title

        # 2. Acronymes (tout en majuscules, 2-6 lettres)
        acronyms = re.findall(r'\b[A-Z]{2,6}\b', content)
        for acro in acronyms:
            if acro.lower() not in question_words:
                extracted_terms.append(acro)
                if acro not in term_sources:
                    term_sources[acro] = title

        # 3. Termes des titres (filtr√©s pour exclure m√©tadonn√©es)
        title_words = [w for w in title.split() if len(w) > 3 and w.lower() not in FRENCH_STOPWORDS]
        for tw in title_words:
            # Filtrer les termes non pertinents (noms de fichiers, mots g√©n√©riques)
            if tw.lower() not in question_words and is_valid_vocabulary_term(tw):
                extracted_terms.append(tw)
                if tw not in term_sources:
                    term_sources[tw] = title

        # 4. Termes proches des mots de la question (contexte)
        for word in question_words:
            if len(word) > 3:
                content_lower = content.lower()
                for match in re.finditer(re.escape(word), content_lower):
                    start = max(0, match.start() - 50)
                    end = min(len(content), match.end() + 50)
                    context = content[start:end]
                    # Extraire mots significatifs du contexte
                    nearby_words = re.findall(r'\b\w{4,}\b', context)
                    for nw in nearby_words:
                        if nw.lower() not in question_words and nw.lower() not in FRENCH_STOPWORDS:
                            extracted_terms.append(nw)
                            if nw not in term_sources:
                                term_sources[nw] = title

    # Compter et classer les termes
    term_counts = Counter(t.lower() for t in extracted_terms)

    # Garder les termes qui apparaissent au moins 2 fois (ou 1 fois si capitalis√©/acronyme)
    # ET qui passent le filtre de validit√© (pas de m√©tadonn√©es/noms de fichiers)
    ranked_terms = []
    seen = set()
    for term, count in term_counts.most_common(20):  # Plus de candidats pour compenser filtrage
        if term not in seen:
            # Retrouver la forme originale (avec casse)
            original_form = next((t for t in extracted_terms if t.lower() == term), term)
            # Filtrer les termes non pertinents (m√©tadonn√©es, noms de fichiers, mots g√©n√©riques)
            if not is_valid_vocabulary_term(original_form):
                continue
            if count >= 2 or (original_form.isupper() and len(original_form) <= 6):
                ranked_terms.append(original_form)
                seen.add(term)

    # Context snippets pour le LLM
    context_snippets = [r.get("content", "")[:300] for r in search_results[:3]]

    return ExtractedVocabulary(
        terms=ranked_terms[:10],
        context_snippets=context_snippets,
        term_sources={k: v for k, v in term_sources.items() if k.lower() in seen}
    )


# ============================================================================
# G√©n√©ration de Suggestions via LLM
# ============================================================================

LLM_GENERIC_PROMPT = """Tu reformules des questions pour am√©liorer la recherche documentaire.

QUESTION UTILISATEUR: "{question}"

INTENTION D√âTECT√âE: {detected_intent}

DOCUMENTS TROUV√âS:
{document_context}

TERMES CL√âS EXTRAITS: {extracted_terms}

R√àGLES CRITIQUES:
1. PR√âSERVE L'INTENTION: Si l'utilisateur demande "comment configurer", garde "configurer" (pas "comprendre" ou "expliquer")
2. AJOUTE DE LA SP√âCIFICIT√â avec les termes extraits des documents
3. NE CHANGE PAS LE SENS de la question
4. MAX 15 mots par suggestion
5. UTILISE UNIQUEMENT les termes extraits (pas d'invention)

EXEMPLES CORRECTS:
- "comment configurer ?" + termes=[SSO, LDAP] ‚Üí "Comment configurer le SSO LDAP ?"
- "√ßa marche pas" + termes=[OAuth, authentification] ‚Üí "Pourquoi l'authentification OAuth ne fonctionne pas ?"
- "c'est quoi le truc" + termes=[JWT, token] ‚Üí "Qu'est-ce que le JWT ?"

EXEMPLES INCORRECTS (√† √©viter):
- "comment configurer ?" ‚Üí "Qu'est-ce que le SSO ?" (change l'intention configurer‚Üíexpliquer)
- "√ßa marche comment" ‚Üí "Comment fonctionne OAuth ?" si l'utilisateur voulait r√©soudre un probl√®me

R√âPONDS EN JSON:
{{
  "needs_reformulation": true/false,
  "reasoning": "Explication en 1 phrase",
  "suggestions": [
    {{"text": "Question reformul√©e", "reason": "Utilise le terme X pour plus de pr√©cision"}}
  ]
}}

NE REFORMULE PAS SI:
- La question est d√©j√† sp√©cifique et claire
- Les documents ne sugg√®rent pas de meilleur vocabulaire
- L'intention ne peut pas √™tre d√©termin√©e"""


async def generate_llm_suggestions(
    question: str,
    vocabulary: ExtractedVocabulary,
    timeout: float = None
) -> Tuple[bool, List[ReformulationSuggestion], str]:
    """
    G√©n√®re des suggestions de reformulation via LLM avec contexte documentaire.

    Args:
        question: Question originale
        vocabulary: Vocabulaire extrait des documents
        timeout: Timeout en secondes (d√©faut: config)

    Returns:
        (needs_reformulation, suggestions, reasoning)
    """
    if timeout is None:
        timeout = REFORMULATION_LLM_TIMEOUT

    try:
        from app.utils.generic_llm_provider import get_generic_llm_model

        model = get_generic_llm_model()
        api_url = model.api_url.rstrip('/')

        # D√©tecter l'intention pour pr√©server le sens
        intent_type, preserve_verb, intent_label = detect_intent(question)
        detected_intent = f"{intent_label}"
        if preserve_verb:
            detected_intent += f" (pr√©server le verbe: {preserve_verb})"

        logger.debug(f"üéØ Intention d√©tect√©e: {intent_type} -> {intent_label}")

        # Construire le prompt
        document_context = "\n---\n".join(vocabulary.context_snippets) if vocabulary.context_snippets else "Aucun document trouv√©"
        extracted_terms = ", ".join(vocabulary.terms) if vocabulary.terms else "Aucun terme extrait"

        prompt = LLM_GENERIC_PROMPT.format(
            question=question,
            detected_intent=detected_intent,
            document_context=document_context,
            extracted_terms=extracted_terms
        )

        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(
                f"{api_url}/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {model.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.2,
                    "max_tokens": 500
                }
            )
            response.raise_for_status()
            result = response.json()

        content = result["choices"][0]["message"]["content"].strip()

        # Parser JSON (g√©rer les code blocks markdown)
        if content.startswith("```"):
            content = re.sub(r'^```json?\s*', '', content)
            content = re.sub(r'\s*```$', '', content)

        analysis = json.loads(content)

        needs_reformulation = analysis.get("needs_reformulation", False)
        reasoning = analysis.get("reasoning", "")

        suggestions = []
        for s in analysis.get("suggestions", [])[:3]:
            suggestions.append(ReformulationSuggestion(
                text=s.get("text", ""),
                type="llm_suggestion",
                reason=s.get("reason", ""),
                source_document=None
            ))

        logger.info(
            f"ü§ñ LLM reformulation: needs={needs_reformulation}, "
            f"suggestions={len(suggestions)}"
        )

        return (needs_reformulation, suggestions, reasoning)

    except json.JSONDecodeError as e:
        logger.warning(f"Erreur parsing JSON LLM: {e}")
        return (False, [], f"Erreur parsing: {e}")

    except httpx.TimeoutException:
        logger.warning(f"Timeout LLM ({timeout}s)")
        raise  # Propager pour utiliser fallback

    except Exception as e:
        logger.error(f"Erreur LLM suggestions: {e}", exc_info=True)
        return (False, [], f"Erreur: {e}")


# ============================================================================
# D√©tection d'Intention (pour pr√©servation dans reformulations)
# ============================================================================

# Patterns d'intention avec action associ√©e
INTENT_PATTERNS = {
    "howto_configure": {
        "patterns": [
            r"comment\s+(configurer|param√©trer|r√©gler|ajuster|modifier\s+les?\s+param√®tres?)",
            r"(configuration|param√©trage)\s+de",
        ],
        "label": "Configuration/Param√©trage",
        "preserve_verb": "configurer"
    },
    "howto_create": {
        "patterns": [
            r"comment\s+(cr√©er|ajouter|mettre\s+en\s+place|installer|g√©n√©rer)",
            r"(cr√©ation|ajout|installation)\s+d",
        ],
        "label": "Cr√©ation/Installation",
        "preserve_verb": "cr√©er"
    },
    "howto_fix": {
        "patterns": [
            r"comment\s+(r√©parer|corriger|r√©soudre|fixer|d√©bugger)",
            r"(ne\s+(marche|fonctionne)\s+(pas|plus))",
            r"(erreur|probl√®me|bug|√©chec)\s+(avec|de|sur)",
            r"√ßa\s+(marche|fonctionne)\s+(pas|plus)",
        ],
        "label": "R√©solution de probl√®me",
        "preserve_verb": "r√©soudre"
    },
    "howto_use": {
        "patterns": [
            r"comment\s+(utiliser|employer|se\s+servir\s+de)",
            r"(utilisation|usage)\s+de",
        ],
        "label": "Utilisation",
        "preserve_verb": "utiliser"
    },
    "explain": {
        "patterns": [
            r"c['']?est\s+quoi",
            r"qu['']?est[- ]ce\s+que",
            r"(d√©finition|signification)\s+de",
            r"√†\s+quoi\s+sert",
        ],
        "label": "Explication/D√©finition",
        "preserve_verb": None
    },
    "locate": {
        "patterns": [
            r"o√π\s+(trouver|est|se\s+trouve)",
            r"dans\s+quelle?\s+(section|partie|menu)",
        ],
        "label": "Localisation",
        "preserve_verb": "trouver"
    },
    "compare": {
        "patterns": [
            r"(diff√©rence|comparaison)\s+entre",
            r"(quel|quelle)\s+est\s+(la\s+diff√©rence|mieux)",
        ],
        "label": "Comparaison",
        "preserve_verb": None
    },
}


def detect_intent(question: str) -> Tuple[str, Optional[str], str]:
    """
    D√©tecte l'intention de la question pour pr√©server le sens dans les reformulations.

    Returns:
        (intent_type, preserve_verb, human_label)
        - intent_type: cl√© technique (howto_configure, explain, etc.)
        - preserve_verb: verbe √† pr√©server dans les reformulations (ou None)
        - human_label: description lisible pour le prompt LLM
    """
    question_lower = question.lower()

    for intent_type, config in INTENT_PATTERNS.items():
        for pattern in config["patterns"]:
            if re.search(pattern, question_lower, re.IGNORECASE):
                return (
                    intent_type,
                    config.get("preserve_verb"),
                    config["label"]
                )

    # Intention g√©n√©rique si aucun pattern ne matche
    return ("generic", None, "Question g√©n√©rale")


# ============================================================================
# Fallback: Suggestions bas√©es sur termes extraits (Am√©lior√©)
# ============================================================================

# Patterns de questions fran√ßaises pour reformulation intelligente
QUESTION_PATTERNS = {
    "comment": {
        "patterns": [
            "Comment {action} {term} ?",
            "Quelle est la proc√©dure pour {action} avec {term} ?",
            "Comment fonctionne {term} ?",
        ],
        "verbs": ["faire", "configurer", "utiliser", "cr√©er", "modifier", "g√©rer"]
    },
    "pourquoi": {
        "patterns": [
            "Pourquoi {term} {verb} ?",
            "Quelle est la raison de {term} ?",
            "Pour quelle raison {term} est {adjective} ?",
        ],
        "verbs": ["fonctionne", "est n√©cessaire", "doit √™tre", "a √©t√© con√ßu"]
    },
    "quoi": {
        "patterns": [
            "Qu'est-ce que {term} ?",
            "Quelle est la d√©finition de {term} ?",
            "√Ä quoi sert {term} ?",
        ],
        "verbs": []
    },
    "ou": {
        "patterns": [
            "O√π trouver {term} ?",
            "Dans quelle section est {term} ?",
            "O√π se situe {term} dans le syst√®me ?",
        ],
        "verbs": []
    },
    "generic": {
        "patterns": [
            "Pouvez-vous expliquer {term} ?",
            "Quelles sont les informations sur {term} ?",
            "Comment {term} fonctionne-t-il ?",
        ],
        "verbs": []
    }
}


def detect_question_type(question: str) -> str:
    """D√©tecte le type de question pour choisir le bon pattern de reformulation."""
    question_lower = question.lower().strip()

    if question_lower.startswith("comment"):
        return "comment"
    elif question_lower.startswith("pourquoi"):
        return "pourquoi"
    elif "quoi" in question_lower or "qu'est" in question_lower or "c'est quoi" in question_lower:
        return "quoi"
    elif question_lower.startswith("o√π") or question_lower.startswith("ou "):
        return "ou"
    else:
        return "generic"


def extract_action_from_question(question: str) -> Optional[str]:
    """Extrait l'action principale de la question si pr√©sente."""
    question_lower = question.lower()

    # Verbes d'action courants
    action_verbs = [
        "faire", "configurer", "utiliser", "cr√©er", "modifier", "supprimer",
        "ajouter", "g√©rer", "installer", "d√©sinstaller", "activer", "d√©sactiver",
        "r√©parer", "corriger", "am√©liorer", "optimiser", "r√©soudre", "remettre"
    ]

    for verb in action_verbs:
        if verb in question_lower:
            return verb

    return None


def generate_term_based_suggestions(
    question: str,
    vocabulary: ExtractedVocabulary
) -> List[ReformulationSuggestion]:
    """
    G√©n√®re des suggestions intelligentes bas√©es sur les termes extraits.
    Utilis√© comme fallback si le LLM timeout.

    Am√©liorations par rapport √† la version basique:
    1. D√©tection du type de question (comment, pourquoi, quoi, etc.)
    2. Extraction de l'action de la question originale
    3. G√©n√©ration de reformulations naturelles avec les termes extraits
    4. Suggestions vari√©es (pas juste "concernant X")

    Args:
        question: Question originale
        vocabulary: Vocabulaire extrait

    Returns:
        Liste de suggestions (max 3)
    """
    if not vocabulary.terms:
        return []

    suggestions = []
    question_lower = question.lower()
    question_type = detect_question_type(question)
    action = extract_action_from_question(question)

    # Filtrer les termes non pr√©sents dans la question
    relevant_terms = [t for t in vocabulary.terms if t.lower() not in question_lower]

    if not relevant_terms:
        # Si tous les termes sont d√©j√† dans la question, sugg√©rer des clarifications
        if vocabulary.terms:
            term = vocabulary.terms[0]
            source_doc = vocabulary.term_sources.get(term, "documents trouv√©s")
            suggestions.append(ReformulationSuggestion(
                text=f"Pouvez-vous pr√©ciser votre question concernant {term} ?",
                type="clarification",
                reason="Besoin de pr√©cision sur le contexte",
                source_document=source_doc
            ))
        return suggestions

    patterns = QUESTION_PATTERNS.get(question_type, QUESTION_PATTERNS["generic"])["patterns"]

    for i, term in enumerate(relevant_terms[:3]):
        source_doc = vocabulary.term_sources.get(term, "documents trouv√©s")

        if i == 0:
            # Premi√®re suggestion: reformulation directe avec le terme le plus pertinent
            if question_type == "comment" and action:
                suggestion_text = f"Comment {action} {term} ?"
                reason = f"Reformulation avec le terme '{term}' des documents"
            elif question_type == "quoi":
                suggestion_text = f"Qu'est-ce que {term} et comment √ßa fonctionne ?"
                reason = f"Clarification du concept '{term}'"
            elif question_type == "pourquoi":
                suggestion_text = f"Pourquoi {term} est-il important ?"
                reason = f"Question sur l'importance de '{term}'"
            else:
                suggestion_text = f"Comment fonctionne {term} ?"
                reason = f"Question sur le fonctionnement de '{term}'"

            suggestions.append(ReformulationSuggestion(
                text=suggestion_text,
                type="vocabulary",
                reason=reason,
                source_document=source_doc
            ))

        elif i == 1:
            # Deuxi√®me suggestion: question de d√©finition/explication
            suggestion_text = f"Pouvez-vous expliquer {term} et son utilisation ?"
            suggestions.append(ReformulationSuggestion(
                text=suggestion_text,
                type="clarification",
                reason=f"Demande d'explication sur '{term}'",
                source_document=source_doc
            ))

        elif i == 2:
            # Troisi√®me suggestion: question pratique
            suggestion_text = f"Quelles sont les √©tapes pour utiliser {term} ?"
            suggestions.append(ReformulationSuggestion(
                text=suggestion_text,
                type="expansion",
                reason=f"Question pratique sur '{term}'",
                source_document=source_doc
            ))

    logger.info(f"üìù Fallback: {len(suggestions)} suggestions g√©n√©r√©es (type={question_type}, action={action})")
    return suggestions[:3]


# ============================================================================
# Point d'Entr√©e Principal
# ============================================================================

async def analyze_and_suggest_reformulation(
    question: str,
    db_pool,
    universe_ids: Optional[List[UUID]] = None,
    conversation_context: Optional[Dict] = None
) -> ReformulationResult:
    """
    Point d'entr√©e principal pour l'analyse de reformulation.

    Flow:
    1. Score structurel (heuristiques g√©n√©riques)
    2. Si score bas ‚Üí Probe search
    3. Extraction vocabulaire dynamique
    4. LLM suggestions (avec timeout)
    5. Fallback sur termes si timeout

    Args:
        question: Question utilisateur
        db_pool: Pool de connexions DB
        universe_ids: Filtrage par univers
        conversation_context: Contexte conversationnel (optionnel)

    Returns:
        ReformulationResult avec suggestions √©ventuelles
    """
    if not REFORMULATION_ENABLED:
        return ReformulationResult(
            needs_reformulation=False,
            reasoning="Reformulation d√©sactiv√©e",
            analyzed_by="disabled"
        )

    # 1. Score structurel
    structural_score = compute_structural_score(question)
    logger.info(f"üìä Score structurel: {structural_score:.3f}, seuil: {REFORMULATION_HEURISTIC_THRESHOLD}")

    # Fast path si question structurellement OK
    if structural_score >= REFORMULATION_HEURISTIC_THRESHOLD:
        logger.info(f"‚úÖ Fast path: question structurellement OK")
        return ReformulationResult(
            needs_reformulation=False,
            reasoning="Question structurellement claire",
            analyzed_by="heuristics"
        )

    # 2. Probe search pour obtenir du contexte
    logger.info(f"üîç Probe search (k={REFORMULATION_PROBE_K})")
    probe_results = await probe_search(
        question=question,
        db_pool=db_pool,
        universe_ids=universe_ids
    )

    if not probe_results:
        logger.warning("‚ö†Ô∏è Probe search: aucun r√©sultat")
        return ReformulationResult(
            needs_reformulation=False,
            reasoning="Aucun document trouv√© pour contexte",
            analyzed_by="probe_search"
        )

    # 3. Extraction vocabulaire dynamique
    vocabulary = extract_vocabulary_from_search_results(probe_results, question)
    logger.info(f"üìö Vocabulaire extrait: {len(vocabulary.terms)} termes")

    # 4. LLM suggestions avec timeout
    try:
        needs_reformulation, suggestions, reasoning = await asyncio.wait_for(
            generate_llm_suggestions(question, vocabulary),
            timeout=REFORMULATION_LLM_TIMEOUT
        )

        return ReformulationResult(
            needs_reformulation=needs_reformulation,
            suggestions=suggestions,
            extracted_terms=vocabulary.terms,
            reasoning=reasoning,
            analyzed_by="llm"
        )

    except asyncio.TimeoutError:
        logger.warning(f"‚è±Ô∏è Timeout LLM ({REFORMULATION_LLM_TIMEOUT}s), fallback sur termes")

        # 5. Fallback: suggestions bas√©es sur termes extraits
        suggestions = generate_term_based_suggestions(question, vocabulary)

        return ReformulationResult(
            needs_reformulation=len(suggestions) > 0,
            suggestions=suggestions,
            extracted_terms=vocabulary.terms,
            reasoning="Suggestions bas√©es sur vocabulaire extrait (timeout LLM)",
            analyzed_by="fallback"
        )


# ============================================================================
# Export
# ============================================================================

__all__ = [
    "ReformulationResult",
    "ReformulationSuggestion",
    "ExtractedVocabulary",
    "analyze_and_suggest_reformulation",
    "compute_structural_score",
    "probe_search",
    "extract_vocabulary_from_search_results",
    "generate_llm_suggestions",
    "generate_term_based_suggestions",
    "detect_intent",
    "INTENT_PATTERNS",
    "REFORMULATION_ENABLED",
    "REFORMULATION_HEURISTIC_THRESHOLD",
]
