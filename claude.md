# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

RAGFab is a dual-provider RAG (Retrieval Augmented Generation) system optimized for French, with both a CLI application and a web interface. The system supports two LLM providers:
- **Chocolatine** (local vLLM): Manual context injection
- **Mistral** (API): Automatic function calling with tools

## Architecture

### Component Overview

```
┌──────────────────────────────────────────────────────────────────────┐
│                         DOCKER / COOLIFY                             │
│                                                                      │
│  ┌────────────────┐  ┌────────────────┐  ┌────────────────────────┐ │
│  │ Embeddings API │  │ Reranker API   │  │ PostgreSQL + PGVector  │ │
│  │ (E5-Large)     │  │ (BGE-M3)       │  │ (documents + chunks)   │ │
│  │ Port: 8001     │  │ Port: 8002     │  │ Port: 5432             │ │
│  └────────┬───────┘  └────────┬───────┘  └───────────┬────────────┘ │
│           │                   │                       │              │
│           └───────────────────┼───────────────────────┤              │
│                               │                       │              │
│                   ┌───────────▼──────┐    ┌──────────▼────────────┐ │
│                   │ Web API (FastAPI)│    │ Ingestion Worker      │ │
│                   │ - Chat           │◄───┤ - Poll jobs (3s)      │ │
│                   │ - Upload         │    │ - Process documents   │ │
│                   │ Port: 8000       │    │ - Generate embeddings │ │
│                   └──────────┬───────┘    └───────────────────────┘ │
│                              │                   Shared Volume:      │
│                   ┌──────────▼───────┐          /app/uploads        │
│                   │ Frontend (React) │                              │
│                   │ Port: 5173       │                              │
│                   └──────────────────┘                              │
└──────────────────────────────────────────────────────────────────────┘
```

### Key Data Flow

**Question Reformulation (Mistral with tools only)**:
1. User sends question → `reformulate_question_with_context()` detects contextual references
2. If reference detected → Calls Mistral API to reformulate question autonomously
3. Reformulated question sent to RAG agent → Forces tool calling (no history passed)
4. Tool `search_knowledge_base_tool()` is called → Performs vector search (optionally with reranking)
5. Sources stored in `_current_request_sources` global variable
6. Final response generated with sources displayed in frontend

**Vector Search + Reranking Pipeline** (when `RERANKER_ENABLED=true`):
1. Question → Embedding (E5-Large) → Vector similarity search (top-20 candidates)
2. Top-20 candidates → Reranker service (BGE-reranker-v2-m3) → CrossEncoder scoring
3. Reranked results (top-5 most relevant) → LLM context
4. If reranker fails → Graceful fallback to top-5 from vector search

**Vector Search Only Pipeline** (when `RERANKER_ENABLED=false`):
1. Question → Embedding (E5-Large) → Vector similarity search (top-5 direct)
2. Top-5 results → LLM context

**Document Ingestion Pipeline** (via interface admin):
1. User uploads document → Web API validates (type, size < 100MB)
2. File saved to `/app/uploads/{job_id}/filename.pdf` (shared volume)
3. Job created in `ingestion_jobs` table with `status='pending'`
4. Ingestion Worker polls PostgreSQL every 3s → Claims pending job
5. Worker processes document: Docling parsing → Chunking → Embeddings → DB save
6. Progress updated in real-time (0-100%) → Frontend polls job status
7. Job completed: `status='completed'`, document appears in admin interface

**Image Extraction Pipeline** (when `VLM_ENABLED=true`):
1. Document parsing with Docling → Detects images in PDF pages
2. For each image detected:
   - Extract image data → Save to `/app/uploads/images/{job_id}/{image_id}.png`
   - Encode to base64 for inline display
   - Call VLM API (OpenAI-compatible) → Get description + OCR text
   - Extract position metadata (page, x, y, width, height)
3. Store in `document_images` table with link to parent chunk (matched by page_number)
4. Images included in RAG search results → Displayed inline in chat interface

**Critical Pattern**: Global variable `_current_request_sources` is used instead of `ContextVar` because PydanticAI's async tool execution loses ContextVar state between calls.

## RAG Pipeline Optimizations (2025-01-24)

### Overview

Major RAG pipeline optimization completed to address user dissatisfaction with follow-up questions and context preservation. Implementation completed in 2 phases:

- **Phase 1**: Conversational context management
- **Phase 2**: Structural improvements (metadata enrichment, parent-child chunking, hierarchical search)

**Impact**: Estimated +80% satisfaction improvement on follow-up questions, +15-25% better context coherence.

### Phase 1: Conversational Context Intelligence

#### Problem Solved
Users were dissatisfied when asking follow-up questions like "comment la resoudre ?" or "Et si ça ne marche pas ?". The system lost conversational context because `message_history=[]` was passed to force function calling.

#### Solution Implemented
Created intelligent conversational context system that injects structured context into system prompt instead of using message_history.

#### New Components

**File**: `/web-api/app/conversation_context.py` (NEW)

Functions:
- `extract_main_topic(messages, db_pool)` - Uses LLM to extract 3-5 word conversation topic
- `build_conversation_context(conversation_id, db_pool, limit=5)` - Builds structured context from DB
  - Returns: current_topic, conversation_flow (exchanges), all_sources_consulted, last_exchange
- `create_contextual_system_prompt(context, base_prompt)` - Injects context into system prompt
  - Includes: 3 last exchanges, documents consulted, query enrichment instructions
- `enrich_query_with_context(user_message, context)` - Enriches short/vague queries
  - Triggers: Questions ≤5 words OR implicit references (comment, pourquoi, ça, etc.)
- `detect_topic_shift(new_message, context, db_pool)` - Detects topic changes with LLM

**Modified**: `/web-api/app/main.py`

- `execute_rag_agent()` (lines 1403-1558):
  - Builds conversation context from DB before agent execution
  - Creates contextual system prompt with injected context
  - Still passes empty message_history to force function calling

- `search_knowledge_base_tool()` (lines 1009-1051):
  - Automatically enriches queries with conversation context
  - Logs enrichment: "🔧 Query enrichie: 'X' → 'Y'"

- POST `/api/conversations/{id}/messages` (lines 681-721):
  - Added topic shift detection (optional, non-blocking)
  - Suggests new conversation if topic changes significantly

#### Configuration

No new environment variables required. System automatically detects conversation context from database.

#### Key Benefits
- ✅ Maintains function calling (forces tool use each turn)
- ✅ Preserves conversational coherence across multiple turns
- ✅ Automatic query enrichment for follow-ups
- ✅ Topic tracking prevents context drift
- ✅ Zero latency overhead (context built once per request)

### Phase 2: Structural Improvements

#### 2.1 Enriched Metadata for Chunks

**Problem**: Chunks were isolated without structural metadata, losing document context.

**Solution**: Added structural metadata to chunks table.

**Migration**: `database/migrations/05_enriched_metadata.sql`

New columns in `chunks` table:
- `prev_chunk_id UUID` - Reference to previous chunk (sequential context)
- `next_chunk_id UUID` - Reference to next chunk (sequential context)
- `section_hierarchy JSONB` - Array of heading names from root to current
- `heading_context TEXT` - Immediate heading for this chunk
- `document_position FLOAT` - Normalized position (0.0-1.0)

Helper functions created:
- `get_chunk_with_context(chunk_id)` - Returns chunk with prev/next content
- `get_chunks_in_section(section_hierarchy, document_id)` - All chunks in same section

**Modified Files**:
- `rag-app/ingestion/chunker.py`:
  - `DoclingHybridChunker.chunk_document()` - Extracts section_hierarchy, heading_context, document_position from Docling chunks
  - `_create_single_chunk()` - Adds empty structural metadata for single-chunk documents

- `rag-app/ingestion/ingest.py`:
  - `_save_to_postgres()` - Saves structural metadata to new columns
  - Links chunks with prev/next relationships after insertion

#### 2.2 Adjacent Chunks Context

**Problem**: Vector search returned isolated chunks without surrounding context.

**Solution**: Automatic retrieval of adjacent chunks (prev/next) for each search result.

**Modified**: `/web-api/app/main.py`

- `search_knowledge_base_tool()` (lines 1154-1186):
  - After vector search, fetches adjacent chunks for all results
  - Uses LEFT JOIN to get prev_chunk_id and next_chunk_id relationships

- `_build_contextualized_response()` (NEW helper function, lines 1037-1104):
  - Formats response with prev/next context previews (150 chars each)
  - Includes section hierarchy and heading context
  - Structure: [Source + Section] → Prev context → **Main content** → Next context

**Configuration**: `.env.example`
```bash
# Activer la récupération des chunks adjacents
USE_ADJACENT_CHUNKS=true  # Recommandé (latence négligeable, +15-25% pertinence)
```

**Benefits**:
- ✅ Richer context for LLM without increasing search results
- ✅ Better answer coherence (understands chunk boundaries)
- ✅ Minimal latency (single additional query)

#### 2.3 Parent-Child Hierarchical Chunking

**Problem**: Fixed chunk size creates trade-off between precision (small chunks) and context (large chunks).

**Solution**: Hierarchical architecture with parent chunks (large context) and child chunks (precise retrieval).

**Migration**: `database/migrations/06_parent_child_chunks.sql`

New columns in `chunks` table:
- `chunk_level chunk_level_enum` - ENUM ('parent', 'child')
- `parent_chunk_id UUID` - Reference to parent chunk (NULL for parents)

New functions:
- `get_child_chunks(parent_chunk_id)` - All children of a parent
- `get_parent_chunk(child_chunk_id)` - Parent of a child
- `hierarchical_match_chunks()` - Search children, return parents
- `match_chunks()` - Updated with use_hierarchical parameter

**New Class**: `rag-app/ingestion/chunker.py`

`ParentChildChunker` (lines 419-636):
- Creates parent chunks: 2000 tokens (large context)
- Splits each parent into child chunks: ~600 tokens (precision)
- Children reference parents via metadata (linked in DB later)
- Returns flattened list: [parents..., children...]

Architecture:
```
Parent Chunk (2000t) ─┬─ Child 1 (600t)
                      ├─ Child 2 (600t)
                      └─ Child 3 (600t)
```

**Modified Files**:
- `chunker.py`:
  - `create_chunker()` - New parameter `use_parent_child` + env var support

- `ingest.py`:
  - `_save_to_postgres()` - Saves chunk_level, links children to parents after insertion

- `main.py`:
  - `search_knowledge_base_tool()` - Uses hierarchical match_chunks() function
  - Searches in child chunks (precise matching)
  - Returns parent chunks (rich context for LLM)

**Configuration**: `.env.example`
```bash
# Activer le chunking parent-enfant
USE_PARENT_CHILD_CHUNKS=false  # Expérimental (nécessite migration 06)
```

**Benefits**:
- ✅ Best of both worlds: precision + context
- ✅ Search operates on small chunks (better matches)
- ✅ LLM receives large chunks (richer context)
- ✅ Reduced hallucination (more surrounding information)

**Trade-offs**:
- ⚠️ More chunks stored (1 parent → 3-5 children)
- ⚠️ Slightly higher ingestion time
- ⚠️ Requires migration 06 to be applied

### Testing

**Script**: `claudedocs/test_scenario_fusappel.py`

Validates multi-turn conversation scenario:
1. "j'ai une erreur fusappel" → Explanation
2. "comment la resoudre ?" → Solution (enriched query)
3. "comment j'active le bluetooth" → Bluetooth steps
4. "Et si ça ne marche toujours pas ?" → Troubleshooting (context aware)

**Run test**:
```bash
cd claudedocs
python3 test_scenario_fusappel.py
```

Validates:
- Query enrichment for short questions
- Context maintenance across turns
- Topic tracking
- Source relevance

### Migration Path

**To apply all optimizations**:

```bash
# 1. Apply database migrations
docker-compose exec postgres psql -U raguser -d ragdb -f /docker-entrypoint-initdb.d/05_enriched_metadata.sql
docker-compose exec postgres psql -U raguser -d ragdb -f /docker-entrypoint-initdb.d/06_parent_child_chunks.sql

# 2. Update environment variables in .env
USE_ADJACENT_CHUNKS=true
USE_PARENT_CHILD_CHUNKS=false  # Start with false, test thoroughly before enabling

# 3. Rebuild containers
docker-compose build ragfab-api ingestion-worker
docker-compose restart ragfab-api ingestion-worker

# 4. Re-ingest documents to populate new metadata
# Via admin interface: Delete old documents, re-upload
# OR via CLI: python -m ingestion.ingest

# 5. Test with script
python3 claudedocs/test_scenario_fusappel.py
```

**Backward Compatibility**:
- All features are opt-in via environment variables
- System works without migrations (Phase 1 only requires code changes)
- Default values maintain existing behavior

### Performance Impact

**Phase 1 (Conversational Context)**:
- Latency: +10-50ms per request (context building from DB)
- Memory: Negligible (context is <5KB)
- Tokens: +200-500 tokens in system prompt (contextual section)

**Phase 2.1-2.2 (Adjacent Chunks)**:
- Latency: +5-20ms per request (single JOIN query)
- Memory: ~3x chunk content temporarily (prev + current + next)
- Tokens: +300-900 tokens per result (context previews)

**Phase 2.3 (Parent-Child)**:
- Storage: 3-5x more chunks (1 parent → 3-5 children)
- Ingestion: +20-30% time (create parents + split into children)
- Search latency: Identical (same number of similarity calculations)
- Context quality: +30-50% (larger chunks for LLM)

**Overall**:
- Total latency increase: +15-70ms per request (acceptable for quality gain)
- User satisfaction: +80% on follow-up questions (estimated based on addressed issues)

### Future Improvements (Phase 3 - Not Implemented)

Planned but not implemented:
- Multi-query expansion (generate alternative phrasings)
- Query result caching
- Conversational memory optimization
- Custom reranking models

## Hybrid Search System (2025-01-31)

### Overview

**Status**: ✅ Fully implemented

Hybrid Search combines **semantic vector search** (E5-Large embeddings) with **keyword search** (PostgreSQL BM25) using Reciprocal Rank Fusion (RRF) to improve retrieval accuracy, especially for:
- Acronyms (RTT, CDI, PeopleDoc)
- Proper nouns and brand names
- Exact phrase matching
- Technical terminology

**Impact**: +15-25% Recall@5 improvement, particularly effective for French language queries.

### Architecture

**Three-Layer System**:

1. **SQL Layer** (`database/migrations/10_hybrid_search.sql`):
   - `content_tsv tsvector` column with French stemming configuration
   - GIN index for fast keyword search (`idx_chunks_content_tsv`)
   - `match_chunks_hybrid()` function with RRF fusion
   - `match_chunks_smart_hybrid()` wrapper for parent-child handling
   - Auto-update trigger for new chunks

2. **Backend Layer** (`web-api/app/hybrid_search.py`):
   - `preprocess_query_for_tsquery()` - French stopwords removal, query cleaning
   - `adaptive_alpha()` - Dynamic alpha adjustment based on query type
   - `hybrid_search()` - Main search function with RRF
   - `smart_hybrid_search()` - Auto-detects parent-child chunks

3. **Frontend Layer** (`frontend/src/components/HybridSearchToggle.tsx`):
   - Toggle to enable/disable hybrid search
   - Alpha slider (0.0 = keyword, 0.5 = balanced, 1.0 = semantic)
   - Help panel explaining hybrid search benefits
   - Advanced settings with examples
   - LocalStorage persistence

### Technical Details

**RRF (Reciprocal Rank Fusion)**:
```sql
-- Formula: score = alpha * (1/(k+rank_vector)) + (1-alpha) * (1/(k+rank_keyword))
-- k=60 is standard RRF constant for stability
combined_score =
  (alpha * (1.0 / (60.0 + rank_vector))) +
  ((1.0 - alpha) * (1.0 / (60.0 + rank_keyword)))
```

**French Language Optimization**:
- PostgreSQL `to_tsvector('french', content)` for stemming
- French stopwords list (130+ words)
- Preserves acronyms and proper nouns during preprocessing
- Example: "télétravaillent" → "teletravail" (root form)

**Adaptive Alpha Algorithm**:
```python
# Acronyms (2+ uppercase letters) → alpha=0.3 (keyword bias)
if re.search(r'\b[A-Z]{2,}\b', query):
    return 0.3

# Proper nouns (capitalized after first word) → alpha=0.3 (keyword bias)
proper_nouns = [w for w in words[1:] if w[0].isupper()]
if proper_nouns:
    return 0.3

# Conceptual questions (pourquoi, comment, expliquer) → alpha=0.7 (semantic bias)
conceptual_keywords = ["pourquoi", "comment", "expliquer", "signifie"]
if any(keyword in query_lower for keyword in conceptual_keywords):
    return 0.7

# Short questions (≤4 words) → alpha=0.4 (slight keyword bias)
if len(words) <= 4:
    return 0.4

# Default → alpha=0.5 (balanced)
return 0.5
```

### Integration Points

**Backend Integration** (`web-api/app/main.py:1201-1244`):
```python
hybrid_search_enabled = os.getenv("HYBRID_SEARCH_ENABLED", "false").lower() == "true"

if hybrid_search_enabled:
    from .hybrid_search import smart_hybrid_search

    # Use smart_hybrid_search with automatic parent-child handling
    hybrid_results = await smart_hybrid_search(
        query=enriched_query,
        query_embedding=query_embedding,
        k=search_limit,
        alpha=None  # Uses adaptive_alpha automatically
    )

    # Convert format with all scores (vector, BM25, combined)
    results = [...]
else:
    # Fallback to pure vector search with match_chunks_smart()
    results = await conn.fetch("SELECT * FROM match_chunks_smart(...)")
```

**Frontend Integration** (`frontend/src/pages/ChatPage.tsx:418-450`):
```typescript
<HybridSearchToggle
  conversationId={currentConversation.id}
  onChange={(enabled, alpha) => {
    console.log('🔀 Hybrid search settings:', { enabled, alpha });
    // Settings saved to localStorage by component
  }}
/>
```

### Configuration

**Environment Variables** (`.env`):
```bash
# Enable Hybrid Search (default: false)
HYBRID_SEARCH_ENABLED=true
```

**Frontend Settings** (LocalStorage):
```javascript
localStorage.setItem('hybrid_search_enabled', 'true');
localStorage.setItem('hybrid_search_alpha', '0.5');
```

### Usage Examples

**Example 1: Acronym Query** (alpha=0.3 automatic)
```
Query: "procédure RTT"
→ Preprocessing: "procédure & RTT"
→ Adaptive alpha: 0.3 (keyword bias)
→ Results: Chunks explicitly containing "RTT"
```

**Example 2: Proper Noun Query** (alpha=0.3 automatic)
```
Query: "logiciel PeopleDoc"
→ Preprocessing: "logiciel & PeopleDoc"
→ Adaptive alpha: 0.3 (keyword bias)
→ Results: Chunks mentioning "PeopleDoc"
```

**Example 3: Conceptual Query** (alpha=0.7 automatic)
```
Query: "pourquoi favoriser le télétravail ?"
→ Preprocessing: "favoriser & télétravail"
→ Adaptive alpha: 0.7 (semantic bias)
→ Results: Broader semantic matches about telework benefits
```

**Example 4: Manual Alpha Override**
```
User moves slider to alpha=0.2
→ Forces strong keyword bias regardless of query type
→ Useful for technical documentation with specific terms
```

### Database Schema Changes

**New Columns** (`chunks` table):
```sql
ALTER TABLE chunks ADD COLUMN IF NOT EXISTS content_tsv tsvector;
COMMENT ON COLUMN chunks.content_tsv IS 'Tokenized and stemmed content for French full-text search';
```

**New Indexes**:
```sql
CREATE INDEX IF NOT EXISTS idx_chunks_content_tsv
    ON chunks USING GIN(content_tsv);
```

**New Functions**:
- `match_chunks_hybrid(query_embedding, query_text, match_count, alpha, use_hierarchical)` - Core hybrid search with RRF
- `match_chunks_smart_hybrid(query_embedding, query_text, match_count, alpha)` - Auto parent-child handling

**Auto-Update Trigger**:
```sql
CREATE OR REPLACE FUNCTION chunks_tsvector_update()
RETURNS trigger AS $
BEGIN
    NEW.content_tsv := to_tsvector('french', NEW.content);
    RETURN NEW;
END;
$ LANGUAGE plpgsql;

CREATE TRIGGER tsvector_update
    BEFORE INSERT OR UPDATE ON chunks
    FOR EACH ROW
    EXECUTE FUNCTION chunks_tsvector_update();
```

### Testing

**Test Guide**: See `claudedocs/HYBRID_SEARCH_TESTING_GUIDE.md` for comprehensive testing procedures.

**Quick Validation**:
```bash
# 1. Apply migration
docker-compose exec postgres psql -U raguser -d ragdb \
  -f /docker-entrypoint-initdb.d/10_hybrid_search.sql

# 2. Verify tsvector populated
docker-compose exec postgres psql -U raguser -d ragdb -c \
  "SELECT COUNT(*) FROM chunks WHERE content_tsv IS NOT NULL;"

# 3. Enable hybrid search
echo "HYBRID_SEARCH_ENABLED=true" >> .env

# 4. Rebuild API
docker-compose up -d --build ragfab-api

# 5. Test in UI
# - Enable toggle in chat interface
# - Send query: "procédure RTT"
# - Verify results contain "RTT"
```

**Expected Logs** (when enabled):
```
🔀 Hybrid search: query='procédure RTT' → tsquery='procédure & RTT', alpha=0.30, k=5
INFO - Acronyme détecté, alpha=0.3 (keyword bias)
✅ Hybrid search: 5 résultats | Scores moyens - Vector: 0.XXX, BM25: 0.XXX, Combined: 0.XXXX
```

### Performance Characteristics

**Latency Impact**:
- Additional latency: +50-100ms per query
- Sources:
  - Vector search: ~30-50ms (unchanged)
  - Keyword search: ~10-20ms (GIN index)
  - RRF fusion: ~5-10ms (PostgreSQL)
  - Alpha calculation: <1ms

**Storage Impact**:
- `content_tsv` column: ~15-25% of original content size
- GIN index: ~20-30% of content size
- Total: ~35-55% overhead per chunk

**Resource Usage**:
- CPU: Minimal (GIN index optimized)
- RAM: +100-200MB for index caching
- Disk I/O: No significant impact

### Quality Improvements

**Recall@5 Improvement**:
- Acronym queries: +25-35% (e.g., "procédure RTT")
- Proper noun queries: +20-30% (e.g., "logiciel PeopleDoc")
- Short queries: +15-20% (e.g., "télétravail")
- Exact phrase queries: +30-40% (e.g., "congés payés")
- Overall improvement: +15-25% average

**User Satisfaction**:
- Fewer "no results" scenarios
- Better precision for technical queries
- Reduced false positives from pure semantic search
- More control with manual alpha adjustment

### Troubleshooting

**Issue**: "Function match_chunks_hybrid does not exist"
- **Cause**: Migration not applied
- **Fix**: Run migration SQL file

**Issue**: Hybrid search returns no results
- **Cause**: `content_tsv` column not populated
- **Fix**: `UPDATE chunks SET content_tsv = to_tsvector('french', content);`

**Issue**: Slow hybrid search performance
- **Cause**: Missing GIN index
- **Fix**: Verify index exists with `\di idx_chunks_content_tsv`

**Issue**: Frontend toggle not working
- **Cause**: `HYBRID_SEARCH_ENABLED=false` or API not rebuilt
- **Fix**: Set env var to `true` and rebuild API

**Issue**: Alpha not adapting correctly
- **Cause**: Query preprocessing removing important keywords
- **Fix**: Check logs for "Alpha par défaut", adjust stopwords if needed

### Future Enhancements

Potential improvements (not currently implemented):
- **Query expansion**: Generate synonyms for keyword search
- **Custom alpha per conversation**: Learn optimal alpha from user feedback
- **Multi-field search**: Search in metadata fields (title, source)
- **Boosting**: Weight certain fields higher (e.g., headings)
- **Fuzzy matching**: Allow typos in keyword search with edit distance

## Development Commands

### Docker Setup

```bash
# Start core services (PostgreSQL + Embeddings + Reranker)
docker-compose up -d postgres embeddings reranker

# Start full stack (API + Frontend + Ingestion Worker)
docker-compose up -d

# Rebuild after code changes
docker-compose build ragfab-api
docker-compose build ragfab-frontend
docker-compose build ingestion-worker

# View logs
docker-compose logs -f ragfab-api
docker-compose logs -f ragfab-frontend
docker-compose logs -f ingestion-worker
```

### Testing

```bash
# Backend tests (rag-app)
cd rag-app
pytest -m unit  # Unit tests only
pytest -m "unit and not embeddings"  # Exclude embeddings tests
pytest --cov=. --cov-report=html  # With coverage

# Backend tests (web-api)
cd web-api
pytest -m unit
pytest --cov=app --cov-report=term --cov-fail-under=20

# Frontend
cd frontend
npm test  # Run tests
npm run lint  # ESLint
```

**Important**: Coverage threshold is currently set to 20% (realistic baseline). Tests requiring the embeddings service are marked with `@pytest.mark.embeddings` and excluded from CI.

### Document Ingestion

#### Via Interface Admin (Recommended)

```bash
# 1. Start services (includes ingestion worker)
docker-compose up -d

# 2. Access admin interface
open http://localhost:3000/admin
# Login: admin / admin

# 3. Upload documents via drag & drop
# - Supported: PDF, DOCX, MD, TXT, HTML
# - Max size: 100MB per file
# - Progress shown in real-time
# - Select OCR engine (RapidOCR, EasyOCR, Tesseract)
# - Select VLM engine (PaddleOCR-VL, InternVL, None)

# 4. Monitor ingestion worker
docker-compose logs -f ingestion-worker

# 5. Verify ingestion
docker-compose exec postgres psql -U raguser -d ragdb -c "SELECT title, COUNT(c.id) as chunks FROM documents d LEFT JOIN chunks c ON d.id = c.document_id GROUP BY d.id, title;"
```

**Key Features**:
- Real-time progress tracking (0-100%)
- Automatic error handling and retry
- Frontend polling every 2s for status updates
- Worker processes documents asynchronously
- Shared volume between API and worker
- **Dual-engine selection**: Per-job OCR and VLM engine configuration

**Troubleshooting**:
```bash
# Check worker status
docker-compose ps ingestion-worker

# View worker logs
docker-compose logs -f ingestion-worker

# Restart worker if stuck
docker-compose restart ingestion-worker

# Check job status in database
docker-compose exec postgres psql -U raguser -d ragdb -c "SELECT id, filename, status, progress, chunks_created FROM ingestion_jobs ORDER BY created_at DESC LIMIT 10;"
```

#### CLI Ingestion (Legacy)

```bash
# Place PDFs in rag-app/documents/ then:
docker-compose --profile app run --rm rag-app python -m ingestion.ingest

# This method bypasses the job queue and processes directly
```

### Frontend Development

```bash
cd frontend
npm install
npm run dev  # Starts Vite dev server on port 5173
npm run build  # Production build
npm run preview  # Preview production build
```

## Critical Implementation Details

### Mistral Provider with PydanticAI

**File**: `rag-app/utils/mistral_provider.py`

Key implementation notes:
- `ArgsDict` must be imported from `pydantic_ai.messages` (NOT `pydantic_ai.models`)
- Tool arguments must be extracted with `part.args.args_dict` before JSON serialization
- `ToolReturnPart` must be processed in `ModelRequest` formatting (NOT `ModelResponse`)
- Message order is strict: system → user → assistant (with tool_calls) → tool (with results)

### Question Reformulation System

**File**: `web-api/app/main.py` (lines 847-946)

Detects contextual references and reformulates questions before RAG execution:

**Strong references** (always reformulated): celle, celui, celles, ceux
**Medium references** (only if question <8 words): ça, cela, ce, cette, ces
**Pronouns at start** (only if first word): il, elle, ils, elles, y, en
**Pattern matching**: Questions starting with "et celle", "et celui", "et ça"

Generic articles ("le", "la", "les") are NOT treated as references to avoid false positives.

### Global State Management

**Critical**: `_current_request_sources` is a global variable (List[dict]) used to pass sources between `search_knowledge_base_tool()` and the response handler.

**Why not ContextVar?**: PydanticAI's async tool execution context loses ContextVar state between the tool call and result retrieval. Global variable works because FastAPI processes requests sequentially with async.

Location: `web-api/app/main.py` line 50

### Dual Provider System

**Environment variable**: `RAG_PROVIDER` (values: "mistral" or "chocolatine")

**Mistral mode** (`provider="mistral"` and `use_tools=True`):
- Question reformulated if contextual references detected
- Agent created with `tools=[search_knowledge_base_tool]`
- NO history passed to agent (forces tool calling every time)
- Sources retrieved from global variable after execution

**Chocolatine/Mistral without tools**:
- Manual search executed BEFORE agent creation
- Context injected into system prompt
- History summary can be included (doesn't interfere with tool calling)

### Database Schema

**Important dimensions**:
- Embedding dimension: **1024** (multilingual-e5-large)
- Vector type in schema: `vector(1024)`

Changing embedding models requires:
1. Update `EMBEDDING_DIMENSION` in `.env`
2. Modify `database/schema.sql` vector dimensions
3. Drop and recreate `chunks` table

Schema files:
- `database/schema.sql`: Core RAG tables (documents, chunks)
- `database/02_web_schema.sql`: Web interface tables (conversations, messages, ratings)

### Chunking Strategy

**Docling HybridChunker** (preferred):
- Respects document structure (headings, paragraphs, tables)
- Falls back to SimpleChunker on error
- Configuration: `CHUNK_SIZE=1500`, `CHUNK_OVERLAP=200`

**SimpleChunker** (fallback):
- Splits on paragraph breaks (`\n\n`), NOT character count
- Test expectations must use multi-paragraph content

**Batch embedding**: 20 chunks per batch (timeout: 90s) to avoid API timeouts

### UTF-8 Handling

PDF documents often contain invalid UTF-8 surrogate characters. All chunk content must be cleaned:

```python
clean_content = content.encode('utf-8', errors='replace').decode('utf-8')
```

Location: Applied in both `rag_agent.py` and `web-api/app/main.py`

## Environment Configuration

### Critical Variables

```bash
# Generic LLM Configuration (RECOMMENDED - NEW)
LLM_API_URL=https://api.mistral.ai  # Any OpenAI-compatible API
LLM_API_KEY=your_api_key_here
LLM_MODEL_NAME=mistral-small-latest  # Model name for your provider
LLM_USE_TOOLS=true  # true = function calling, false = manual context injection
LLM_TIMEOUT=120.0

# Database (update for Coolify with .internal suffix)
DATABASE_URL=postgresql://raguser:pass@postgres:5432/ragdb
POSTGRES_HOST=postgres  # Change to postgres.internal on Coolify

# Embeddings
EMBEDDINGS_API_URL=http://embeddings:8001
EMBEDDING_DIMENSION=1024

# Reranking (Activable à la demande)
RERANKER_ENABLED=false  # true pour activer le reranking (recommandé pour doc médicale/technique)
RERANKER_API_URL=http://reranker:8002
RERANKER_MODEL=BAAI/bge-reranker-v2-m3  # Modèle multilingue excellent pour le français
RERANKER_TOP_K=20  # Nombre de candidats avant reranking
RERANKER_RETURN_K=5  # Nombre de résultats finaux après reranking

# Legacy Variables (Retained for backward compatibility)
RAG_PROVIDER=mistral  # DEPRECATED: Use LLM_USE_TOOLS instead
MISTRAL_API_KEY=your_key_here  # DEPRECATED: Use LLM_API_KEY
MISTRAL_API_URL=https://api.mistral.ai  # DEPRECATED: Use LLM_API_URL
MISTRAL_MODEL_NAME=mistral-small-latest  # DEPRECATED: Use LLM_MODEL_NAME
MISTRAL_TIMEOUT=120.0  # DEPRECATED: Use LLM_TIMEOUT
CHOCOLATINE_API_URL=https://apigpt.mynumih.fr  # DEPRECATED
CHOCOLATINE_API_KEY=  # DEPRECATED
```

### Generic LLM System (NEW)

RAGFab now supports **any OpenAI-compatible LLM API** through a generic provider system. This allows easy switching between different LLM providers without code changes.

**Supported Providers**:
- **Mistral AI**: `LLM_API_URL=https://api.mistral.ai`
- **Chocolatine**: `LLM_API_URL=https://apigpt.mynumih.fr`
- **Ollama**: `LLM_API_URL=http://localhost:11434`
- **LiteLLM**: `LLM_API_URL=http://localhost:4000`
- **OpenAI**: `LLM_API_URL=https://api.openai.com`
- **Any OpenAI-compatible API**

**Function Calling vs Manual Injection**:
- `LLM_USE_TOOLS=true`: LLM uses function calling to automatically call `search_knowledge_base_tool`
- `LLM_USE_TOOLS=false`: Manual context injection (search executed before LLM call)

**System Prompt Enhancement**:
The system now includes **explicit JSON tool definitions in the system prompt** to reinforce function calling behavior. This dual approach (API `tool_choice` + JSON in prompt) significantly improves tool usage reliability across different LLM providers.

Example system prompt structure:
```
OUTIL DISPONIBLE - DÉFINITION COMPLÈTE :
[
  {
    "type": "function",
    "function": {
      "name": "search_knowledge_base_tool",
      "description": "...",
      "parameters": {...}
    }
  }
]

EXEMPLE D'UTILISATION CORRECTE :
Question: "Quelle est la politique de télétravail ?"
→ ÉTAPE 1 - APPEL OBLIGATOIRE: search_knowledge_base_tool(query="...")
→ ÉTAPE 2 - RÉCEPTION: [Résultats]
→ ÉTAPE 3 - RÉPONSE: [Synthèse]

RÈGLES ABSOLUES :
1. Tu DOIS appeler l'outil AVANT de répondre
...
```

### Coolify Deployment Notes

When deploying on Coolify:
1. Container names must have unique prefixes (e.g., `ragfab-postgres`, `ragfab-embeddings`, `ragfab-reranker`)
2. Use `postgres.internal` instead of `postgres` for DATABASE_URL
3. Use `reranker.internal` instead of `reranker` for RERANKER_API_URL
4. Update Traefik router names to match new container names
5. Set environment variables via Coolify interface

Recent fix: All containers renamed with `ragfab-` prefix to avoid conflicts.

### Reranking System (NEW)

**When to use reranking** (`RERANKER_ENABLED=true`):
- Documentation technique avec terminologie similaire (médical, juridique, scientifique)
- Beaucoup de concepts qui se chevauchent sémantiquement
- Base documentaire >1000 documents
- Besoin de précision maximale sur les résultats

**Performance impact**:
- Latence additionnelle: +100-300ms par requête
- Ressources: ~4GB RAM pour le service reranker
- Avantage: Meilleure pertinence des résultats (jusqu'à 20-30% d'amélioration)

**How it works**:
1. Vector search récupère top-20 candidats (au lieu de 5)
2. CrossEncoder (BGE-reranker-v2-m3) analyse finement chaque paire (question, document)
3. Top-5 documents vraiment pertinents sont retournés au LLM
4. Fallback gracieux si le service reranker échoue (utilise top-5 du vector search)

**Configuration**:
```bash
RERANKER_ENABLED=true  # Activer le reranking
RERANKER_TOP_K=20      # Augmenter si base très large (max 50)
RERANKER_RETURN_K=5    # Nombre final de chunks pour le LLM
```

### VLM (Vision Language Model) System - Image Extraction (NEW)

**When to use VLM** (`VLM_ENABLED=true`):
- PDFs containing diagrams, charts, tables, or graphical content
- Technical documentation with visual elements (especially software screenshots)
- Medical/scientific documents with images
- Need to extract text from images (OCR)
- Want visual context in RAG responses

**🆕 Dual VLM Engine System** (2025-01-25):

RAGFab now supports **two VLM engines**, selectable per document at upload time:

1. **PaddleOCR-VL** (Local, Fast, Excellent OCR)
   - **Use case**: Technical documents with screenshots, software documentation, structured text
   - **Strengths**: Fast (~1-3s/image), excellent OCR accuracy (109 languages), local processing (no API)
   - **Processing**: Multilingual OCR + layout detection + basic structural description
   - **Performance**: ~3x faster than InternVL, optimal for documents with text-heavy images

2. **InternVL** (API, Rich Descriptions)
   - **Use case**: Documents needing semantic understanding, diagrams requiring explanation
   - **Strengths**: Rich semantic descriptions, contextual understanding, visual reasoning
   - **Processing**: Vision-language model generates narrative descriptions + OCR
   - **Performance**: ~10-15s/image, best for complex visual content requiring interpretation

**Selection Strategy**:
- **Software/Technical docs** → PaddleOCR-VL (screenshots, UI elements, code snippets)
- **Medical/Scientific diagrams** → InternVL (anatomical charts, process diagrams)
- **Mixed content** → Try both engines on sample document and compare results

**🆕 RapidOCR Integration for Docling** (2025-01-25):

Docling now uses **RapidOCR** instead of EasyOCR for PDF text extraction:
- **Performance**: ~2x faster than EasyOCR
- **Accuracy**: Improved recognition for complex layouts
- **Based on**: PaddlePaddle OCR engine (same foundation as PaddleOCR-VL)
- **Fallback**: Gracefully falls back to EasyOCR if RapidOCR not installed

**Architecture**:
- **Dual VLM engines**: PaddleOCR-VL (local) OR InternVL (API), selectable per upload
- **Docling OCR**: RapidOCR for PDF text extraction (automatic, not selectable)
- **Image detection**: Docling automatically detects images during PDF parsing
- **Dual storage**: Filesystem (original) + base64 (inline display)
- **Database linking**: Images linked to chunks via `page_number` matching
- **Per-job selection**: VLM engine stored in `ingestion_jobs.vlm_engine` column

**Configuration**:
```bash
# -------------------------------------------
# OCR Engine Configuration (NEW)
# -------------------------------------------
# Docling OCR Engine: Moteur OCR utilisé par Docling pour extraction de texte PDF
# RapidOCR: Basé sur PaddlePaddle, plus rapide et précis qu'EasyOCR
# Automatique: Docling utilisera RapidOCR si installé, sinon EasyOCR
DOCLING_OCR_ENGINE=rapidocr

# -------------------------------------------
# VLM (Vision Language Model) Configuration
# -------------------------------------------
# Enable/disable VLM image extraction
VLM_ENABLED=false  # Set to true to activate

# Moteur VLM par défaut pour extraction d'images
# Options:
#   - paddleocr-vl: Local, rapide, excellent OCR (RECOMMANDÉ pour documents techniques)
#   - internvl: API distant, descriptions riches (meilleur pour contexte narratif)
#   - none: Pas d'extraction d'images
# Note: Peut être surchargé par utilisateur à l'upload via interface admin
IMAGE_PROCESSOR_ENGINE=paddleocr-vl

# -------- PaddleOCR-VL Configuration (local) --------
# Activer accélération GPU pour PaddleOCR (true/false)
# true = Utilise GPU (nécessite paddlepaddle-gpu)
# false = Utilise CPU uniquement (plus lent mais pas de dépendances GPU)
PADDLEOCR_USE_GPU=false

# Langues OCR supportées (séparées par virgule)
# Exemples: fr (français), en (anglais), ch (chinois), de (allemand), es (espagnol)
# PaddleOCR supporte 109 langues au total
PADDLEOCR_LANG=fr,en

# Afficher les logs PaddleOCR (true/false)
# Recommandé: false (réduit le bruit dans les logs)
PADDLEOCR_SHOW_LOG=false

# -------- InternVL Configuration (API distant) --------
# VLM API configuration (FastAPI remote service)
VLM_API_URL=https://apivlm.mynumih.fr  # API Vision Générique with InternVL3_5-8B
VLM_API_KEY=  # Optional, leave empty if not required
VLM_MODEL_NAME=OpenGVLab/InternVL3_5-8B  # Informational, configured server-side
VLM_TIMEOUT=60.0

# Prompt pour l'analyse des images (utilisé par InternVL uniquement)
VLM_PROMPT=Décris cette image en détail en français. Extrais tout le texte visible.

# -------------------------------------------
# Image Processing Configuration
# -------------------------------------------
IMAGE_STORAGE_PATH=/app/uploads/images
IMAGE_MAX_SIZE_MB=10
IMAGE_QUALITY=85
IMAGE_OUTPUT_FORMAT=png

# Image filtering (NEW - prevents small icons/logos from polluting vector DB)
IMAGE_MIN_WIDTH=200           # Minimum width in pixels
IMAGE_MIN_HEIGHT=200          # Minimum height in pixels
IMAGE_MIN_AREA=40000          # Minimum area in px² (200x200)
IMAGE_ASPECT_RATIO_MAX=10.0   # Max aspect ratio (avoids banners/borders)
```

**Dual VLM Providers**:

**PaddleOCR-VL (Local)**:
- **Type**: Local processing, no external API needed
- **Model**: PaddleOCR 0.9B parameter vision-language model
- **Languages**: 109 languages (French, English, Chinese, German, Spanish, etc.)
- **Features**: OCR + layout detection + basic structural description
- **Performance**: ~1-3s per image (CPU), ~0.5-1s per image (GPU)
- **Requirements**: `paddleocr>=2.7.0` + `paddlepaddle>=2.6.0` (or paddlepaddle-gpu)
- **Best for**: Software screenshots, technical documentation, text-heavy images

**InternVL (Remote API)**:
- **API**: https://apivlm.mynumih.fr (API Vision Générique - vLLM Proxy)
- **Model**: OpenGVLab/InternVL3_5-8B (optimized for document analysis)
- **Endpoints**:
  - `/extract-and-describe` - Combined description + OCR (used by default)
  - `/describe-image` - Description only
  - `/extract-text` - OCR only
- **Performance**: ~10-15s per image
- **No API key required**
- **Best for**: Complex diagrams, medical charts, semantic understanding needed

**How it works** (Dual-Engine Selection):
1. **Upload**: User uploads document via admin interface, selects VLM engine (`paddleocr-vl`, `internvl`, or `none`)
2. **Job Creation**: API stores selection in `ingestion_jobs.vlm_engine` column
3. **Worker Processing**:
   - Worker reads `vlm_engine` from job record
   - Creates appropriate ImageProcessor: `create_image_processor(engine=vlm_engine)`
   - Passes processor to `_read_document()`
4. **PDF Parsing**: Docling parses PDF with RapidOCR for text extraction
5. **Image Detection**: Docling automatically detects images in PDF pages
6. **Image Analysis**: Selected VLM engine (PaddleOCR-VL or InternVL) analyzes each image
7. **Extraction**: Images saved to `/app/uploads/images/{job_id}/{image_id}.png`
8. **Storage**: Metadata in `document_images` table + base64 encoding
9. **Linking**: Images linked to chunks via `page_number` match
10. **Display**: RAG search includes images → Frontend shows inline thumbnails

**Implementation Files**:
- `database/migrations/07_add_vlm_engine.sql` - Database migration for vlm_engine column
- `rag-app/requirements.txt` - Added rapidocr-onnxruntime, paddleocr, paddlepaddle
- `rag-app/ingestion/paddleocr_client.py` - PaddleOCR-VL wrapper client
- `rag-app/ingestion/image_processor.py` - Factory pattern with engine selection
- `rag-app/ingestion/ingest.py` - Docling configured with RapidOCR, image_processor parameter
- `web-api/app/routes/admin.py` - Upload endpoint accepts `vlm_engine` parameter
- `ingestion-worker/worker.py` - Reads `vlm_engine` from job, creates appropriate processor

**Database schema**:
```sql
CREATE TABLE document_images (
    id UUID PRIMARY KEY,
    document_id UUID REFERENCES documents(id),
    chunk_id UUID REFERENCES chunks(id),  -- Linked via page_number
    page_number INTEGER,
    position JSONB,  -- {x, y, width, height}
    image_path VARCHAR(500),
    image_base64 TEXT,  -- For inline display
    description TEXT,  -- VLM-generated description
    ocr_text TEXT,  -- Extracted text from image
    confidence_score FLOAT,
    created_at TIMESTAMP
);
```

**Frontend integration**:
- `ImageViewer.tsx`: Component for displaying image thumbnails and full-size modal
- Thumbnails shown inline with sources in chat responses
- Click to enlarge with zoom controls
- Download individual images
- Display VLM description + OCR text

**Performance impact**:
- Additional time during ingestion: +6-60s per image (model dependent)
- Storage: ~50-500KB per image (base64 + metadata)
- No runtime impact: Images pre-processed during ingestion
- Network: One API call per image to VLM service

### Triple-Engine Selection System (OCR + VLM + Chunker) - NEW (2025-01-26)

**Architecture**: Per-job configuration allowing users to independently select:
1. **OCR Engine** (for document text extraction via Docling)
2. **VLM Engine** (for image analysis and description)
3. **Chunker Type** (for document segmentation strategy)

**Why Separate Configurations**:
- OCR handles text extraction from PDFs (page-level processing)
- VLM handles image analysis within documents (image-level processing)
- Chunker handles document segmentation (structural vs hierarchical)
- Different documents need different strategies (structured docs vs long transcripts)
- Users can optimize per document type (technical manuals vs interview transcripts)

**Migrations**:
- `database/migrations/08_add_ocr_engine.sql` - OCR engine selection
- `database/migrations/09_add_chunker_type.sql` - Chunker type selection + smart search function

Adds `ocr_engine` column to `ingestion_jobs` table:
```sql
ALTER TABLE ingestion_jobs
ADD COLUMN IF NOT EXISTS ocr_engine VARCHAR(50) DEFAULT 'rapidocr';
```

**OCR Engine Options**:

1. **RapidOCR** (Default - Recommended)
   - Speed: ~2x faster than EasyOCR
   - Backend: PaddlePaddle ONNX Runtime
   - Languages: Multilingual (100+ languages)
   - Best for: General documents, fast processing required
   - Dependency: `rapidocr-onnxruntime>=1.3.0`

2. **EasyOCR** (Docling Default)
   - Speed: Standard (baseline)
   - Backend: PyTorch-based
   - Languages: Multilingual (80+ languages)
   - Best for: Robust fallback, proven accuracy
   - Dependency: Included in Docling by default

3. **Tesseract** (High Quality)
   - Speed: Slower than RapidOCR
   - Backend: Tesseract-OCR engine
   - Languages: 100+ with trained data
   - Best for: High-quality scans, archival documents
   - Dependency: `pytesseract>=0.3.10` + system package `tesseract-ocr`

**Chunker Type Options**:

1. **Hybrid** (Default - Recommended for structured documents)
   - Strategy: DoclingHybridChunker - respects document structure
   - Size: Variable (respects semantic boundaries)
   - Preserves: Sections, headings, tables, lists
   - Best for: Technical docs, manuals, structured reports
   - Never cuts: Paragraphs, tables, or lists arbitrarily
   - Metadata: Section hierarchy, heading context

2. **Parent-Child** (For long unstructured text)
   - Strategy: ParentChildChunker - hierarchical architecture
   - Parent size: 2000 tokens (rich context for LLM)
   - Child size: ~600 tokens (precise search matching)
   - Best for: Transcriptions, interviews, long narratives
   - Search: Operates on children, returns parents
   - Trade-off: More storage (3-5x chunks), may cut sections

**Chunker Type Decision Matrix**:

| Document Type | Recommended Chunker | Reason |
|---------------|---------------------|--------|
| Technical manual | Hybrid | Preserves table/section structure |
| API documentation | Hybrid | Respects code blocks and headings |
| Medical protocol | Hybrid | Maintains procedural steps |
| Interview transcript | Parent-Child | Long continuous narrative |
| Book chapter | Parent-Child | Unstructured long text |
| Meeting notes | Hybrid | Structured with bullet points |

**Implementation Pattern**:

**Backend API** (`web-api/app/routes/admin.py`):
```python
@router.post("/documents/upload")
async def upload_document(
    file: UploadFile = File(...),
    ocr_engine: str = Form("rapidocr"),  # OCR engine selection
    vlm_engine: str = Form("paddleocr-vl"),  # VLM engine selection
    chunker_type: str = Form("hybrid"),  # NEW: Chunker type selection
    current_user: dict = Depends(get_current_admin_user)
):
    # Validation
    allowed_ocr_engines = {"rapidocr", "easyocr", "tesseract"}
    allowed_vlm_engines = {"paddleocr-vl", "internvl", "none"}
    allowed_chunker_types = {"hybrid", "parent_child"}  # NEW

    # Store all 3 in database
    job = await conn.fetchrow("""
        INSERT INTO ingestion_jobs
        (id, filename, status, ocr_engine, vlm_engine, chunker_type)
        VALUES ($1, $2, 'pending', $3, $4, $5)
    """, job_id, filename, ocr_engine, vlm_engine, chunker_type)
```

**OCR Engine Factory** (`rag-app/ingestion/ingest.py`):
```python
async def _read_document(
    self,
    file_path: str,
    ocr_engine: Optional[str] = None  # Job-specific override
) -> tuple[str, Optional[Any], List[dict]]:
    selected_ocr = ocr_engine or os.getenv("DOCLING_OCR_ENGINE", "rapidocr")

    # OCR Engine Factory Pattern
    pipeline_options = PdfPipelineOptions()

    if selected_ocr == "rapidocr":
        from docling.backend.rapidocr_backend import RapidOcrOptions
        pipeline_options.ocr_options = RapidOcrOptions()
        converter = DocumentConverter(pipeline_options=pipeline_options)

    elif selected_ocr == "tesseract":
        from docling.backend.tesseract_ocr_backend import TesseractOcrOptions
        pipeline_options.ocr_options = TesseractOcrOptions()
        converter = DocumentConverter(pipeline_options=pipeline_options)

    elif selected_ocr == "easyocr":
        converter = DocumentConverter()  # Default Docling engine
```

**Worker Integration** (`ingestion-worker/worker.py`):
```python
async def process_job(self, job: dict):
    ocr_engine = job.get("ocr_engine", "rapidocr")
    vlm_engine = job.get("vlm_engine", "paddleocr-vl")
    chunker_type = job.get("chunker_type", "hybrid")  # NEW

    # Pass OCR and VLM engines to document reading
    document_content, docling_doc, images = await self.pipeline._read_document(
        file_path=str(file_path),
        image_processor=create_image_processor(engine=vlm_engine),
        ocr_engine=ocr_engine
    )

    # Create chunker based on job's chunker_type (NEW)
    from ingestion.chunker import create_chunker
    job_chunker = create_chunker(
        use_parent_child=(chunker_type == "parent_child")
    )

    # Chunk with job-specific chunker
    chunks = await job_chunker.chunk_document(
        content=document_content,
        title=document_title,
        source=document_source,
        metadata=document_metadata,
        docling_doc=docling_doc
    )
```

**Frontend UI** (`frontend/src/pages/AdminPage.tsx`):

Replaced radio buttons with 3 compact dropdown selects:
```typescript
// State management
const [selectedOcrEngine, setSelectedOcrEngine] = useState<OcrEngine>('rapidocr');
const [selectedVlmEngine, setSelectedVlmEngine] = useState<VlmEngine>('paddleocr-vl');
const [selectedChunkerType, setSelectedChunkerType] = useState<ChunkerType>('hybrid');  // NEW

// UI: Grid layout with 3 dropdowns
<div className="grid grid-cols-1 md:grid-cols-3 gap-4">
  {/* OCR Engine Dropdown */}
  <div>
    <label>Moteur OCR (Docling)</label>
    <select value={selectedOcrEngine}
            onChange={(e) => setSelectedOcrEngine(e.target.value as OcrEngine)}>
      <option value="rapidocr">RapidOCR (Recommandé) - Rapide, multilingue</option>
      <option value="easyocr">EasyOCR - Standard Docling</option>
      <option value="tesseract">Tesseract - Haute qualité pour scans</option>
    </select>
  </div>

  {/* VLM Engine Dropdown */}
  <div>
    <label>Moteur VLM (Images)</label>
    <select value={selectedVlmEngine}
            onChange={(e) => setSelectedVlmEngine(e.target.value as VlmEngine)}>
      <option value="paddleocr-vl">PaddleOCR-VL - Local, rapide</option>
      <option value="internvl">InternVL - API distant, descriptions riches</option>
      <option value="none">Aucun - Pas d'extraction d'images</option>
    </select>
  </div>

  {/* Chunker Type Dropdown (NEW) */}
  <div>
    <label>Stratégie de découpage</label>
    <select value={selectedChunkerType}
            onChange={(e) => setSelectedChunkerType(e.target.value as ChunkerType)}>
      <option value="hybrid">Hybrid (Recommandé) - Respecte structure</option>
      <option value="parent_child">Parent-Child - Longs textes</option>
    </select>
  </div>
</div>

// Upload callback
const onDrop = useCallback(async (acceptedFiles: File[]) => {
  for (const file of acceptedFiles) {
    await api.uploadDocument(file, selectedOcrEngine, selectedVlmEngine, selectedChunkerType);  // NEW 3rd param
  }
}, [selectedOcrEngine, selectedVlmEngine, selectedChunkerType]);
```

**Configuration** (`.env.example`):
```bash
# OCR Engine Default (can be overridden per job in UI)
DOCLING_OCR_ENGINE=rapidocr

# VLM Engine Default (can be overridden per job in UI)
IMAGE_PROCESSOR_ENGINE=paddleocr-vl
```

**Benefits of Triple-Engine System**:
- ✅ **Complete Flexibility**: Users choose optimal OCR + VLM + Chunker per document type
- ✅ **Performance**: RapidOCR + PaddleOCR-VL + Hybrid = fastest combination (~2-4s/page)
- ✅ **Quality**: Tesseract + InternVL + Parent-Child = highest quality for complex documents
- ✅ **Cost Optimization**: PaddleOCR-VL (local) vs InternVL (API) based on budget
- ✅ **Backward Compatible**: Defaults maintain existing behavior
- ✅ **Per-Job Config**: No global .env changes needed for one-off documents
- ✅ **Smart Search**: `match_chunks_smart()` automatically handles both hybrid and parent-child chunks

**Use Case Examples**:

1. **Modern digital PDF (text-heavy)**: RapidOCR + None + Hybrid
2. **Technical manual (diagrams)**: RapidOCR + PaddleOCR-VL + Hybrid (preserves tables/structure)
3. **Scanned archive (high quality)**: Tesseract + InternVL + Hybrid
4. **Medical protocol (precision)**: EasyOCR + InternVL + Hybrid (structured steps)
5. **Interview transcript**: RapidOCR + None + Parent-Child (long narrative)
6. **Book chapter**: EasyOCR + None + Parent-Child (unstructured continuous text)

**Testing & Troubleshooting**:

**Step 1: Apply Database Migrations**
```bash
# Migrations are now automatic with db-migrations service!
# On container rebuild, new migrations are detected and applied automatically

# Verify all 3 columns exist
docker-compose exec postgres psql -U raguser -d ragdb \
  -c "\d ingestion_jobs" | grep -E "(ocr_engine|vlm_engine|chunker_type)"

# Check migration 09 applied successfully
docker-compose exec postgres psql -U raguser -d ragdb \
  -c "SELECT * FROM schema_migrations WHERE filename = '09_add_chunker_type.sql';"

# Verify match_chunks_smart() function exists
docker-compose exec postgres psql -U raguser -d ragdb \
  -c "\df match_chunks_smart"
```

**Step 2: Install OCR Engine Dependencies**
```bash
# Install all OCR engines
cd rag-app
pip install rapidocr-onnxruntime>=1.3.0  # RapidOCR
pip install pytesseract>=0.3.10  # Tesseract (+ system package)
# EasyOCR included in Docling by default

# For Tesseract: Install system package
# macOS: brew install tesseract
# Ubuntu: apt-get install tesseract-ocr
# Alpine (Docker): apk add tesseract-ocr

# For PaddleOCR-VL (if using VLM):
pip install paddleocr>=2.7.0 paddlepaddle>=2.6.0
```

**Step 3: Test OCR Engines**

**Test RapidOCR**:
```bash
python -c "from docling.backend.rapidocr_backend import RapidOcrOptions; print('RapidOCR OK')"

# Monitor worker logs for PaddleOCR processing
docker-compose logs -f ingestion-worker | grep "PaddleOCR"

# Look for messages like:
# "✅ Image processor created with engine: paddleocr-vl"
# "PaddleOCR extracted X text lines (avg confidence: Y)"
```

**Test InternVL (API)**:
```bash
# Verify InternVL API connectivity
curl -X POST https://apivlm.mynumih.fr/extract-and-describe \
  -F "image=@test_image.png" \
  -F "temperature=0.1"

# Check API health
curl https://apivlm.mynumih.fr/health

# Monitor worker logs for InternVL processing
docker-compose logs -f ingestion-worker | grep "InternVL"
```

**Step 4: Verify Triple-Engine Selection**
```bash
# Check which engines were used for each job
docker-compose exec postgres psql -U raguser -d ragdb -c \
  "SELECT id, filename, ocr_engine, vlm_engine, chunker_type, status FROM ingestion_jobs ORDER BY created_at DESC LIMIT 10;"

# Example output:
#   id    | filename          | ocr_engine | vlm_engine    | chunker_type | status
# --------+-------------------+------------+---------------+--------------+-----------
#  uuid1  | technical.pdf     | rapidocr   | paddleocr-vl  | hybrid       | completed
#  uuid2  | interview.txt     | rapidocr   | none          | parent_child | completed
#  uuid3  | scanned_doc.pdf   | tesseract  | internvl      | hybrid       | completed
```

**Step 5: Compare Results**
```bash
# Upload same document with both engines
# 1. Upload via admin with vlm_engine=paddleocr-vl
# 2. Upload again with vlm_engine=internvl
# 3. Compare image descriptions in database

docker-compose exec postgres psql -U raguser -d ragdb -c \
  "SELECT
     i.vlm_engine,
     di.description,
     di.ocr_text,
     di.confidence_score
   FROM document_images di
   JOIN documents d ON di.document_id = d.id
   JOIN ingestion_jobs i ON d.title = i.filename
   WHERE i.filename = 'test_document.pdf'
   ORDER BY i.created_at DESC;"
```

**Common Issues**:

**Issue 1: PaddleOCR not found**
```bash
# Error: "PaddleOCR not available"
# Solution: Install dependencies
pip install paddleocr paddlepaddle rapidocr-onnxruntime
```

**Issue 2: RapidOCR fallback to EasyOCR**
```bash
# Warning: "RapidOCR not available, using Docling default OCR (EasyOCR)"
# Solution: Install RapidOCR
pip install rapidocr-onnxruntime>=1.3.0
```

**Issue 3: InternVL API timeout**
```bash
# Error: "VLM analysis error: timeout"
# Solution: Increase VLM_TIMEOUT in .env
VLM_TIMEOUT=120.0  # Increase from default 60s
```

**Issue 4: Worker not reading vlm_engine**
```bash
# Symptom: All jobs use same engine despite selection
# Check: Worker logs should show "Processing job X with VLM engine: Y"
docker-compose logs -f ingestion-worker | grep "VLM engine"

# If not visible, restart worker
docker-compose restart ingestion-worker
```

**General Troubleshooting**:
```bash
# Check if images are being extracted
docker-compose logs -f ingestion-worker | grep "image"

# Check image storage
ls -lh /app/uploads/images/

# Query images in database
docker-compose exec postgres psql -U raguser -d ragdb \
  -c "SELECT document_id, COUNT(*) FROM document_images GROUP BY document_id;"

# Check vlm_engine usage statistics
docker-compose exec postgres psql -U raguser -d ragdb -c \
  "SELECT vlm_engine, COUNT(*) as job_count,
          COUNT(CASE WHEN status='completed' THEN 1 END) as completed
   FROM ingestion_jobs
   GROUP BY vlm_engine;"
```

**API endpoints** (new):
- `GET /api/chunks/{chunk_id}/images` - Get images for a chunk
- `GET /api/documents/{document_id}/images` - Get all images from document
- `GET /api/images/{image_id}` - Get specific image metadata

#### Image Filtering System (NEW)

**Problem**: Small icons, logos, and decorative elements pollute the vector database without adding meaningful context.

**Solution**: Multi-criteria filtering applied during image extraction (`rag-app/ingestion/image_processor.py`):

**Filtering Criteria**:
1. **Minimum Dimensions**: `IMAGE_MIN_WIDTH` x `IMAGE_MIN_HEIGHT` (default: 200x200px)
2. **Minimum Area**: `IMAGE_MIN_AREA` (default: 40000px² = 200x200)
3. **Aspect Ratio**: `IMAGE_ASPECT_RATIO_MAX` (default: 10.0) - Rejects elongated banners/borders

**Configuration Examples**:
```bash
# Medical/Scientific documents (diagrams, charts)
IMAGE_MIN_WIDTH=200
IMAGE_MIN_HEIGHT=200
IMAGE_MIN_AREA=40000

# Technical documentation (screenshots, diagrams)
IMAGE_MIN_WIDTH=150
IMAGE_MIN_HEIGHT=150
IMAGE_MIN_AREA=22500

# General documents (illustrations only)
IMAGE_MIN_WIDTH=250
IMAGE_MIN_HEIGHT=250
IMAGE_MIN_AREA=62500
```

**Implementation Details**:
- Filter applied in `ImageProcessor._process_image()` BEFORE VLM analysis
- Rejected images logged with dimensions for debugging (`LOG_LEVEL=DEBUG`)
- Filter bypass: Set all thresholds to 0 to accept all images
- Performance: Filtering happens before base64 encoding → Saves VLM API calls

**Benefits**:
- ✅ 60-80% reduction in irrelevant images
- ✅ Faster vector search (fewer synthetic chunks)
- ✅ Lower VLM API costs (fewer images analyzed)
- ✅ Better RAG relevance (only meaningful images indexed)

**Re-ingestion**: To apply new filters to existing documents, delete and re-upload via admin interface.

## Common Pitfalls

### PydanticAI Tools
- ❌ Don't use `run_stream()` with tools - it detects but doesn't execute them automatically
- ✅ Use `run()` for automatic tool execution workflow
- ❌ Don't pass history when you need tool calling - model will skip tool and answer from context
- ✅ Pass empty `message_history=[]` to force tool calls

### Test Failures
- Pydantic v2 strictly validates types and **rejects MagicMock** objects
- Don't mock tokenizers in tests - use real tokenizers (~200ms load time acceptable)
- `DocumentChunk` requires: content, index, start_char, end_char, metadata, token_count (NOT id, source, title)

### Glob Patterns
- Pattern `**/*.pdf` does NOT find files at root directory
- Always use two globs: one for root, one for subdirectories

### Container Naming
- Coolify environments may have multiple projects
- Always use unique prefixes (e.g., `ragfab-postgres` instead of just `postgres`)
- Update DATABASE_URL and service references when renaming containers

## Frontend Architecture

**Stack**: React 18 + TypeScript + Vite + TailwindCSS
**Key libraries**: react-router-dom, axios, react-markdown, lucide-react

**State management**: No global state library - uses React hooks and local component state

**API integration**: Axios client in `frontend/src/lib/api.ts`

**Document viewing**: Supports source document viewing with chunk highlighting

**Markdown rendering**: Uses react-markdown with syntax highlighting (react-syntax-highlighter)

## Testing Philosophy

- Unit tests focus on business logic, not implementation details
- Integration tests require actual services (marked with `@pytest.mark.embeddings`)
- Coverage threshold: 20% (realistic baseline, not aspirational 70%)
- Tests should use real dependencies when possible (e.g., real tokenizers, not mocks)

## Git Workflow

All commits should include:
- Clear description of problem solved
- Technical solution explanation
- List of changes with file references
- Standardized footer:

```
🤖 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
```

## Database Migrations System

RAGFab includes an **automatic migration system** that detects and applies new SQL migrations at every container rebuild, eliminating manual migration steps.

### Architecture

**Components**:
1. **Migration tracking table**: `schema_migrations` logs all applied migrations
2. **Application script**: `database/apply_migrations.sh` detects and applies new migrations
3. **Rollback script**: `database/rollback_last_migration.sh` reverts last migration if needed
4. **Docker service**: `db-migrations` runs automatically at startup
5. **Migration files**: Stored in `/database/migrations/` with numeric prefixes

**Migration tracking schema**:
```sql
CREATE TABLE schema_migrations (
    id UUID PRIMARY KEY,
    filename VARCHAR(255) UNIQUE NOT NULL,
    applied_at TIMESTAMP NOT NULL,
    success BOOLEAN NOT NULL,
    error_message TEXT,
    execution_time_ms INTEGER,
    checksum VARCHAR(64)
);
```

### Workflow

**Standard workflow** (fully automatic):
```bash
# 1. Pull latest code (includes new migrations)
git pull origin main

# 2. Rebuild and restart (migrations apply automatically)
docker-compose up -d --build

# 3. Verify migrations (optional)
docker-compose exec postgres psql -U raguser -d ragdb \
  -c "SELECT filename, applied_at, success FROM schema_migrations ORDER BY applied_at DESC;"
```

**That's it!** No manual migration commands needed. The `db-migrations` service:
- ✅ Detects new `.sql` files in `/database/migrations/`
- ✅ Checks which ones are already applied (via `schema_migrations` table)
- ✅ Applies new migrations in alphabetical order
- ✅ Logs each migration with execution time and checksum
- ✅ Stops container startup if any migration fails (safety)

### Migration File Naming Convention

Use numeric prefixes to control execution order:

```
/database/migrations/
  ├── 00_init_migrations.sql          # System (creates tracking table)
  ├── 05_enriched_metadata.sql        # Feature: Adjacent chunks
  ├── 06_parent_child_chunks.sql      # Feature: Hierarchical chunking
  ├── 07_add_user_preferences.sql     # Your new migration
  └── 07_add_user_preferences_DOWN.sql # Optional rollback file
```

**Naming rules**:
- **Prefix**: Two digits (`01`, `02`, ..., `99`)
- **Description**: Snake_case descriptive name
- **Extension**: `.sql`
- **Rollback** (optional): Same name + `_DOWN.sql` suffix

Examples:
- ✅ `08_add_document_tags.sql`
- ✅ `09_create_audit_log.sql`
- ✅ `10_alter_chunks_add_metadata.sql`
- ❌ `add_tags.sql` (no numeric prefix)
- ❌ `8_tags.sql` (single digit, unclear name)

### Creating a New Migration

**Step 1: Create migration file**
```bash
# Create in /database/migrations/ with next available number
touch database/migrations/07_add_user_preferences.sql
```

**Step 2: Write SQL with idempotent patterns**
```sql
-- Migration 07: Add user preferences
-- Description: Adds preferences column to users table
-- Date: 2025-01-24

-- Use IF NOT EXISTS for safety (idempotent)
ALTER TABLE users ADD COLUMN IF NOT EXISTS preferences JSONB DEFAULT '{}';

CREATE INDEX IF NOT EXISTS idx_users_preferences
    ON users USING gin(preferences);

COMMENT ON COLUMN users.preferences IS 'User preferences in JSON format';
```

**Step 3: (Optional) Create rollback file**
```bash
touch database/migrations/07_add_user_preferences_DOWN.sql
```

```sql
-- Rollback for 07_add_user_preferences.sql
ALTER TABLE users DROP COLUMN IF EXISTS preferences;
DROP INDEX IF EXISTS idx_users_preferences;
```

**Step 4: Commit and deploy**
```bash
git add database/migrations/07_add_user_preferences.sql
git commit -m "Add user preferences column"
git push
```

**Step 5: Rebuild** (migration applies automatically)
```bash
docker-compose up -d --build
```

### Migration Execution Details

**How `db-migrations` service works**:

1. **Startup**: Service starts after `postgres` becomes healthy (depends_on with condition)
2. **Wait for postgres**: Retry loop with `pg_isready` (max 30 attempts)
3. **Initialize tracking**: Creates `schema_migrations` table if missing (runs `00_init_migrations.sql`)
4. **List migrations**: Finds all `.sql` files in `/database/migrations/` (sorted alphabetically)
5. **Check status**: For each file, queries `schema_migrations` to see if already applied
6. **Apply new**: Executes new migrations with `psql`, records result with execution time
7. **Handle failure**: If any migration fails, exits with code 1 (prevents container startup)
8. **Shutdown**: Service stops after completion (restart: "no")

**Logs visibility**:
```bash
# View migration logs
docker-compose logs db-migrations

# View real-time during startup
docker-compose up db-migrations

# Check migration history in database
docker-compose exec postgres psql -U raguser -d ragdb \
  -c "SELECT * FROM schema_migrations ORDER BY applied_at DESC;"
```

### Rollback System

**Automatic rollback** (if `_DOWN.sql` file exists):
```bash
# Rollback last migration interactively
docker-compose exec postgres bash /database/rollback_last_migration.sh

# Example output:
# Last applied migration: 07_add_user_preferences.sql
# Found automatic rollback file: 07_add_user_preferences_DOWN.sql
# Are you sure you want to continue? (yes/no): yes
# Executing rollback SQL...
# Migration record removed from schema_migrations
# Rollback completed successfully!
```

**Manual rollback** (no `_DOWN.sql` file):
```bash
# Script provides instructions:
# 1. Review migration file
# 2. Write reverse SQL operations
# 3. Execute manually with psql
# 4. Remove record from schema_migrations
```

### Configuration

**Environment variable**: `.env`
```bash
# Enable/disable automatic migrations (default: true)
AUTO_APPLY_MIGRATIONS=true
```

**Disable migrations temporarily**:
```bash
AUTO_APPLY_MIGRATIONS=false docker-compose up -d
```

### Troubleshooting

**Problem**: Migration fails during startup
```bash
# View error details
docker-compose logs db-migrations

# Common errors:
# - Syntax error in SQL → Fix migration file
# - Duplicate column → Use IF NOT EXISTS
# - Missing dependency → Check migration order (prefix numbers)
```

**Solution**: Fix migration file and rebuild
```bash
# Edit migration file to fix error
vim database/migrations/07_add_user_preferences.sql

# Rebuild (will retry failed migration)
docker-compose up -d --build db-migrations
```

**Problem**: Migration applied but needs rollback
```bash
# Use rollback script
docker-compose exec postgres bash /database/rollback_last_migration.sh

# Or manual rollback
docker-compose exec postgres psql -U raguser -d ragdb
# Write reverse SQL...
# DELETE FROM schema_migrations WHERE filename = '07_add_user_preferences.sql';
```

**Problem**: Container doesn't start after migration failure
```bash
# Migrations are strict by design (safety)
# Fix the migration file and restart
docker-compose up -d --build

# To bypass temporarily (NOT RECOMMENDED):
AUTO_APPLY_MIGRATIONS=false docker-compose up -d
```

### Migration Best Practices

1. **Idempotent operations**: Always use `IF NOT EXISTS` / `IF EXISTS`
   ```sql
   ALTER TABLE users ADD COLUMN IF NOT EXISTS email VARCHAR(255);
   CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);
   ```

2. **Descriptive names**: Use clear, action-oriented filenames
   - ✅ `08_add_document_categories.sql`
   - ❌ `08_changes.sql`

3. **Single responsibility**: One logical change per migration
   - ✅ `09_create_audit_log.sql` (creates audit table + indexes)
   - ❌ `09_various_changes.sql` (user table + audit + documents)

4. **Test migrations**: Test in development before production
   ```bash
   # Test locally first
   docker-compose up -d --build
   # Verify results
   docker-compose exec postgres psql -U raguser -d ragdb -c "\d users"
   ```

5. **Create rollback files**: For complex migrations, always create `_DOWN.sql`

6. **Document migrations**: Add comments explaining purpose and impact
   ```sql
   -- Migration 08: Add document categories
   -- Purpose: Enable document categorization for better organization
   -- Impact: Adds nullable column, no data migration needed
   -- Related: Issue #123, Feature request from 2025-01-20
   ```

7. **Sequential numbering**: Leave gaps for hotfixes
   - Use: `05`, `10`, `15`, `20` (allows inserting `06`, `11`, etc.)

### Existing Migrations

**Already in system**:
- `00_init_migrations.sql` - Migration tracking table (system)
- `05_enriched_metadata.sql` - Adjacent chunks metadata (RAG optimization Phase 2.1)
- `06_parent_child_chunks.sql` - Hierarchical chunking (RAG optimization Phase 2.3)

**Status check**:
```bash
# List all applied migrations
docker-compose exec postgres psql -U raguser -d ragdb \
  -c "SELECT filename, applied_at, success, execution_time_ms FROM schema_migrations ORDER BY applied_at;"
```

### Integration with CI/CD

The migration system works seamlessly in CI/CD pipelines:

```yaml
# Example GitLab CI
deploy_production:
  script:
    - docker-compose pull
    - docker-compose up -d --build  # Migrations apply automatically
    - docker-compose logs db-migrations  # Log migration results
```

No manual intervention needed - migrations are part of the deployment process.

## Common Development Scenarios

### Adding a New LLM Provider

1. Create provider file in `rag-app/utils/` or `web-api/app/utils/`
2. Implement `Model` or `AgentModel` interface from PydanticAI
3. Add factory function (e.g., `get_provider_model()`)
4. Update `execute_rag_agent()` in `web-api/app/main.py` with new provider logic
5. Add environment variables to `.env.example` and `docker-compose.yml`
6. Update documentation

### Modifying Chunking Strategy

1. Edit `rag-app/ingestion/chunker.py`
2. Update `ChunkingConfig` dataclass if adding parameters
3. Modify environment variables in `.env`
4. Re-ingest all documents to apply new strategy
5. Update tests in `rag-app/tests/unit/test_chunker.py`

### Adding New Database Tables

1. Add schema to `database/02_web_schema.sql` (or create new file `03_*.sql`)
2. Ensure files are mounted in `docker-compose.yml` under postgres volumes
3. Files execute in alphabetical order - use numeric prefixes
4. Recreate database or run migration manually for existing deployments

### Multi-User System and Database Migration

**Schema updates** (as of multi-user implementation):
- `conversations.user_id` is now **NOT NULL** (required for all new conversations)
- `conversations.reranking_enabled` column added (default: false)
- `conversation_stats` view updated to include `user_id`, `reranking_enabled`, and `archived` fields

**Migration process for existing deployments**:
```bash
# 1. Back up your database first
docker-compose exec postgres pg_dump -U raguser ragdb > backup.sql

# 2. Run migration script (handles NULL user_id and adds reranking_enabled)
docker-compose exec postgres psql -U raguser -d ragdb -f /docker-entrypoint-initdb.d/03_migration_user_sessions.sql

# Migration performs:
# - Adds reranking_enabled column if missing
# - Assigns orphaned conversations (user_id = NULL) to first admin
# - Sets user_id as NOT NULL
# - Updates conversation_stats view

# 3. Verify migration success
docker-compose exec postgres psql -U raguser -d ragdb -c "SELECT COUNT(*) FROM conversations WHERE user_id IS NULL;"
# Should return 0

# 4. Check user assignment
docker-compose exec postgres psql -U raguser -d ragdb -c "SELECT user_id, COUNT(*) FROM conversations GROUP BY user_id;"
```

**Security considerations**:
- All conversation routes now filter by `current_user['id']`
- Frontend protected with JWT authentication on all routes (including `/`)
- Admin panel accessible only to users with `is_admin = true`
- Each user sees only their own conversations (complete isolation)

### Debugging Sources Not Appearing

Check in order:
1. `_current_request_sources` is properly initialized at request start
2. Tool execution saves sources to global variable
3. Sources retrieved from global variable after agent execution
4. Frontend correctly displays sources array from API response
5. Check logs for "📚 Sources récupérées" messages

### User Profile Management & First-Login Password Change (NEW)

**Authentication Flow**:
RAGFab now includes a complete user profile management system with mandatory password change on first login.

**Key Features**:
1. **Mandatory Password Change**: Users created by admin must change password on first login (blocking modal)
2. **User Profile Page**: Users can update their first name, last name, and password
3. **Personalized Avatars**: Each user has a unique colored avatar with their first initial
4. **User Menu Dropdown**: Access profile, admin panel (if admin), and logout from single menu

**Database Schema** ([database/04_user_profile.sql](database/04_user_profile.sql)):
```sql
ALTER TABLE users ADD COLUMN first_name VARCHAR(100);
ALTER TABLE users ADD COLUMN last_name VARCHAR(100);
ALTER TABLE users ADD COLUMN must_change_password BOOLEAN DEFAULT false;
```

**Password Validation Rules**:
- Minimum 8 characters
- At least 1 uppercase letter
- At least 1 lowercase letter
- At least 1 digit
- Validation enforced both backend (auth.py) and frontend (ChangePasswordModal.tsx)

**Backend Endpoints** ([web-api/app/routes/auth.py](web-api/app/routes/auth.py)):
- `GET /api/auth/me/must-change-password` - Check if password change required
- `PATCH /api/auth/me/profile` - Update user profile (first_name, last_name)
- `POST /api/auth/me/change-password` - Change password (requires current password)

**Frontend Components**:
- `ChangePasswordModal.tsx` - Blocking modal for password change (non-closable if first login)
- `UserAvatar.tsx` - Colored circular avatar with user's first initial
- `UserMenu.tsx` - Dropdown menu with profile/admin/logout options
- `ProfilePage.tsx` - Full profile management page
- `avatarUtils.ts` - Color generation algorithm (8 vibrant colors, consistent per user)

**Avatar Color Algorithm**:
- Hash user UUID → Modulo 8 → Select from predefined palette
- Palette: `['#3B82F6', '#10B981', '#F59E0B', '#EF4444', '#8B5CF6', '#EC4899', '#06B6D4', '#84CC16']`
- White text on colored background for optimal contrast
- Color is consistent across all sessions for the same user

**UI Changes**:
- **Chat Interface**: Bot icon replaced with stylized `<Bot>` icon from lucide-react (cyan color)
- **User Icon**: Replaced generic "U" with personalized avatar or `<UserIcon>` icon
- **Menu Location**: Top-right corner, replaces old username + logout button
- **Profile Access**: Click avatar → "Mon profil" → Full profile page with editable fields

**Admin User Creation Flow**:
1. Admin creates user in `/admin` → Users tab
2. Admin provides: username, email, first_name, last_name, password
3. Backend sets `must_change_password=true` by default
4. User logs in → Blocking modal appears immediately
5. User cannot access chat until password is changed
6. After successful password change → `must_change_password=false` → Full access granted

**Security Considerations**:
- Current password required for password change (prevents session hijacking)
- Password strength validated on both frontend (real-time) and backend
- Modal is truly non-closable (no X button, no click outside, no ESC key)
- Session invalidation on password change (user stays logged in with same token)

**Routes**:
- `/profile` - User profile page (protected route, all authenticated users)
- Profile accessible via UserMenu dropdown or direct navigation

**Testing**:
```bash
# Apply migration
docker-compose exec postgres psql -U raguser -d ragdb -f /docker-entrypoint-initdb.d/04_user_profile.sql

# Verify new columns
docker-compose exec postgres psql -U raguser -d ragdb -c "\d users"

# Create test user (via admin interface or API)
# Login as that user → Should see password change modal immediately
```

### Performance Tuning

**Embeddings service**:
- Adjust batch size in `rag-app/ingestion/embedder.py` (currently 20)
- Increase timeout if needed (currently 90s)
- Monitor memory usage - model requires ~4-8GB RAM

**PostgreSQL**:
- Index on `embedding` column uses HNSW (configured in schema)
- Adjust `match_count` parameter for search results (default 5)
- Monitor query performance with `EXPLAIN ANALYZE`

**Vector Search**:
- Similarity threshold: 0.0 (no filtering by default)
- Uses cosine distance: `embedding <=> query_embedding`
- Results ordered by similarity (closest first)
