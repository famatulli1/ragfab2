# Session Claude Code - RAGFab avec Mistral Function Calling

**Date:** 5 octobre 2025
**Objectif:** Impl√©menter le function calling avec l'API Mistral pour le syst√®me RAG

---

## üéØ R√©sultat Final

‚úÖ **Syst√®me RAG complet avec function calling op√©rationnel**
- Provider Mistral avec tool calling automatique
- Ingestion PDF avec Docling HybridChunker
- Recherche vectorielle intelligente
- R√©ponses structur√©es avec citations des sources

---

## üìã Travaux R√©alis√©s

### 1. **Cr√©ation du Provider Mistral** (`utils/mistral_provider.py`)

**Fonctionnalit√©s impl√©ment√©es :**
- `MistralModel` : Provider de base compatible PydanticAI
- `MistralAgentModel` : Provider avec support des tools
- Conversion automatique des tools PydanticAI ‚Üí format Mistral API
- Gestion compl√®te du workflow de function calling

**Corrections techniques :**
- Import `ArgsDict` depuis `pydantic_ai.messages` (pas depuis `models`)
- S√©rialisation des arguments : `part.args.args_dict` au lieu de `part.args`
- Traitement des `ToolReturnPart` dans `ModelRequest` (pas dans `ModelResponse`)
- Messages format√©s correctement pour l'ordre attendu par Mistral API

**Fichier cr√©√© :** [utils/mistral_provider.py](rag-app/utils/mistral_provider.py)

---

### 2. **Mise √† Jour de l'Agent RAG** (`rag_agent.py`)

**Dual-provider system :**
```python
# S√©lection automatique du provider via variable d'environnement
RAG_PROVIDER = "mistral"  # ou "chocolatine"
```

**Modes disponibles :**
- **Mistral** : Function calling automatique avec `search_knowledge_base` tool
- **Chocolatine** : Injection manuelle du contexte (pour vLLM local)

**Modifications :**
- Factory function `get_rag_provider()` pour basculer entre providers
- Mode non-streaming pour Mistral (`run()` au lieu de `run_stream()`)
- Nettoyage UTF-8 des chunks pour √©viter les erreurs d'encodage

**Fichier modifi√© :** [rag_agent.py](rag-app/rag_agent.py:116-213)

---

### 3. **Configuration Environment** (`.env`)

**Nouvelles variables ajout√©es :**
```bash
# Provider RAG
RAG_PROVIDER=mistral

# API Mistral
MISTRAL_API_KEY=0SINPnbC1ebzLbEzxrRmUaPBkVo9Fhvf
MISTRAL_API_URL=https://api.mistral.ai
MISTRAL_MODEL_NAME=mistral-small-latest
MISTRAL_TIMEOUT=120.0

# Logs
LOG_LEVEL=INFO
```

**Fichier modifi√© :** [.env](\.env:40-71)

---

### 4. **Docker Compose** (`docker-compose.yml`)

**Variables d'environnement rag-app :**
```yaml
environment:
  RAG_PROVIDER: ${RAG_PROVIDER:-chocolatine}
  MISTRAL_API_KEY: ${MISTRAL_API_KEY:-}
  MISTRAL_API_URL: ${MISTRAL_API_URL:-https://api.mistral.ai}
  MISTRAL_MODEL_NAME: ${MISTRAL_MODEL_NAME:-mistral-small-latest}
  MISTRAL_TIMEOUT: ${MISTRAL_TIMEOUT:-120.0}
```

**Fichier modifi√© :** [docker-compose.yml](docker-compose.yml:64-71)

---

### 5. **Optimisations Ingestion**

#### **A. Fix d√©tection fichiers PDF** (`ingestion/ingest.py`)

**Probl√®me :** Le glob pattern `**/*.pdf` ne trouvait pas les PDFs √† la racine du dossier `documents/`

**Solution :**
```python
for pattern in patterns:
    # Chercher √† la racine du dossier
    files.extend(glob.glob(os.path.join(self.documents_folder, pattern)))
    # Chercher dans les sous-dossiers
    files.extend(glob.glob(os.path.join(self.documents_folder, "**", pattern), recursive=True))
```

**Fichier modifi√© :** [ingestion/ingest.py](rag-app/ingestion/ingest.py:251-258)

#### **B. Optimisation batch embeddings** (`ingestion/embedder.py`)

**Probl√®me :** Timeouts lors du traitement de 51 chunks en un seul batch

**Solution :**
- `batch_size: 100 ‚Üí 20` chunks par batch
- `timeout: 60s ‚Üí 90s` par requ√™te
- Le syst√®me g√©n√®re maintenant 3 batches au lieu d'un seul

**R√©sultat :** Moins de timeouts, fallback individuel uniquement en cas d'erreur r√©elle

**Fichier modifi√© :** [ingestion/embedder.py](rag-app/ingestion/embedder.py:32-35)

#### **C. Nettoyage caract√®res UTF-8** (`rag_agent.py`)

**Probl√®me :** Caract√®res surrogates UTF-8 invalides dans les chunks PDF causaient des erreurs

**Solution :**
```python
# Nettoyer le contenu des caract√®res mal encod√©s
clean_content = content.encode('utf-8', errors='replace').decode('utf-8')
clean_title = doc_title.encode('utf-8', errors='replace').decode('utf-8')
```

**Fichier modifi√© :** [rag_agent.py](rag-app/rag_agent.py:98-108)

---

### 6. **Logs Debug D√©sactiv√©s** (`utils/mistral_provider.py`)

Pour une sortie console plus propre, les logs debug ont √©t√© comment√©s :
- `logger.debug(f"Mistral API payload: ...")` ‚Üí comment√©
- `logger.debug(f"Formatting {len(messages)} messages:")` ‚Üí comment√©

**Fichier modifi√© :** [utils/mistral_provider.py](rag-app/utils/mistral_provider.py:188-297)

---

## üîß Probl√®mes R√©solus

### Erreur 1: `ImportError: cannot import name 'ModelResponsePart'`
**Cause :** Mauvais module d'import
**Solution :** Importer depuis `pydantic_ai.messages` au lieu de `pydantic_ai.models`

### Erreur 2: `'dict' object has no attribute 'args_dict'`
**Cause :** Tentative d'acc√®s √† `.args_dict` sur un dict brut
**Solution :** Cr√©er `ArgsDict(args_dict=args_data)` au lieu de passer le dict directement

### Erreur 3: `Object of type ArgsDict is not JSON serializable`
**Cause :** S√©rialisation directe de l'objet `ArgsDict`
**Solution :** Extraire le dict avec `part.args.args_dict` avant `json.dumps()`

### Erreur 4: `Expected last role User or Tool but got assistant`
**Cause :** `ToolReturnPart` trait√© dans `ModelResponse` au lieu de `ModelRequest`
**Solution :** D√©placer le traitement dans la section `ModelRequest`

### Erreur 5: Timeout batch embeddings
**Cause :** 51 chunks trop lourd pour un seul batch de 60s
**Solution :** R√©duire `batch_size` √† 20 et augmenter `timeout` √† 90s

### Erreur 6: `'utf-8' codec can't encode character '\udcc3'`
**Cause :** Caract√®res surrogates UTF-8 invalides dans le PDF
**Solution :** Nettoyage avec `encode('utf-8', errors='replace')`

### Erreur 7: PDFs non d√©tect√©s lors de l'ingestion
**Cause :** Pattern glob `**/*.pdf` ne trouve pas les fichiers √† la racine
**Solution :** Ajouter un glob pour la racine + un pour les sous-dossiers

---

## üìä Test du Syst√®me

### Document ing√©r√©
- **Fichier :** `mes_manuel_utilisateur_medimail_webmail_v1.5.pdf` (1.1 MB)
- **Chunks cr√©√©s :** 51 chunks avec HybridChunker
- **Temps de conversion Docling :** ~135 secondes
- **Qualit√© :** Chunks coh√©rents et bien structur√©s

### Exemples de questions test√©es

**Q1:** "Qu'est-ce que RAGFab ?"
‚úÖ R√©ponse structur√©e compl√®te avec architecture, fonctionnalit√©s, d√©ploiement

**Q2:** "Comment acc√©der √† Medimail ?"
‚úÖ R√©ponse avec 3 m√©thodes d'acc√®s + dur√©e de conservation

**Q3:** "Quels sont les dossiers par d√©faut dans Medimail ?"
‚úÖ Liste exacte des 4 dossiers + info sur sous-dossiers

### Workflow observ√©
```
Question utilisateur
    ‚Üì
Mistral API (tool_choice: auto)
    ‚Üì
Tool call d√©tect√©: search_knowledge_base(query="...", limit=5)
    ‚Üì
Embeddings de la query (multilingual-e5-large)
    ‚Üì
Recherche vectorielle PostgreSQL (similarit√© cosinus)
    ‚Üì
Top 5 chunks retourn√©s
    ‚Üì
Tool results envoy√©s √† Mistral
    ‚Üì
R√©ponse finale structur√©e avec citations
```

---

## üöÄ Commandes Utiles

### D√©marrer les services
```bash
cd c:\Users\famat\Documents\rag-cole\ragfab
docker-compose up -d postgres embeddings
```

### Lancer l'agent RAG (mode interactif)
```bash
docker-compose --profile app run --rm rag-app
```

### Ing√©rer un document PDF
```bash
# Placer le PDF dans rag-app/documents/
docker-compose --profile app run --rm rag-app python -m ingestion.ingest
```

### V√©rifier les documents ing√©r√©s
```bash
docker-compose exec postgres psql -U raguser -d ragdb -c "SELECT title, COUNT(c.id) as chunks FROM documents d LEFT JOIN chunks c ON d.id = c.document_id GROUP BY d.id, title;"
```

### Voir le contenu des chunks
```bash
docker-compose exec postgres psql -U raguser -d ragdb -c "SELECT LEFT(content, 200) FROM chunks ORDER BY chunk_index LIMIT 5;"
```

### Rebuild apr√®s modifications
```bash
docker-compose build rag-app
```

---

## üìÅ Fichiers Modifi√©s/Cr√©√©s

| Fichier | Type | Description |
|---------|------|-------------|
| `rag-app/utils/mistral_provider.py` | **CR√â√â** | Provider Mistral avec function calling |
| `rag-app/rag_agent.py` | Modifi√© | Dual-provider + nettoyage UTF-8 |
| `rag-app/ingestion/ingest.py` | Modifi√© | Fix d√©tection PDFs racine |
| `rag-app/ingestion/embedder.py` | Modifi√© | Optimisation batch size |
| `.env` | Modifi√© | Variables Mistral ajout√©es |
| `docker-compose.yml` | Modifi√© | Env vars Mistral pour rag-app |

---

## üéì Concepts Cl√©s Utilis√©s

### PydanticAI Framework
- `Agent` : Agent conversationnel avec tools
- `Model` / `AgentModel` : Providers pour diff√©rents LLMs
- `ToolDefinition` : D√©finition des tools disponibles
- `ModelRequest` / `ModelResponse` : Format des messages
- `ArgsDict` : Wrapper pour arguments de tool calls

### Mistral API
- Format OpenAI-compatible
- `tools` : Liste des fonctions disponibles
- `tool_choice: "auto"` : LLM d√©cide quand appeler les tools
- `tool_calls` : Appels de fonctions dans la r√©ponse
- Messages role `"tool"` : R√©sultats des tools

### Docling
- **HybridChunker** : Chunking intelligent respectant la structure du document
- **DocumentConverter** : Conversion PDF ‚Üí Markdown
- **DoclingDocument** : Repr√©sentation interne du document avec structure pr√©serv√©e

### Embeddings
- **Mod√®le :** multilingual-e5-large (1024 dimensions)
- **Serveur :** FastAPI autonome sur port 8001
- **Batch processing :** 20 chunks par batch pour √©viter timeouts

---

## üîÑ Workflow Function Calling

### √âtape 1: Requ√™te initiale
```json
{
  "model": "mistral-small-latest",
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "Qu'est-ce que RAGFab ?"}
  ],
  "tools": [{
    "type": "function",
    "function": {
      "name": "search_knowledge_base",
      "parameters": {...}
    }
  }],
  "tool_choice": "auto"
}
```

### √âtape 2: R√©ponse Mistral avec tool call
```json
{
  "role": "assistant",
  "tool_calls": [{
    "id": "call_xyz",
    "function": {
      "name": "search_knowledge_base",
      "arguments": "{\"query\": \"RAGFab\", \"limit\": 5}"
    }
  }]
}
```

### √âtape 3: Ex√©cution du tool par PydanticAI
```python
# PydanticAI ex√©cute automatiquement search_knowledge_base
results = await search_knowledge_base(ctx, query="RAGFab", limit=5)
```

### √âtape 4: Envoi des r√©sultats √† Mistral
```json
{
  "messages": [
    {...syst√®me...},
    {...user...},
    {...assistant avec tool_calls...},
    {
      "role": "tool",
      "tool_call_id": "call_xyz",
      "content": "Trouv√© 2 r√©sultats pertinents:\n[Source: Guide]..."
    }
  ]
}
```

### √âtape 5: R√©ponse finale de Mistral
```json
{
  "role": "assistant",
  "content": "RAGFab est un syst√®me RAG optimis√© pour le fran√ßais..."
}
```

---

## üí° Le√ßons Apprises

1. **PydanticAI streaming ne supporte pas bien les tools** ‚Üí Utiliser `run()` au lieu de `run_stream()`

2. **ArgsDict n'est pas un dict** ‚Üí Toujours extraire avec `.args_dict` avant s√©rialisation JSON

3. **L'ordre des messages Mistral est strict** ‚Üí Tool results doivent √™tre dans des messages s√©par√©s avec role "tool"

4. **Les caract√®res UTF-8 invalides sont fr√©quents dans les PDFs** ‚Üí Toujours nettoyer avec `errors='replace'`

5. **Glob patterns `**/*` ne trouvent pas les fichiers √† la racine** ‚Üí Ajouter un pattern pour la racine explicitement

6. **Les batches d'embeddings trop gros timeout** ‚Üí Batch size de 20 est un bon compromis

7. **Docling HybridChunker est excellent** ‚Üí Chunks beaucoup plus coh√©rents que le d√©coupage caract√®re simple

---

## üéØ Prochaines √âtapes Possibles

- [ ] Ajouter support pour plus de mod√®les Mistral (large, codestral)
- [ ] Impl√©menter le streaming avec tools (complexe avec PydanticAI)
- [ ] Ajouter d'autres tools (r√©sum√© de document, extraction d'entit√©s)
- [ ] Monitoring des co√ªts API Mistral (tokens utilis√©s)
- [ ] Cache des embeddings pour √©viter regeneration
- [ ] Interface web Streamlit/Gradio pour d√©mo
- [ ] Multi-turn conversation avec contexte enrichi
- [ ] Support de plusieurs langues simultan√©ment

---

## üìù Notes Techniques

### Pourquoi Mistral Small Latest ?
- `open-mistral-7b` inventait des r√©ponses au lieu d'appeler les tools
- `mistral-small-latest` a un meilleur support du function calling
- Bon compromis co√ªt/performance pour le fran√ßais

### Pourquoi non-streaming ?
- `run_stream()` de PydanticAI d√©tecte les tool calls mais ne les ex√©cute pas automatiquement
- Il faudrait g√©rer manuellement l'ex√©cution des tools et le renvoi des r√©sultats
- `run()` g√®re tout le workflow automatiquement

### Structure des ArgsDict
```python
# Cr√©ation
args = ArgsDict(args_dict={"query": "test", "limit": 5})

# Acc√®s
args.args_dict  # ‚Üí {"query": "test", "limit": 5}

# S√©rialisation JSON
json.dumps(args.args_dict)  # ‚úÖ OK
json.dumps(args)            # ‚ùå TypeError
```

---

**Fin de session - Syst√®me RAGFab op√©rationnel avec Mistral function calling ! üéâ**
